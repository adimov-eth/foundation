1
00:00:00,240 --> 00:00:05,200
Знаешь, что безумно? Что всё это реально.
Что ты имеешь в виду?

2
00:00:05,200 --> 00:00:10,320
Ты так не думаешь? Всё это про ИИ и
вся эта Bay Area… что это происходит.

3
00:00:11,440 --> 00:00:16,240
Разве это не прямо из научной фантастики?
Ещё одна безумная вещь — это то, насколько

4
00:00:16,240 --> 00:00:21,520
нормальным ощущается медленный взлёт.
Мысль о том, что мы будем инвестировать 1%

5
00:00:21,520 --> 00:00:26,880
ВВП в ИИ... мне кажется, это должно было ощущаться как
что-то более грандиозное, а сейчас это просто ощущается...

6
00:00:26,880 --> 00:00:32,640
Оказывается, мы довольно быстро ко всему привыкаем.
Но это также довольно абстрактно. Что это

7
00:00:32,640 --> 00:00:37,920
значит? Это значит, что ты видишь в новостях,
что такая-то компания объявила о такой-то

8
00:00:37,920 --> 00:00:45,840
сумме в долларах. Это всё, что ты видишь.
Пока что это не ощущается как-то иначе.

9
00:00:45,840 --> 00:00:48,000
Может, начнём с этого? Я думаю,
это интересная дискуссия.

10
00:00:48,000 --> 00:00:49,920
Конечно.
Я думаю, твой аргумент

11
00:00:49,920 --> 00:00:55,440
о том, что с точки зрения обычного человека
ничего особо не изменилось, останется

12
00:00:55,440 --> 00:00:58,880
верным даже в момент сингулярности.
Нет, я так не думаю.

13
00:00:58,880 --> 00:01:01,920
Окей, интересно.
То, что я имел в виду под

14
00:01:01,920 --> 00:01:10,880
"не ощущается иначе" — это, окей, такая-то
компания объявила о какой-то сложно представимой

15
00:01:10,880 --> 00:01:15,040
сумме инвестиций.
Я не думаю, что кто-то знает, что с этим делать.

16
00:01:15,920 --> 00:01:24,160
Но я думаю, что влияние ИИ будет ощутимым.
ИИ будет диффундировать через экономику.

17
00:01:24,160 --> 00:01:28,560
Для этого будут очень сильные экономические силы,
и я думаю, что влияние

18
00:01:28,560 --> 00:01:32,640
будет ощущаться очень сильно.
Когда ты ожидаешь это влияние?

19
00:01:32,640 --> 00:01:38,480
Мне кажется, модели кажутся умнее, чем
подразумевает их экономическое влияние.

20
00:01:38,480 --> 00:01:44,720
Да. Это одна из очень сбивающих с толку
вещей в моделях прямо сейчас.

21
00:01:44,720 --> 00:01:52,400
Как примирить тот факт, что
они так хорошо справляются с эвалами?

22
00:01:53,040 --> 00:01:57,680
Ты смотришь на эвалы и думаешь: "Это
довольно сложные эвалы". Они справляются

23
00:01:57,680 --> 00:02:04,080
так хорошо. Но экономическое влияние,
кажется, драматически отстаёт.

24
00:02:07,920 --> 00:02:12,240
В этом очень трудно разобраться:
как модель может, с одной стороны,

25
00:02:12,240 --> 00:02:18,880
делать эти удивительные вещи, а с другой —
повторять одно и то же дважды в какой-то ситуации?

26
00:02:20,560 --> 00:02:24,560
Примером может быть, скажем, ты
используешь вайб-кодинг, чтобы что-то сделать.

27
00:02:24,560 --> 00:02:28,240
Ты заходишь куда-то и получаешь баг.
Затем ты говоришь модели:

28
00:02:28,240 --> 00:02:32,000
"Можешь, пожалуйста, исправить баг?"
И модель говорит: "О боже,

29
00:02:32,000 --> 00:02:35,760
ты так прав. У меня баг. Дай-ка я
исправлю это". И она вносит второй баг.

30
00:02:36,960 --> 00:02:40,080
Затем ты говоришь ей: "У тебя этот
новый второй баг", и она говорит тебе:

31
00:02:40,080 --> 00:02:43,520
"О боже, как я могла это сделать?
Ты снова так прав", и возвращает

32
00:02:43,520 --> 00:02:52,000
первый баг, и ты можешь чередовать
их. Как это возможно? Я не уверен, но это

33
00:02:52,000 --> 00:03:02,080
действительно предполагает, что происходит что-то странное. У
меня есть два возможных объяснения. Более причудливое

34
00:03:02,080 --> 00:03:07,760
объяснение заключается в том, что, возможно, RL-тренировка делает
модели немного слишком однонаправленными и узко

35
00:03:07,760 --> 00:03:17,680
сфокусированными, немного слишком неосведомлёнными, даже если
это также делает их осведомлёнными в других отношениях.

36
00:03:17,680 --> 00:03:25,040
Из-за этого они не могут делать базовые вещи.
Но есть и другое объяснение. Когда

37
00:03:25,040 --> 00:03:31,440
люди занимались предобучением,
вопрос о том, на каких данных тренировать, был

38
00:03:31,440 --> 00:03:41,040
решён, потому что ответом было "всё".
Когда ты делаешь предобучение, тебе нужны все данные.

39
00:03:41,040 --> 00:03:44,960
Так что тебе не нужно думать, будут ли
это эти данные или те данные.

40
00:03:44,960 --> 00:03:48,560
Но когда люди делают RL-тренировку,
им действительно нужно думать.

41
00:03:48,560 --> 00:03:52,080
Они говорят: "Окей, мы хотим иметь этот
вид RL-тренировки для этой вещи

42
00:03:52,080 --> 00:03:58,160
и тот вид RL-тренировки для той вещи".
Насколько я слышал, во всех компаниях есть команды,

43
00:03:58,160 --> 00:04:02,800
которые просто создают новые RL-среды
и просто добавляют их в тренировочный микс.

44
00:04:02,800 --> 00:04:06,080
Вопрос в том, что это такое?
Там так много степеней свободы.

45
00:04:06,080 --> 00:04:10,080
Существует такое огромное разнообразие
RL-сред, которые можно создать.

46
00:04:12,720 --> 00:04:17,360
Одна вещь, которую можно сделать, и я думаю, что это
делается непреднамеренно,

47
00:04:17,360 --> 00:04:24,880
это то, что люди черпают вдохновение из эвалов.
Ты говоришь: "Эй, я бы хотел, чтобы наша модель справлялась

48
00:04:24,880 --> 00:04:28,720
действительно хорошо, когда мы её выпустим.
Я хочу, чтобы эвалы выглядели отлично.

49
00:04:28,720 --> 00:04:32,640
Какой может быть RL-тренировка,
которая помогла бы в этой задаче?"

50
00:04:33,840 --> 00:04:39,040
Я думаю, это то, что происходит, и
это может объяснить многое из того, что творится.

51
00:04:39,040 --> 00:04:44,720
Если объединить это с тем, что генерализация
моделей на самом деле неадекватна,

52
00:04:44,720 --> 00:04:48,640
это имеет потенциал объяснить многое
из того, что мы видим, этот разрыв

53
00:04:48,640 --> 00:04:56,320
между результатами эвалов и реальной производительностью
в мире, что является чем-то, что мы сегодня

54
00:04:56,320 --> 00:05:03,680
даже не понимаем, что мы под этим подразумеваем.
Мне нравится эта идея, что настоящий хакинг награды —

55
00:05:03,680 --> 00:05:07,920
это люди-исследователи, которые
слишком сфокусированы на эвалах.

56
00:05:09,040 --> 00:05:12,960
Я думаю, есть два способа
понять, или попытаться подумать о том,

57
00:05:12,960 --> 00:05:18,880
что ты только что отметил.
Один — это если дело в том, что

58
00:05:18,880 --> 00:05:23,600
просто став сверхчеловеком в соревнованиях по кодингу,
модель не станет автоматически

59
00:05:23,600 --> 00:05:30,080
обладать лучшим вкусом и проявлять лучшее суждение
о том, как улучшить твою кодовую базу, ну тогда тебе

60
00:05:30,080 --> 00:05:35,120
следует расширить набор сред так,
чтобы ты не просто тестировал её на наличие

61
00:05:35,120 --> 00:05:38,560
лучших результатов в соревнованиях по кодингу.
Она также должна быть способна создавать лучший вид

62
00:05:38,560 --> 00:05:44,400
приложения для вещи X, или вещи Y, или вещи Z.
Другой, может быть, это то, на что ты намекаешь,

63
00:05:44,400 --> 00:05:50,320
это сказать: "Почему вообще должно быть так,
что становление сверхчеловеком

64
00:05:50,320 --> 00:05:54,400
в соревнованиях по кодингу не делает тебя
более искусным программистом в целом?"

65
00:05:54,400 --> 00:05:59,280
Может быть, нужно не продолжать
наращивать количество и разнообразие

66
00:05:59,280 --> 00:06:05,200
сред, а придумать подход,
который позволит учиться на одной среде и

67
00:06:05,200 --> 00:06:12,560
улучшать свои показатели в чём-то другом.
У меня есть человеческая аналогия, которая может быть полезна.

68
00:06:14,160 --> 00:06:18,160
Давай возьмём случай спортивного программирования,
раз уж ты упомянул это. Допустим, у тебя есть два

69
00:06:18,160 --> 00:06:24,240
студента. Один из них решил, что хочет
быть лучшим спортивным программистом, поэтому

70
00:06:24,240 --> 00:06:31,200
он будет практиковаться 10 000 часов в этой области.
Он решит все задачи, запомнит все

71
00:06:31,200 --> 00:06:39,920
техники доказательств и будет очень искусен в быстрой
и правильной реализации всех алгоритмов.

72
00:06:40,640 --> 00:06:46,240
Сделав это, он стал одним из лучших.
Студент номер два подумал: "О,

73
00:06:46,240 --> 00:06:50,400
спортивное программирование — это круто".
Может быть, он практиковался 100 часов,

74
00:06:50,400 --> 00:06:54,640
гораздо меньше, и он тоже справился очень хорошо.
Кто из них, по-твоему, добьётся большего успеха

75
00:06:54,640 --> 00:06:56,800
в своей карьере позже?
Второй.

76
00:06:56,800 --> 00:07:00,560
Верно. Я думаю, это в принципе то, что происходит.
Модели гораздо больше похожи на

77
00:07:00,560 --> 00:07:04,400
первого студента, но даже ещё больше.
Потому что тогда мы говорим: модель должна

78
00:07:04,400 --> 00:07:10,080
быть хороша в спортивном программировании, так что давай возьмём
каждую задачу по спортивному программированию, которая когда-либо была.

79
00:07:10,080 --> 00:07:13,120
А затем давай сделаем аугментацию данных,
чтобы у нас было ещё больше задач по

80
00:07:13,120 --> 00:07:18,480
спортивному программированию, и натренируем на этом.
Теперь у тебя есть этот великолепный спортивный программист.

81
00:07:18,480 --> 00:07:27,120
С этой аналогией, я думаю, это более интуитивно.
Да, окей, если она так хорошо натренирована, все

82
00:07:27,120 --> 00:07:32,480
разные алгоритмы и все разные
техники доказательств у неё прямо под рукой.

83
00:07:32,480 --> 00:07:36,320
И более интуитивно понятно, что с таким
уровнем подготовки она не

84
00:07:36,320 --> 00:07:42,160
обязательно генерализуется на другие вещи.
Но тогда какова аналогия для того, что

85
00:07:42,160 --> 00:07:48,160
делает второй студент до того, как
потратит 100 часов на файн-тюнинг?

86
00:07:48,160 --> 00:07:56,480
Я думаю, у него есть "это". Фактор "это".
Когда я был студентом,

87
00:07:56,480 --> 00:08:01,280
я помню, был такой студент, который
учился со мной, так что я знаю, что это существует.

88
00:08:01,840 --> 00:08:06,160
Я думаю, интересно отличить
"это" от того, что делает предобучение.

89
00:08:06,160 --> 00:08:10,240
Один из способов понять то, что ты только что сказал
о том, что не нужно выбирать данные при

90
00:08:10,240 --> 00:08:15,040
предобучении — это сказать, что это на самом деле
не отличается от 10 000 часов практики.

91
00:08:15,040 --> 00:08:20,320
Просто ты получаешь эти 10 000 часов
практики бесплатно, потому что они уже

92
00:08:20,320 --> 00:08:25,120
где-то в распределении предобучения.
Но, может быть, ты предполагаешь, что на самом деле

93
00:08:25,120 --> 00:08:28,800
от предобучения не так уж много генерализации.
В предобучении просто так много данных, но

94
00:08:28,800 --> 00:08:33,360
оно не обязательно генерализуется лучше, чем RL.
Главная сила предобучения в том,

95
00:08:33,360 --> 00:08:40,160
что: А, его так много, и Б,
тебе не нужно сильно думать о том, какие

96
00:08:40,160 --> 00:08:45,680
данные поместить в предобучение.
Это очень естественные данные, и они

97
00:08:45,680 --> 00:08:54,800
действительно включают в себя многое из того, что делают люди:
мысли людей и множество особенностей.

98
00:08:54,800 --> 00:09:01,120
Это как целый мир, спроецированный
людьми в текст, и предобучение пытается

99
00:09:01,120 --> 00:09:08,960
захватить это, используя огромное количество данных.
О предобучении очень трудно рассуждать,

100
00:09:08,960 --> 00:09:17,760
потому что так сложно понять, каким образом
модель полагается на данные предобучения.

101
00:09:17,760 --> 00:09:23,600
Всякий раз, когда модель совершает ошибку, может ли это быть
потому, что что-то случайно не так хорошо подкреплено

102
00:09:23,600 --> 00:09:30,720
данными предобучения? "Подкреплено
предобучением" — это, возможно, неточный термин.

103
00:09:30,720 --> 00:09:33,680
Я не знаю, могу ли я добавить
что-то более полезное по этому поводу.

104
00:09:36,000 --> 00:09:38,400
Я не думаю, что существует
человеческий аналог предобучения.

105
00:09:39,440 --> 00:09:43,600
Вот аналогии, которые люди предлагали
для того, что является человеческим аналогом предобучения.

106
00:09:43,600 --> 00:09:47,920
Мне любопытно узнать твои мысли о том,
почему они потенциально неверны.

107
00:09:47,920 --> 00:09:54,160
Одна — это думать о первых 18, или 15,
или 13 годах жизни человека, когда он

108
00:09:54,160 --> 00:10:00,320
не обязательно экономически продуктивен,
но делает что-то, что заставляет

109
00:10:00,320 --> 00:10:07,200
его лучше понимать мир и так далее.
Другая — думать об эволюции как о выполнении

110
00:10:07,200 --> 00:10:14,480
некоего поиска в течение 3 миллиардов лет, который
затем приводит к экземпляру человеческой жизни.

111
00:10:14,480 --> 00:10:17,360
Мне любопытно, считаешь ли ты какую-либо из
них аналогичной предобучению.

112
00:10:18,000 --> 00:10:22,800
Как бы ты думал о том, на что похоже пожизненное
человеческое обучение, если не на предобучение?

113
00:10:22,800 --> 00:10:28,800
Я думаю, есть некоторые сходства между обеими
этими вещами и предобучением, и предобучение пытается

114
00:10:28,800 --> 00:10:32,000
играть роль обеих.
Но я думаю, что есть и некоторые

115
00:10:32,000 --> 00:10:38,240
большие различия.
Объём данных предобучения очень,

116
00:10:38,240 --> 00:10:41,520
очень ошеломляющий.
Да.

117
00:10:41,520 --> 00:10:47,600
Каким-то образом человек, даже спустя 15 лет
с крошечной долей данных

118
00:10:47,600 --> 00:10:50,800
предобучения, знает гораздо меньше.
Но то, что он знает,

119
00:10:50,800 --> 00:10:57,120
он знает как-то гораздо глубже.
Уже в этом возрасте ты бы не

120
00:10:57,120 --> 00:11:02,240
совершал ошибок, которые совершают наши ИИ. Есть ещё одна
вещь. Ты можешь сказать, может ли это быть чем-то вроде

121
00:11:02,240 --> 00:11:07,600
эволюции? Ответ — возможно. Но в этом случае,
я думаю, у эволюции может быть преимущество.

122
00:11:08,880 --> 00:11:19,680
Я помню, читал об этом случае.
Один из способов, которым нейробиологи могут

123
00:11:19,680 --> 00:11:25,920
узнать о мозге — это изучение людей с
повреждениями различных частей мозга.

124
00:11:26,960 --> 00:11:30,640
У некоторых людей самые странные симптомы,
которые только можно представить. Это на самом деле очень,

125
00:11:30,640 --> 00:11:35,680
очень интересно. Один случай, который
приходит на ум, уместен здесь.

126
00:11:35,680 --> 00:11:44,000
Я читал об этом человеке, у которого было какое-то
повреждение мозга, инсульт или несчастный случай,

127
00:11:44,000 --> 00:11:51,520
который вывел из строя его эмоциональную обработку.
Так что он перестал чувствовать какие-либо эмоции.

128
00:11:54,800 --> 00:11:58,240
Он всё ещё оставался очень красноречивым
и мог решать маленькие головоломки,

129
00:11:58,240 --> 00:12:03,520
и на тестах он казался совершенно нормальным.
Но он не чувствовал эмоций. Он не чувствовал грусти,

130
00:12:03,520 --> 00:12:10,000
он не чувствовал гнева, он не чувствовал оживления.
Он стал как-то чрезвычайно плох в принятии каких-либо

131
00:12:10,000 --> 00:12:12,240
решений вообще.
Ему требовались

132
00:12:12,240 --> 00:12:17,120
часы, чтобы решить, какие носки надеть.
Он принимал очень плохие финансовые решения.

133
00:12:23,120 --> 00:12:34,240
Что это говорит о роли наших встроенных
эмоций в том, чтобы делать нас жизнеспособным агентом, по сути?

134
00:12:34,240 --> 00:12:41,680
Чтобы связать это с твоим вопросом о предобучении,
может быть, если ты достаточно хорош в извлечении всего

135
00:12:41,680 --> 00:12:46,480
из предобучения, ты мог бы получить и это.
Но это та вещь, которая кажется...

136
00:12:50,960 --> 00:12:56,160
Ну, это может быть возможно, а может и нет,
получить это из предобучения.

137
00:12:56,160 --> 00:13:04,800
Что такое "это"? Явно не просто напрямую
эмоции. Это кажется чем-то почти похожим на

138
00:13:04,800 --> 00:13:12,080
функцию ценности, которая говорит тебе, какой
должна быть конечная награда за любое решение.

139
00:13:12,080 --> 00:13:15,200
Ты думаешь, это не приходит как бы
неявно из предобучения?

140
00:13:15,200 --> 00:13:19,440
Я думаю, могло бы. Я просто
говорю, что это не на 100% очевидно.

141
00:13:20,400 --> 00:13:26,320
Но что это? Как ты думаешь об эмоциях?
Какова ML-аналогия для эмоций?

142
00:13:26,320 --> 00:13:31,440
Это должно быть что-то вроде функции ценности.
Но я не думаю, что есть отличная ML-аналогия,

143
00:13:31,440 --> 00:13:36,480
потому что прямо сейчас функции ценности не играют
очень заметной роли в том, что делают люди.

144
00:13:36,480 --> 00:13:40,000
Может быть, стоит определить для аудитории, что такое
функция ценности, если ты хочешь это сделать.

145
00:13:40,560 --> 00:13:50,880
Конечно, я буду очень рад это сделать.
Когда люди занимаются обучением с подкреплением,

146
00:13:50,880 --> 00:13:56,560
то, как обучение с подкреплением делается
прямо сейчас, как люди тренируют этих агентов?

147
00:13:56,560 --> 00:14:00,160
У тебя есть нейросеть, и ты
даёшь ей задачу, а затем ты

148
00:14:00,160 --> 00:14:03,440
говоришь модели: "Иди реши её".
Модель предпринимает, может быть, тысячи,

149
00:14:03,440 --> 00:14:09,040
сотни тысяч действий или мыслей или
чего-то ещё, а затем она выдаёт решение.

150
00:14:09,040 --> 00:14:14,960
Решение оценивается. И затем оценка
используется для предоставления тренировочного сигнала

151
00:14:14,960 --> 00:14:23,200
для каждого отдельного действия в твоей траектории.
Это означает, что если ты делаешь что-то,

152
00:14:23,200 --> 00:14:29,120
что длится долгое время — если ты тренируешь
задачу, решение которой занимает много времени —

153
00:14:29,120 --> 00:14:33,440
обучения не будет вообще, пока ты не
придёшь к предложенному решению.

154
00:14:33,440 --> 00:14:39,760
Вот как обучение с подкреплением делается наивно.
Вот как o1, R1 якобы делаются.

155
00:14:40,800 --> 00:14:48,240
Функция ценности говорит что-то вроде:
"Может быть, я могла бы иногда, не всегда,

156
00:14:48,240 --> 00:14:52,400
говорить тебе, хорошо ты справляешься или плохо".
Понятие функции ценности более

157
00:14:52,400 --> 00:14:56,960
полезно в одних областях, чем в других.
Например, когда ты играешь в шахматы и

158
00:14:56,960 --> 00:15:01,440
ты теряешь фигуру, я облажался.
Тебе не нужно играть всю

159
00:15:01,440 --> 00:15:08,400
партию, чтобы знать, что то, что я только что сделал, было плохо,
и, следовательно, всё, что этому предшествовало, тоже было плохо.

160
00:15:08,960 --> 00:15:14,800
Функция ценности позволяет тебе сократить
ожидание до самого конца.
161
00:15:19,040 --> 00:15:23,040
Давай предположим, что ты занимаешься какой-то
математической вещью или программированием,

162
00:15:23,040 --> 00:15:26,800
и ты пытаешься исследовать
конкретное решение или направление.

163
00:15:26,800 --> 00:15:34,320
После, скажем, тысячи шагов размышлений,
ты пришёл к выводу, что это направление бесперспективно.

164
00:15:34,320 --> 00:15:39,760
Как только ты приходишь к этому выводу, ты
уже мог бы получить сигнал награды

165
00:15:39,760 --> 00:15:43,520
тысячу временных шагов назад, когда
ты решил пойти по этому пути.

166
00:15:43,520 --> 00:15:49,600
Ты говоришь: "В следующий раз я не должен идти по этому
пути в похожей ситуации", задолго до того, как ты

167
00:15:49,600 --> 00:15:56,480
на самом деле пришёл к предложенному решению.
Это было в статье DeepSeek R1 — что

168
00:15:56,480 --> 00:16:02,640
пространство траекторий настолько широко, что,
возможно, трудно выучить отображение

169
00:16:02,640 --> 00:16:08,800
из промежуточной траектории и ценности.
И также учитывая, что в кодинге, например,

170
00:16:08,800 --> 00:16:12,720
у тебя будет неправильная идея, потом ты
вернёшься назад, потом ты что-то изменишь.

171
00:16:12,720 --> 00:16:15,840
Это звучит как такое отсутствие
веры в глубокое обучение.

172
00:16:16,800 --> 00:16:23,520
Конечно, это может быть трудно, но
нет ничего, чего глубокое обучение не могло бы сделать.

173
00:16:23,520 --> 00:16:32,640
Моё ожидание таково, что функция ценности должна
быть полезной, и я полностью ожидаю, что они будут

174
00:16:32,640 --> 00:16:37,040
использоваться в будущем, если не уже.
То, на что я намекал с человеком,

175
00:16:37,040 --> 00:16:47,680
чей эмоциональный центр был повреждён, это скорее
то, что, возможно, это предполагает, что функция

176
00:16:47,680 --> 00:16:55,360
ценности людей модулируется эмоциями
каким-то важным образом, который жёстко закодирован эволюцией.

177
00:16:55,360 --> 00:17:00,400
И, может быть, это важно для того, чтобы
люди были эффективны в мире.

178
00:17:00,400 --> 00:17:02,640
Это то, о чём я планировал тебя спросить.

179
00:17:02,640 --> 00:17:06,160
Есть что-то действительно интересное в
эмоциях функции ценности, а именно то, что

180
00:17:06,160 --> 00:17:16,160
впечатляет, что они имеют такую большую полезность,
оставаясь при этом довольно простыми для понимания.

181
00:17:16,160 --> 00:17:25,600
У меня есть два ответа. Я согласен, что по сравнению с
теми вещами, которые мы изучаем, и вещами,

182
00:17:25,600 --> 00:17:30,240
о которых мы говорим, видом ИИ, о котором мы
говорим, эмоции относительно просты.

183
00:17:31,120 --> 00:17:35,680
Они могут быть даже настолько простыми, что, возможно, ты
мог бы отобразить их понятным для человека способом.

184
00:17:35,680 --> 00:17:40,960
Я думаю, это было бы круто сделать.
Что касается полезности, однако,

185
00:17:40,960 --> 00:17:49,360
я думаю, здесь есть вещь, где существует этот
компромисс между сложностью и надежностью, где сложные

186
00:17:49,360 --> 00:17:58,960
вещи могут быть очень полезными, но простые вещи
очень полезны в очень широком диапазоне ситуаций.

187
00:18:00,240 --> 00:18:06,720
Один из способов интерпретировать то, что мы видим, это то, что
у нас есть эти эмоции, которые эволюционировали в основном

188
00:18:06,720 --> 00:18:13,200
от наших предков-млекопитающих, а затем были немного
подстроены, пока мы были гоминидами, совсем чуть-чуть.

189
00:18:13,200 --> 00:18:19,760
У нас действительно есть приличное количество социальных эмоций,
которых, возможно, не хватает млекопитающим. Но они

190
00:18:19,760 --> 00:18:24,720
не очень сложные. И поскольку они
не сложные, они служат нам так хорошо в

191
00:18:24,720 --> 00:18:28,480
этом очень разном мире по сравнению с
тем, в котором мы жили. На самом деле,

192
00:18:28,480 --> 00:18:32,800
они также совершают ошибки. Например, наши
эмоции… Ну на самом деле, я не знаю.

193
00:18:32,800 --> 00:18:39,760
Считается ли голод эмоцией? Это спорно.
Но я думаю, например, наше интуитивное чувство

194
00:18:39,760 --> 00:18:49,120
голода не преуспевает в том, чтобы направлять нас
правильно в этом мире с изобилием еды.

195
00:18:50,000 --> 00:18:56,240
Люди говорят о масштабировании
данных, масштабировании параметров, масштабировании вычислений.

196
00:18:56,240 --> 00:18:58,000
Есть ли более общий
способ думать о масштабировании?

197
00:18:58,000 --> 00:19:10,480
Каковы другие оси масштабирования?
Вот перспектива, которая, я думаю, может быть верной.

198
00:19:12,160 --> 00:19:16,320
То, как ML работало раньше — это то, что
люди просто возились с

199
00:19:16,320 --> 00:19:28,240
вещами и пытались получить интересные результаты.
Вот что происходило в прошлом. Затем

200
00:19:28,240 --> 00:19:39,120
пришло озарение масштабирования. Законы масштабирования, GPT-3,
и внезапно все поняли, что мы должны масштабироваться.

201
00:19:40,400 --> 00:19:47,120
Это пример того, как язык
влияет на мышление. "Масштабирование" — это всего лишь

202
00:19:47,120 --> 00:19:51,120
одно слово, но это такое мощное слово,
потому что оно сообщает людям, что делать.

203
00:19:51,120 --> 00:19:57,280
Они говорят: "Давайте попробуем масштабировать вещи".
И ты говоришь: что мы масштабируем?

204
00:19:57,280 --> 00:20:02,240
Предобучение было тем, что нужно масштабировать.
Это был конкретный рецепт масштабирования.

205
00:20:02,240 --> 00:20:08,000
Большой прорыв предобучения — это
осознание того, что этот рецепт хорош.

206
00:20:08,000 --> 00:20:14,400
Ты говоришь: "Эй, если смешать немного вычислений
с небольшим количеством данных в нейросети

207
00:20:14,400 --> 00:20:19,600
определённого размера, ты получишь результаты.
Ты будешь знать, что станешь лучше, если

208
00:20:19,600 --> 00:20:26,560
просто масштабируешь рецепт". Это тоже отлично.
Компании любят это, потому что это даёт очень

209
00:20:26,560 --> 00:20:34,800
низкорисковый способ инвестирования ресурсов.
Гораздо сложнее инвестировать свои ресурсы

210
00:20:34,800 --> 00:20:39,760
в исследования. Сравни это. Если ты исследуешь,
тебе нужно типа: "Идите, исследователи,

211
00:20:39,760 --> 00:20:45,440
и исследуйте, и придумайте что-нибудь",
против — получи больше данных, получи больше вычислений.

212
00:20:45,440 --> 00:20:52,400
Ты знаешь, что получишь что-то от предобучения.
Действительно, похоже, основываясь на различных

213
00:20:54,640 --> 00:21:00,160
вещах, которые некоторые люди говорят в Твиттере, может быть,
кажется, что Gemini нашли способ получить

214
00:21:00,160 --> 00:21:02,560
больше от предобучения.
В какой-то момент, однако,

215
00:21:02,560 --> 00:21:06,480
у предобучения закончатся данные.
Данные совершенно явно конечны. Что

216
00:21:06,480 --> 00:21:11,280
ты делаешь дальше? Либо ты делаешь какой-то
вид прокачанного предобучения, другой

217
00:21:11,280 --> 00:21:15,920
рецепт, отличный от того, что ты делал раньше, либо
ты делаешь RL, или, может быть, что-то ещё.

218
00:21:15,920 --> 00:21:20,320
Но теперь, когда вычисления большие, вычисления
теперь очень большие, в каком-то смысле мы

219
00:21:20,320 --> 00:21:24,160
вернулись в эпоху исследований.
Может быть, вот ещё один способ сказать это.

220
00:21:24,160 --> 00:21:31,280
Вплоть до 2020 года, с 2012 по
2020, это была эпоха исследований.

221
00:21:31,280 --> 00:21:35,920
Теперь, с 2020 по 2025, это была
эпоха масштабирования — может быть, плюс-минус,

222
00:21:35,920 --> 00:21:39,440
давай добавим погрешности к этим годам — потому что
люди говорят: "Это удивительно. Вы должны

223
00:21:39,440 --> 00:21:45,280
масштабироваться больше. Продолжайте масштабироваться". Одно слово:
масштабирование. Но теперь масштаб такой большой.

224
00:21:46,320 --> 00:21:53,440
Действительно ли вера такова: "О, это так много, но если бы у тебя
было в 100 раз больше, всё было бы так по-другому?"

225
00:21:53,440 --> 00:21:58,000
Это было бы по-другому, безусловно.
Но такова ли вера, что если ты просто

226
00:21:58,000 --> 00:22:04,560
увеличишь масштаб в 100 раз, всё преобразится?
Я не думаю, что это правда. Так что мы снова вернулись в эпоху

227
00:22:04,560 --> 00:22:10,400
исследований, просто с большими компьютерами.
Это очень интересный способ сформулировать.

228
00:22:10,400 --> 00:22:12,560
Но позволь мне задать тебе
вопрос, который ты только что поставил.

229
00:22:12,560 --> 00:22:16,560
Что мы масштабируем, и что
значило бы иметь рецепт?

230
00:22:17,440 --> 00:22:23,680
Я полагаю, я не знаю об очень чистой
взаимосвязи, которая почти выглядит как закон

231
00:22:23,680 --> 00:22:27,920
физики, который существовал в предобучении.
Был степенной закон между данными или

232
00:22:27,920 --> 00:22:33,920
вычислениями или параметрами и лоссом.
Какого рода взаимосвязь

233
00:22:33,920 --> 00:22:38,640
мы должны искать, и как нам следует думать
о том, как может выглядеть этот новый рецепт?

234
00:22:40,240 --> 00:22:48,000
Мы уже стали свидетелями перехода от одного
типа масштабирования к другому типу масштабирования,

235
00:22:48,000 --> 00:22:56,800
от предобучения к RL. Теперь люди масштабируют
RL. Теперь, основываясь на том, что люди говорят в Твиттере,

236
00:22:56,800 --> 00:23:01,280
они тратят больше вычислений на RL, чем на
предобучение в данный момент, потому что RL

237
00:23:01,280 --> 00:23:07,840
на самом деле может потреблять довольно много вычислений.
Ты делаешь очень длинные траектории, так что требуется много

238
00:23:07,840 --> 00:23:11,360
вычислений, чтобы создать эти траектории.
Затем ты получаешь относительно небольшое количество

239
00:23:11,360 --> 00:23:15,760
обучения на траекторию, так что ты
действительно можешь потратить много вычислений.

240
00:23:21,680 --> 00:23:27,280
Я бы даже не назвал это масштабированием.
Я бы сказал: "Эй, что вы делаете?

241
00:23:27,280 --> 00:23:31,840
Является ли то, что вы делаете, самой
продуктивной вещью, которую вы могли бы делать?

242
00:23:31,840 --> 00:23:36,000
Можете ли вы найти более продуктивный
способ использования ваших вычислений?"

243
00:23:36,000 --> 00:23:39,520
Мы обсуждали дело с функцией
ценности ранее.

244
00:23:39,520 --> 00:23:43,920
Может быть, как только люди станут хороши в функциях
ценности, они будут использовать свои

245
00:23:44,720 --> 00:23:50,320
ресурсы более продуктивно.
Если ты найдёшь совершенно другой способ

246
00:23:50,320 --> 00:23:56,240
тренировки моделей, ты мог бы сказать: "Это
масштабирование или это просто использование твоих ресурсов?"

247
00:23:56,240 --> 00:23:59,680
Я думаю, это становится немного двусмысленным.
В том смысле, что, когда люди были в

248
00:23:59,680 --> 00:24:03,600
эпохе исследований тогда, это было:
"Давайте попробуем это и это и это.

249
00:24:03,600 --> 00:24:07,680
Давайте попробуем то и то и то.
О, смотрите, происходит что-то интересное".

250
00:24:07,680 --> 00:24:12,480
Я думаю, будет возвращение к этому.
Если мы снова в эре исследований,

251
00:24:12,480 --> 00:24:17,040
отступая назад, какая часть
рецепта — та, о которой нам нужно думать больше всего?

252
00:24:17,040 --> 00:24:21,200
Когда ты говоришь "функция ценности", люди
уже пробуют текущий рецепт,

253
00:24:21,200 --> 00:24:24,720
но затем имеют LLM-как-судью и так далее.
Ты мог бы сказать, что это функция ценности,

254
00:24:24,720 --> 00:24:26,880
но звучит так, будто у тебя на уме
что-то гораздо более фундаментальное.

255
00:24:29,920 --> 00:24:35,920
Должны ли мы вообще переосмыслить предобучение, а не
просто добавлять больше шагов в конец этого процесса?

256
00:24:38,240 --> 00:24:41,600
Дискуссия о функции ценности,
я думаю, она была интересной.

257
00:24:41,600 --> 00:24:48,320
Я хочу подчеркнуть, что я думаю, что функция
ценности — это то, что сделает RL более

258
00:24:48,320 --> 00:24:55,440
эффективным, и я думаю, что это имеет значение.
Но я думаю, что всё, что ты можешь сделать с функцией

259
00:24:55,440 --> 00:25:02,080
ценности, ты можешь сделать и без неё, просто медленнее.
Вещь, которая, я думаю, является наиболее фундаментальной,

260
00:25:02,080 --> 00:25:08,560
это то, что эти модели как-то просто генерализуются
драматически хуже, чем люди. Это супер

261
00:25:08,560 --> 00:25:18,480
очевидно. Это кажется очень фундаментальной вещью.
Так что это суть: генерализация. Есть два

262
00:25:18,480 --> 00:25:24,160
подвопроса. Есть один, который касается эффективности
использования данных: почему требуется так много данных

263
00:25:24,160 --> 00:25:29,520
для этих моделей, чтобы учиться, по сравнению с людьми? Есть
второй вопрос. Даже отдельно от количества

264
00:25:29,520 --> 00:25:35,840
данных, которое это занимает, почему так трудно научить
модель тому, что мы хотим, по сравнению с человеком?

265
00:25:37,360 --> 00:25:43,920
Для человека нам не обязательно нужна
проверяемая награда, чтобы быть в состоянии… Ты, вероятно,

266
00:25:43,920 --> 00:25:48,560
менторишь кучу исследователей прямо сейчас, и
ты разговариваешь с ними, ты показываешь им свой

267
00:25:48,560 --> 00:25:52,560
код, и ты показываешь им, как ты думаешь.
Из этого они перенимают твой образ

268
00:25:52,560 --> 00:25:56,800
мышления и то, как им следует проводить исследования.
Тебе не нужно устанавливать для них проверяемую награду,

269
00:25:56,800 --> 00:25:59,680
которая типа: "Окей, это следующая часть
учебного плана, а теперь это следующая часть

270
00:25:59,680 --> 00:26:06,080
твоего учебного плана. О, эта тренировка была нестабильной".
Нет этого утомительного, индивидуального процесса.

271
00:26:06,640 --> 00:26:10,160
Возможно, эти две проблемы на самом деле
связаны каким-то образом, но мне было бы любопытно

272
00:26:10,160 --> 00:26:15,520
исследовать эту вторую вещь, которая больше
похожа на непрерывное обучение, и эту первую вещь,

273
00:26:15,520 --> 00:26:22,720
которая ощущается просто как эффективность использования данных.
Ты мог бы на самом деле задаться вопросом, что одно из возможных

274
00:26:22,720 --> 00:26:30,720
объяснений человеческой эффективности использования данных,
которое нужно рассмотреть — это эволюция.

275
00:26:31,760 --> 00:26:38,880
Эволюция дала нам небольшое количество
наиболее полезной информации.

276
00:26:38,880 --> 00:26:45,040
Для таких вещей, как зрение, слух и
локомоция, я думаю, есть довольно

277
00:26:45,040 --> 00:26:55,120
сильный аргумент, что эволюция дала нам многое.
Например, человеческая ловкость намного превосходит… Я имею в виду

278
00:26:55,120 --> 00:27:00,480
роботы тоже могут стать ловкими, если подвергнуть
их огромному количеству тренировок в симуляции.

279
00:27:00,480 --> 00:27:04,560
Но натренировать робота в реальном мире
быстро осваивать новый навык, как

280
00:27:04,560 --> 00:27:10,560
это делает человек, кажется очень недосягаемым.
Здесь ты мог бы сказать: "О да, локомоция.

281
00:27:10,560 --> 00:27:14,640
Всем нашим предкам нужна была
отличная локомоция, белкам.

282
00:27:15,920 --> 00:27:19,760
Так что с локомоцией, может быть, у нас
есть какой-то невероятный прайор".

283
00:27:19,760 --> 00:27:24,400
Ты мог бы привести тот же аргумент для зрения.
Я полагаю, Ян Лекун высказал мысль, что

284
00:27:25,440 --> 00:27:30,800
дети учатся водить машину после 10
часов практики, что правда.

285
00:27:30,800 --> 00:27:35,280
Но наше зрение такое хорошее.
По крайней мере для меня,

286
00:27:35,280 --> 00:27:41,120
я помню себя пятилетним.
Я тогда очень увлекался машинами.

287
00:27:41,120 --> 00:27:47,200
Я почти уверен, что моё распознавание машин было более чем
адекватным для вождения уже в пятилетнем возрасте.

288
00:27:47,200 --> 00:27:49,040
Ты не видишь так уж
много данных, будучи пятилетним.

289
00:27:49,040 --> 00:27:53,200
Ты проводишь большую часть времени в доме
родителей, так что у тебя очень низкое разнообразие данных.

290
00:27:53,200 --> 00:28:00,400
Но ты мог бы сказать, может быть, это тоже эволюция.
Но в языке, математике и кодинге, вероятно, нет.

291
00:28:00,400 --> 00:28:04,720
Это всё ещё кажется лучше, чем модели.
Очевидно, модели лучше, чем средний

292
00:28:04,720 --> 00:28:07,760
человек в языке, математике и кодинге.
Но лучше ли они, чем

293
00:28:07,760 --> 00:28:12,160
средний человек в обучении?
О да. О да, абсолютно. Что я хотел

294
00:28:12,160 --> 00:28:18,320
сказать, это то, что язык, математика и кодинг — и
особенно математика и кодинг — предполагают, что что бы

295
00:28:18,320 --> 00:28:25,920
это ни было, что делает людей хорошими в обучении, это,
вероятно, не столько сложный прайор,

296
00:28:25,920 --> 00:28:30,880
сколько что-то большее, какая-то фундаментальная вещь.
Я не уверен, что понял. Почему

297
00:28:30,880 --> 00:28:35,120
это должно быть так?
Рассмотрим навык, в котором

298
00:28:35,120 --> 00:28:45,120
люди демонстрируют какую-то большую надёжность.
Если навык — это тот, который был очень полезен нашим

299
00:28:45,120 --> 00:28:52,240
предкам на протяжении многих миллионов лет, сотен
миллионов лет, ты мог бы утверждать, что, может быть,

300
00:28:52,240 --> 00:29:00,720
люди хороши в этом из-за эволюции,
потому что у нас есть прайор, эволюционный прайор,

301
00:29:00,720 --> 00:29:06,400
который закодирован каким-то очень неочевидным
образом, что как-то делает нас такими хорошими в этом.

302
00:29:07,200 --> 00:29:16,560
Но если люди демонстрируют большую способность, надёжность,
робастность и способность учиться в области, которая

303
00:29:16,560 --> 00:29:23,760
действительно не существовала до недавнего времени, то
это скорее указывает на то, что у людей

304
00:29:23,760 --> 00:29:34,800
может быть просто лучшее машинное обучение, точка.
Как нам следует думать о том, что это такое? Какова

305
00:29:34,800 --> 00:29:41,200
ML-аналогия? Есть пара интересных
вещей в этом. Это требует меньше сэмплов. Это

306
00:29:41,200 --> 00:29:47,200
более неконтролируемо. Ребёнок, учащийся водить
машину… Дети не учатся водить машину.

307
00:29:47,200 --> 00:29:55,600
Подросток, учащийся водить машину, не
совсем получает какую-то готовую, проверяемую награду.

308
00:29:56,400 --> 00:30:01,760
Это происходит из их взаимодействия с
машиной и с окружающей средой.

309
00:30:02,720 --> 00:30:07,440
Это требует гораздо меньше сэмплов. Это кажется
более неконтролируемым. Это кажется более робастным?

310
00:30:07,440 --> 00:30:12,400
Гораздо более робастным. Робастность
людей действительно ошеломляющая.

311
00:30:14,160 --> 00:30:18,320
Есть ли у тебя единый способ думать о том,
почему все эти вещи происходят одновременно?

312
00:30:18,320 --> 00:30:23,920
Какова ML-аналогия, которая могла бы
реализовать что-то подобное?

313
00:30:26,320 --> 00:30:33,680
Одна из вещей, о которых ты спрашивал — это
как подросток-водитель может самокорректироваться и учиться

314
00:30:33,680 --> 00:30:41,600
на своём опыте без внешнего учителя?
Ответ в том, что у них есть их функция ценности.

315
00:30:41,600 --> 00:30:46,800
У них есть общее чувство, которое также,
кстати, чрезвычайно робастно у людей.

316
00:30:50,480 --> 00:30:56,080
Какова бы ни была человеческая функция ценности,
за некоторыми исключениями, связанными с зависимостью,

317
00:30:56,080 --> 00:31:00,800
она на самом деле очень, очень робастна.
Так что для кого-то вроде подростка,

318
00:31:00,800 --> 00:31:07,520
который учится водить, они начинают водить, и
у них уже есть чувство того, как они водят

319
00:31:07,520 --> 00:31:13,200
немедленно, как плохо у них получается, как неуверенно.
И затем они видят: "Окей". И затем, конечно,

320
00:31:13,200 --> 00:31:17,520
скорость обучения любого подростка так высока.
Через 10 часов ты готов ехать.

321
00:31:17,520 --> 00:31:19,760
Похоже, у людей есть какое-то
решение, но мне любопытно,
322
00:31:20,800 --> 00:31:24,960
как они это делают и почему это так сложно?
Как нам нужно переосмыслить то, как

323
00:31:24,960 --> 00:31:27,520
мы тренируем модели, чтобы сделать
что-то подобное возможным?

324
00:31:28,720 --> 00:31:37,200
Это отличный вопрос, и это
вопрос, по которому у меня много мнений.

325
00:31:37,200 --> 00:31:43,200
Но, к сожалению, мы живём в мире, где
не все идеи машинного обучения обсуждаются

326
00:31:43,200 --> 00:31:49,360
свободно, и это одна из них.
Вероятно, есть способ сделать это.

327
00:31:49,360 --> 00:31:54,240
Я думаю, это можно сделать.
Тот факт, что люди такие,

328
00:31:54,240 --> 00:31:57,840
я думаю, это доказательство того, что это можно сделать.
Однако может быть ещё один блокирующий фактор,

329
00:31:57,840 --> 00:32:07,760
который заключается в том, что существует вероятность, что
человеческие нейроны производят больше вычислений, чем мы думаем.

330
00:32:07,760 --> 00:32:13,760
Если это правда, и если это играет важную
роль, то всё может быть сложнее.

331
00:32:13,760 --> 00:32:20,080
Но независимо от этого, я действительно думаю, что это указывает на
существование некоего принципа машинного обучения,

332
00:32:20,080 --> 00:32:25,840
о котором у меня есть мнения.
Но, к сожалению, обстоятельства

333
00:32:25,840 --> 00:32:31,680
затрудняют детальное обсуждение.
Никто не слушает этот подкаст, Илья.

334
00:32:32,560 --> 00:35:53,360
Мне любопытно. Если ты говоришь, что мы вернулись в эпоху
исследований, ты был там с 2012 по 2020 год.

335
00:35:55,840 --> 00:36:00,640
Каким теперь будет вайб, если
мы вернёмся в эпоху исследований?

336
00:36:00,640 --> 00:36:05,920
Например, даже после AlexNet, количество
вычислений, которое использовалось для

337
00:36:05,920 --> 00:36:12,240
проведения экспериментов, продолжало расти, и
размер передовых систем продолжал расти.

338
00:36:13,440 --> 00:36:18,640
Думаешь ли ты теперь, что эта эпоха исследований всё ещё
будет требовать огромных объёмов вычислений?

339
00:36:19,760 --> 00:36:24,400
Думаешь ли ты, что это потребует возвращения
в архивы и чтения старых статей?

340
00:36:28,000 --> 00:36:34,960
Ты был в Google, OpenAI и Стэнфорде, этих
местах, когда там было больше вайба исследований?

341
00:36:34,960 --> 00:36:38,720
Чего нам стоит
ожидать в сообществе?

342
00:36:40,160 --> 00:36:49,600
Одно из последствий эпохи масштабирования — это то, что
масштабирование высосало весь воздух из комнаты.

343
00:36:53,120 --> 00:36:59,680
Поскольку масштабирование высосало весь воздух из
комнаты, все начали делать одно и то же.

344
00:36:59,680 --> 00:37:05,840
Мы дошли до точки, где мы находимся
в мире, где компаний больше,

345
00:37:05,840 --> 00:37:11,440
чем идей, причём намного.
Кстати об этом, есть такая поговорка в Кремниевой

346
00:37:11,440 --> 00:37:18,480
Долине, которая гласит, что идеи
дешевы, исполнение — это всё.

347
00:37:18,480 --> 00:37:25,280
Люди часто это говорят, и в этом есть правда.
Но потом я видел, как кто-то сказал в Твиттере

348
00:37:25,280 --> 00:37:30,880
что-то вроде: "Если идеи так дешевы,
как так вышло, что ни у кого нет идей?"

349
00:37:30,880 --> 00:37:37,840
И я думаю, это тоже правда.
Если думать о прогрессе исследований с точки зрения

350
00:37:37,840 --> 00:37:47,200
узких мест, есть несколько узких мест.
Одно из них — идеи, и одно из них — твоя

351
00:37:47,200 --> 00:37:52,240
способность воплотить их в жизнь, что
может быть вычислениями, но также и инженерией.

352
00:37:52,240 --> 00:37:56,880
Если вернуться в 90-е, скажем,
были люди, у которых были довольно хорошие идеи,

353
00:37:56,880 --> 00:38:01,120
и если бы у них были гораздо большие компьютеры, может быть, они
могли бы продемонстрировать, что их идеи жизнеспособны.

354
00:38:01,120 --> 00:38:05,200
Но они не могли, поэтому они могли иметь только
очень, очень маленькую демонстрацию,

355
00:38:05,200 --> 00:38:10,480
которая никого не убеждала. Так что узким
местом были вычисления. Затем в

356
00:38:10,480 --> 00:38:17,120
эпоху масштабирования вычисления сильно выросли.
Конечно, есть вопрос о том, сколько

357
00:38:17,120 --> 00:38:26,640
вычислений нужно, но вычисления большие.
Вычисления достаточно большие, так что

358
00:38:26,640 --> 00:38:33,520
неочевидно, что тебе нужно намного больше
вычислений, чтобы доказать какую-то идею. Я дам

359
00:38:33,520 --> 00:38:40,640
тебе аналогию. AlexNet был построен на двух GPU.
Это был общий объём вычислений, использованный для него.

360
00:38:40,640 --> 00:38:48,720
Трансформер был построен на от 8 до 64 GPU.
Ни один эксперимент в статье про трансформер не использовал

361
00:38:48,720 --> 00:38:57,200
более 64 GPU образца 2017 года, что было бы
как, что, два сегодняшних GPU? ResNet,

362
00:38:57,200 --> 00:39:08,000
верно? Можно утверждать, что рассуждения o1 были
не самой тяжёлой по вычислениям вещью в мире.

363
00:39:08,000 --> 00:39:17,200
Так что для исследований тебе определённо нужно
некоторое количество вычислений, но далеко

364
00:39:17,200 --> 00:39:22,160
не очевидно, что тебе нужно абсолютно
наибольшее количество вычислений для исследований.

365
00:39:22,160 --> 00:39:28,800
Ты мог бы утверждать, и я думаю, это правда, что
если ты хочешь построить абсолютно лучшую систему,

366
00:39:31,360 --> 00:39:35,360
то тогда помогает иметь гораздо больше вычислений.
Особенно если все находятся в рамках одной

367
00:39:35,360 --> 00:39:42,000
парадигмы, тогда вычисления становятся
одним из больших дифференциаторов.

368
00:39:46,400 --> 00:39:48,400
Я спрашиваю тебя об истории,
потому что ты на самом деле был там.

369
00:39:48,400 --> 00:39:51,680
Я не уверен, что на самом деле произошло.
Звучит так, будто было возможно развить

370
00:39:51,680 --> 00:39:56,480
эти идеи, используя минимальные объёмы вычислений.
Но трансформер не

371
00:39:56,480 --> 00:39:59,840
сразу стал знаменитым.
Он стал тем, что все начали

372
00:39:59,840 --> 00:40:04,320
делать, а затем начали экспериментировать поверх этого
и строить поверх этого, потому что он был валидирован

373
00:40:04,320 --> 00:40:07,360
на всё более и более высоких уровнях вычислений.
Верно.

374
00:40:07,360 --> 00:40:13,840
И если у вас в SSI есть 50 разных идей, как
вы узнаете, какая из них — следующий трансформер,

375
00:40:13,840 --> 00:40:21,920
а какая хрупкая, не имея тех видов
вычислений, которые есть у других передовых лабораторий?

376
00:40:22,960 --> 00:40:30,320
Я могу прокомментировать это. Короткий
комментарий заключается в том, что ты упомянул SSI.

377
00:40:30,320 --> 00:40:40,640
Конкретно для нас, объём вычислений,
который есть у SSI для исследований, на самом деле не такой уж

378
00:40:40,640 --> 00:40:45,200
маленький. Я хочу объяснить почему. Простая математика
может объяснить, почему объём вычислений, который

379
00:40:45,200 --> 00:40:58,880
у нас есть, сопоставим для исследований, чем можно
подумать. Я объясню. SSI привлекла 3 миллиарда долларов,

380
00:40:58,880 --> 00:41:05,440
что много в любом абсолютном смысле.
Но ты мог бы сказать: "Посмотри на

381
00:41:05,440 --> 00:41:13,920
другие компании, привлекающие гораздо больше".
Но много их вычислений идёт на инференс.

382
00:41:13,920 --> 00:41:20,160
Эти большие цифры, эти большие займы, это
предназначено для инференса. Это номер один.

383
00:41:20,160 --> 00:41:25,120
Номер два, если ты хочешь иметь продукт,
на котором ты делаешь инференс, тебе нужно

384
00:41:25,120 --> 00:41:31,120
иметь большой штат инженеров, продавцов.
Много исследований должно быть посвящено

385
00:41:31,120 --> 00:41:37,440
созданию всевозможных функций, связанных с продуктом.
Так что когда ты смотришь на то, что на самом деле остаётся для

386
00:41:37,440 --> 00:41:45,760
исследований, разница становится намного меньше.
Другая вещь — если ты делаешь что-то

387
00:41:45,760 --> 00:41:51,040
другое, действительно ли тебе нужен
абсолютно максимальный масштаб, чтобы доказать это?

388
00:41:51,040 --> 00:41:58,080
Я вообще не думаю, что это правда.
Я думаю, что в нашем случае у нас достаточно

389
00:41:58,080 --> 00:42:02,320
вычислений, чтобы доказать, убедить себя и
кого угодно ещё, что то, что мы делаем, правильно.

390
00:42:02,880 --> 00:42:08,320
Были публичные оценки, что компании
вроде OpenAI тратят порядка 5-6 миллиардов долларов

391
00:42:08,320 --> 00:42:13,680
в год только пока что, на эксперименты.
Это отдельно от количества

392
00:42:13,680 --> 00:42:18,720
денег, которые они тратят на инференс и так далее.
Так что похоже, они тратят больше в год

393
00:42:18,720 --> 00:42:22,960
на проведение исследовательских экспериментов, чем
у вас, ребята, есть общего финансирования.

394
00:42:22,960 --> 00:42:26,880
Я думаю, это вопрос того, что ты с этим делаешь.
Это вопрос того, что ты с этим делаешь.

395
00:42:29,840 --> 00:42:35,120
В их случае, в случае других, существует
гораздо больший спрос на тренировочные вычисления.

396
00:42:35,120 --> 00:42:41,120
Там гораздо больше разных рабочих потоков, есть
разные модальности, там просто больше

397
00:42:41,120 --> 00:42:46,320
всего. Так что это становится фрагментированным.
Как SSI будет зарабатывать деньги?

398
00:42:48,480 --> 00:42:55,680
Мой ответ на этот вопрос примерно такой.
Прямо сейчас мы просто фокусируемся на исследованиях, а затем

399
00:42:55,680 --> 00:43:01,360
ответ на этот вопрос откроется сам собой.
Я думаю, будет много возможных ответов.

400
00:43:01,360 --> 00:43:05,040
План SSI всё ещё — прямой
путь к сверхителлекту?

401
00:43:05,040 --> 00:43:11,280
Может быть. Я думаю, что в этом есть заслуга.
Я думаю, в этом много заслуги, потому что

402
00:43:11,280 --> 00:43:17,840
очень приятно не быть затронутым
повседневной рыночной конкуренцией.

403
00:43:17,840 --> 00:43:25,200
Но я думаю, есть две причины,
которые могут заставить нас изменить план.

404
00:43:25,200 --> 00:43:31,360
Одна прагматическая, если сроки окажутся
длинными, что может быть.

405
00:43:31,360 --> 00:43:38,400
Вторая, я думаю, есть большая
ценность в том, чтобы лучший и самый

406
00:43:38,400 --> 00:43:46,480
мощный ИИ был там, влияя на мир.
Я думаю, это значимо ценная вещь.

407
00:43:46,480 --> 00:43:49,440
Тогда почему ваш план по умолчанию —
прямой путь к сверхителлекту?

408
00:43:49,440 --> 00:43:54,880
Потому что звучит так, будто OpenAI, Anthropic, все
эти другие компании, их явное мышление

409
00:43:54,880 --> 00:44:01,040
таково: "Смотрите, у нас есть всё более слабые интеллекты,
к которым публика может привыкнуть и подготовиться".

410
00:44:01,040 --> 00:44:06,400
Почему потенциально лучше
построить сверхителлект напрямую?

411
00:44:06,400 --> 00:44:14,000
Я приведу аргументы за и против.
Аргумент за — это то, что одна из проблем,

412
00:44:14,000 --> 00:44:20,320
с которыми сталкиваются люди, когда они на рынке,
это то, что они должны участвовать в крысиных бегах.

413
00:44:20,320 --> 00:44:24,720
Крысиные бега довольно сложны в том,
что они подвергают тебя сложным

414
00:44:24,720 --> 00:44:32,000
компромиссам, которые тебе нужно делать.
Приятно сказать: "Мы изолируем себя

415
00:44:32,000 --> 00:44:38,160
от всего этого и просто сфокусируемся на исследованиях и
выйдем только когда будем готовы, и не раньше".

416
00:44:38,160 --> 00:44:43,600
Но контраргумент тоже валиден,
и это противоборствующие силы.

417
00:44:43,600 --> 00:44:50,880
Контраргумент: "Эй, для мира полезно
видеть мощный ИИ.

418
00:44:50,880 --> 00:44:53,600
Для мира полезно видеть мощный
ИИ, потому что это

419
00:44:53,600 --> 00:44:56,800
единственный способ, которым ты можешь это донести".
Ну, я полагаю, даже не просто то, что ты можешь

420
00:44:56,800 --> 00:44:59,760
донести идею—
Донести ИИ,

421
00:44:59,760 --> 00:45:04,080
не идею. Донести ИИ.
Что ты имеешь в виду, "донести ИИ"?

422
00:45:04,640 --> 00:45:10,080
Давай предположим, ты пишешь эссе об ИИ, и
эссе гласит: "ИИ будет таким, и ИИ будет

423
00:45:10,080 --> 00:45:13,760
сяким, и он будет этим".
Ты читаешь это и говоришь: "Окей,

424
00:45:13,760 --> 00:45:18,320
это интересное эссе".
Теперь предположим, ты видишь ИИ, делающий это,

425
00:45:18,320 --> 00:45:27,920
ИИ, делающий то. Это несравнимо. В принципе
я думаю, что есть большая польза от того, что ИИ

426
00:45:27,920 --> 00:45:35,600
находится в публичном доступе, и это было бы
причиной для нас не идти совсем прямым путём.

427
00:45:36,320 --> 00:45:40,320
Я полагаю, дело даже не в этом, но я
действительно думаю, что это важная часть этого.

428
00:45:40,320 --> 00:45:45,360
Другая большая вещь — это то, что я не могу придумать
другой дисциплины в человеческой инженерии и

429
00:45:45,360 --> 00:45:53,280
исследованиях, где конечный артефакт был сделан
безопаснее в основном просто через размышления о том,

430
00:45:53,280 --> 00:45:58,000
как сделать его безопасным, в отличие от того,
почему авиакатастроф на милю так

431
00:45:58,000 --> 00:46:02,560
намного меньше сегодня, чем десятилетия назад.
Почему намного сложнее найти баг в Linux,

432
00:46:02,560 --> 00:46:06,320
чем это было бы десятилетия назад?
Я думаю, это в основном потому, что эти

433
00:46:06,320 --> 00:46:11,040
системы были развёрнуты в мире.
Ты замечал сбои, эти сбои

434
00:46:11,040 --> 00:46:17,280
исправлялись, и системы становились более робастными.
Я не уверен, почему AGI и сверхчеловеческий интеллект

435
00:46:17,280 --> 00:46:23,600
были бы чем-то иным, особенно учитывая — и я
надеюсь, мы доберёмся до этого — кажется, что

436
00:46:23,600 --> 00:46:29,760
вред от сверхителлекта не просто в том, чтобы
иметь какого-то зловредного скрепочника там.

437
00:46:29,760 --> 00:46:34,640
Но это действительно мощная вещь, и мы даже
не знаем, как концептуализировать, как люди взаимодействуют

438
00:46:34,640 --> 00:46:39,760
с этим, что люди будут делать с этим.
Иметь постепенный доступ к этому кажется

439
00:46:40,880 --> 00:46:45,440
лучшим способом, может быть, распределить влияние
этого и помочь людям подготовиться к этому.

440
00:46:45,440 --> 00:46:52,720
Ну, я думаю, по этому пункту, даже в сценарии прямого
пути, ты всё равно делал бы постепенный

441
00:46:52,720 --> 00:47:01,760
выпуск этого, так я бы это представлял.
Постепенность была бы неотъемлемым

442
00:47:01,760 --> 00:47:04,560
компонентом любого плана.
Это просто вопрос того, что является первой

443
00:47:04,560 --> 00:47:11,680
вещью, которую ты выпускаешь за дверь. Это номер
один. Номер два, я полагаю, ты выступал

444
00:47:11,680 --> 00:47:17,680
за непрерывное обучение больше, чем другие люди,
и я на самом деле думаю, что это важная

445
00:47:17,680 --> 00:47:29,600
и правильная вещь. Вот почему. Я дам тебе
ещё один пример того, как язык влияет на мышление.

446
00:47:29,600 --> 00:47:35,840
В данном случае это будут два слова, которые
сформировали мышление каждого, я утверждаю.

447
00:47:37,520 --> 00:47:48,400
Первое слово: AGI. Второе слово: предобучение.
Давай я объясню. Термин AGI, почему этот

448
00:47:48,400 --> 00:47:55,120
термин существует? Это очень специфический термин. Почему
он существует? Есть причина. Причина,

449
00:47:55,120 --> 00:48:02,640
по которой термин AGI существует, по моему мнению, не
столько в том, что это очень важный, существенный

450
00:48:02,640 --> 00:48:14,160
дескриптор какого-то конечного состояния интеллекта, но
потому что это реакция на другой термин, который

451
00:48:14,160 --> 00:48:19,360
существовал, и этот термин — узкий ИИ.
Если вернуться в древнюю историю

452
00:48:19,360 --> 00:48:25,600
геймплея и ИИ, шашечного ИИ, шахматного
ИИ, ИИ компьютерных игр, все говорили:

453
00:48:25,600 --> 00:48:29,600
посмотрите на этот узкий интеллект.
Конечно, шахматный ИИ может победить Каспарова,

454
00:48:29,600 --> 00:48:34,720
но он не может делать ничего другого.
Он такой узкий, искусственный узкий интеллект.

455
00:48:34,720 --> 00:48:41,280
Так что в ответ, как реакция на это,
некоторые люди сказали: это не хорошо. Это

456
00:48:41,280 --> 00:48:50,800
так узко. Что нам нужно, так это общий ИИ,
ИИ, который может просто делать всё.

457
00:48:53,040 --> 00:48:59,360
Этот термин просто получил большую популярность.
Вторая вещь, которая получила большую популярность,

458
00:48:59,360 --> 00:49:03,120
это предобучение, конкретно
рецепт предобучения.

459
00:49:03,120 --> 00:49:12,960
Я думаю, то, как люди делают RL сейчас, возможно,
отменяет концептуальный отпечаток предобучения.

460
00:49:12,960 --> 00:49:17,520
Но у предобучения было это свойство. Ты
делаешь больше предобучения, и модель становится

461
00:49:17,520 --> 00:49:29,760
лучше во всём, более или менее равномерно.
Общий ИИ. Предобучение даёт AGI. Но

462
00:49:29,760 --> 00:49:36,400
вещь, которая случилась с AGI и предобучением,
это то, что в каком-то смысле они промахнулись мимо цели.

463
00:49:38,160 --> 00:49:43,360
Если подумать о термине "AGI",
особенно в контексте предобучения,

464
00:49:43,360 --> 00:49:53,760
ты поймёшь, что человек — это не AGI.
Да, определённо есть фундамент навыков,

465
00:49:53,760 --> 00:50:00,080
но человеку не хватает
огромного количества знаний.

466
00:50:00,080 --> 00:50:06,720
Вместо этого мы полагаемся на непрерывное обучение.
Так что когда ты думаешь о: "Окей,

467
00:50:06,720 --> 00:50:12,240
так давай предположим, что мы достигли успеха и мы
произвели какой-то безопасный сверхителлект".

468
00:50:12,240 --> 00:50:16,320
Вопрос в том, как ты его определяешь?
Где на кривой непрерывного

469
00:50:16,320 --> 00:50:20,640
обучения он будет находиться?
Я произвожу сверхителлектуального

470
00:50:20,640 --> 00:50:25,200
15-летнего, который очень жаждет действовать.
Он не знает очень многого вообще,

471
00:50:25,200 --> 00:50:29,280
отличный студент, очень жаждущий.
Ты иди и будь программистом,

472
00:50:29,280 --> 00:50:34,720
ты иди и будь врачом, иди и учись.
Так что ты мог бы представить, что развёртывание

473
00:50:34,720 --> 00:50:38,880
само по себе будет включать какой-то
период обучения методом проб и ошибок.

474
00:50:38,880 --> 00:50:43,520
Это процесс, в отличие от того,
что ты сбрасываешь готовую вещь.

475
00:50:44,240 --> 00:50:51,120
Я понимаю. Ты предполагаешь, что вещь,
на которую ты указываешь со сверхителлектом,

476
00:50:51,120 --> 00:50:58,560
это не какой-то законченный разум, который знает, как
делать каждую отдельную работу в экономике.

477
00:50:58,560 --> 00:51:05,120
Потому что то, как, скажем, оригинальный устав OpenAI
или что-то такое определяет AGI — это типа, он может делать

478
00:51:05,120 --> 00:51:11,680
каждую отдельную работу, каждую отдельную вещь, которую может делать человек.
Ты предлагаешь вместо этого разум, который может

479
00:51:11,680 --> 00:51:15,520
научиться делать каждую отдельную работу,
и это и есть сверхителлект.

480
00:51:15,520 --> 00:51:19,840
Да.
Но как только у тебя есть алгоритм обучения,

481
00:51:19,840 --> 00:51:25,200
он развёртывается в мире так же,
как человек-рабочий может присоединиться к организации.

482
00:51:25,200 --> 00:51:27,360
Именно.
Похоже, что одна из этих двух вещей

483
00:51:27,360 --> 00:51:35,440
может произойти, может быть, ни одна из них не произойдёт.
Первая: этот супер-эффективный алгоритм обучения

484
00:51:35,440 --> 00:51:40,400
становится сверхчеловеческим, становится таким же хорошим,
как ты, и потенциально даже лучше,

485
00:51:40,400 --> 00:51:45,920
в задаче ML-исследований.
В результате алгоритм

486
00:51:45,920 --> 00:51:50,560
сам по себе становится всё более и более сверхчеловеческим.
Другая — даже если этого не произойдёт,

487
00:51:50,560 --> 00:51:56,800
если у тебя есть одна модель — это явно
твоё видение — где экземпляры модели,

488
00:51:56,800 --> 00:52:00,800
которые развёрнуты по всей экономике, выполняя
разные работы, учась делать эти работы,

489
00:52:00,800 --> 00:52:05,520
непрерывно обучаясь на работе, перенимая
все навыки, которые мог бы перенять любой человек,

490
00:52:05,520 --> 00:52:10,480
но перенимая их все одновременно,
а затем объединяя их знания,

491
00:52:10,480 --> 00:52:15,440
у тебя по сути есть модель, которая функционально
становится сверхителлектуальной даже без какого-либо

492
00:52:15,440 --> 00:52:20,560
рекурсивного самосовершенствования в программном обеспечении.
Потому что теперь у тебя есть одна модель, которая может делать

493
00:52:20,560 --> 00:52:25,040
каждую отдельную работу в экономике, а люди
не могут объединять наши разумы таким же образом.

494
00:52:25,040 --> 00:52:28,640
Так ожидаешь ли ты какого-то взрыва
интеллекта от широкого развёртывания?

495
00:52:28,640 --> 00:52:37,280
Я думаю, что вероятно, что у нас
будет быстрый экономический рост.

496
00:52:37,280 --> 00:52:46,640
Я думаю, с широким развёртыванием есть два
аргумента, которые можно привести, и они противоречат друг другу.

497
00:52:46,640 --> 00:52:59,120
Один — это то, что как только ты действительно дойдёшь до точки, где
у тебя есть ИИ, который может учиться делать вещи быстро,

498
00:52:59,120 --> 00:53:07,280
и у тебя их много, тогда будет
сильная сила развёртывать их в экономике,

499
00:53:07,280 --> 00:53:13,360
если только не будет какого-то регулирования,
которое остановит это, что, кстати, может быть.

500
00:53:13,360 --> 00:53:19,120
Но идея очень быстрого
экономического роста в течение некоторого времени,

501
00:53:19,120 --> 00:53:25,440
я думаю, очень возможна от широкого развёртывания.
Вопрос в том, насколько быстрым он будет.

502
00:53:25,440 --> 00:53:30,720
Я думаю, это трудно знать, потому что, с одной
стороны, у тебя есть этот очень эффективный работник.

503
00:53:30,720 --> 00:53:36,160
С другой стороны, мир просто
действительно большой, и там много всего,

504
00:53:36,160 --> 00:53:41,600
и это всё движется с разной скоростью.
Но с другой стороны, теперь ИИ мог бы...

505
00:53:41,600 --> 00:53:47,920
Так что я думаю, очень быстрый экономический рост возможен.
Мы увидим всякие вещи, типа разных

506
00:53:47,920 --> 00:53:51,280
стран с разными правилами, и те,
у которых правила более дружелюбные,

507
00:53:51,280 --> 00:55:10,320
экономический рост будет быстрее. Трудно предсказать.
Мне кажется, что это очень шаткая

508
00:55:10,320 --> 00:55:14,080
ситуация, в которой можно оказаться.
В пределе,

509
00:55:14,080 --> 00:55:17,760
мы знаем, что это должно быть возможно.
Если у тебя есть что-то, что так же хорошо

510
00:55:17,760 --> 00:55:24,400
как человек в обучении, но что может объединять свои
мозги — объединять разные экземпляры так, как

511
00:55:24,400 --> 00:55:28,320
люди не могут объединяться — уже это кажется
вещью, которая должна быть физически возможна.

512
00:55:28,320 --> 00:55:30,800
Люди возможны, цифровые
компьютеры возможны.

513
00:55:30,800 --> 00:55:33,440
Тебе просто нужно и то и другое
объединить, чтобы произвести эту вещь.

514
00:55:33,440 --> 00:55:40,080
Также кажется, что такого рода
вещь чрезвычайно мощная.

515
00:55:41,440 --> 00:55:45,680
Экономический рост — это один способ сказать это.
Сфера Дайсона — это много экономического роста.

516
00:55:45,680 --> 00:55:50,480
Но другой способ сказать это — что у тебя будет,
потенциально за очень короткий период времени...

517
00:55:52,080 --> 00:55:55,200
Ты нанимаешь людей в SSI, и через шесть
месяцев они, вероятно, становятся нетто-продуктивными.

518
00:55:56,000 --> 00:56:00,640
Человек учится очень быстро, и эта штука
становится умнее и умнее очень быстро.

519
00:56:01,200 --> 00:56:05,840
Как ты думаешь о том, чтобы сделать так, чтобы это прошло хорошо?
Почему SSI позиционирована, чтобы сделать это хорошо?

520
00:56:05,840 --> 00:56:08,160
Каков план SSI там, вот
в принципе что я пытаюсь спросить.

521
00:56:12,160 --> 00:56:22,880
Один из способов, которым моё мышление
менялось, это то, что я теперь придаю больше значения

522
00:56:22,880 --> 00:56:34,960
тому, чтобы ИИ развёртывался постепенно и заранее.
Одна очень сложная вещь с ИИ — это то, что мы

523
00:56:34,960 --> 00:56:42,480
говорим о системах, которые ещё не
существуют, и их трудно представить.

524
00:56:43,600 --> 00:56:52,080
Я думаю, что одна из вещей, которая происходит, это
то, что на практике очень трудно почувствовать AGI.

525
00:56:52,080 --> 00:57:01,440
Очень трудно почувствовать AGI.
Мы можем говорить об этом, но представь,

526
00:57:01,440 --> 00:57:07,840
что у тебя разговор о том, каково это —
быть старым, когда ты старый и немощный.

527
00:57:07,840 --> 00:57:12,640
Ты можешь поговорить, ты можешь попытаться
представить это, но это просто трудно, и ты возвращаешься

528
00:57:12,640 --> 00:57:22,400
к реальности, где это не так.
Я думаю, что многие проблемы вокруг AGI

529
00:57:22,400 --> 00:57:30,960
и его будущей мощи проистекают из того факта,
что его очень трудно представить.

530
00:57:30,960 --> 00:57:37,680
Будущий ИИ будет другим. Он будет
мощным. Действительно, вся проблема,

531
00:57:37,680 --> 00:57:43,360
в чём проблема ИИ и AGI?
Вся проблема в мощи.

532
00:57:43,360 --> 00:57:48,320
Вся проблема в мощи.
Когда мощь действительно большая,

533
00:57:48,320 --> 00:57:53,120
что произойдёт?
Один из способов, которым я

534
00:57:53,120 --> 00:58:02,320
изменил своё мнение за последний год — и это
изменение мнения, я сделаю небольшую оговорку, может

535
00:58:02,320 --> 00:58:12,960
распространиться назад в планы нашей компании —
это то, что если это трудно представить, что ты делаешь?

536
00:58:12,960 --> 00:58:16,560
Ты должен показывать эту вещь.
Ты должен показывать эту вещь.

537
00:58:16,560 --> 00:58:24,400
Я утверждаю, что большинство людей, которые работают над ИИ, тоже
не могут представить это, потому что это слишком отличается от

538
00:58:24,400 --> 00:58:31,440
того, что люди видят на повседневной основе.
Я действительно утверждаю, вот что-то, что

539
00:58:31,440 --> 00:58:40,960
я предсказываю, случится. Это предсказание.
Я утверждаю, что по мере того, как ИИ становится более мощным,

540
00:58:40,960 --> 00:58:48,320
люди будут менять своё поведение.
Мы увидим всевозможные беспрецедентные

541
00:58:48,320 --> 00:58:56,720
вещи, которые не происходят прямо сейчас. Я
приведу несколько примеров. Я думаю, к лучшему или худшему,

542
00:58:57,440 --> 00:59:03,120
передовые компании будут играть очень важную
роль в том, что происходит, как и правительство.

543
00:59:03,120 --> 00:59:06,800
Вид вещей, которые, я думаю,
ты увидишь, начало которых ты видишь —

544
00:59:06,800 --> 00:59:15,920
это компании, которые являются яростными
конкурентами, начинают сотрудничать по безопасности ИИ.

545
00:59:15,920 --> 00:59:22,880
Ты мог видеть OpenAI и Anthropic, делающих
первый маленький шаг, но этого не существовало.

546
00:59:22,880 --> 00:59:27,360
Это то, что я предсказывал в
одном из моих выступлений года три назад,

547
00:59:27,360 --> 00:59:30,960
что такая вещь случится.
Я также утверждаю, что по мере того, как ИИ продолжает

548
00:59:30,960 --> 00:59:38,320
становиться более мощным, более видимо
мощным, также будет желание со стороны

549
00:59:38,320 --> 00:59:46,160
правительств и общественности что-то сделать.
Я думаю, это очень важная сила,

550
00:59:46,160 --> 00:59:51,600
показа ИИ. Это номер один.
Номер два, окей, итак, ИИ

551
00:59:51,600 --> 00:59:59,600
строится. Что нужно сделать? Одна вещь, которую
я утверждаю, случится — это то, что прямо сейчас,

552
00:59:59,600 --> 01:00:06,560
люди, которые работают над ИИ, я утверждаю, что
ИИ не ощущается мощным из-за его ошибок.

553
01:00:06,560 --> 01:00:10,880
Я действительно думаю, что в какой-то момент ИИ
начнёт ощущаться мощным на самом деле.

554
01:00:10,880 --> 01:00:18,160
Я думаю, когда это случится, мы увидим большое
изменение в том, как все ИИ-компании подходят к

555
01:00:18,160 --> 01:00:25,680
безопасности. Они станут гораздо более параноидальными.
Я говорю это как предсказание, которое мы

556
01:00:25,680 --> 01:00:30,320
увидим, сбудется. Посмотрим, прав ли я. Но я думаю,
это то, что случится, потому что они

557
01:00:30,320 --> 01:00:34,160
увидят, как ИИ становится более мощным.
Всё, что происходит прямо сейчас,

558
01:00:34,160 --> 01:00:42,080
я утверждаю, происходит потому, что люди смотрят на сегодняшний
ИИ, и трудно представить будущий ИИ.

559
01:00:42,640 --> 01:00:49,520
Есть третья вещь, которая должна произойти.
Я говорю об этом в более широких терминах,

560
01:00:49,520 --> 01:00:54,640
не только с точки зрения SSI,
потому что ты спросил меня о нашей компании.

561
01:00:54,640 --> 01:00:58,400
Вопрос в том, что
компании должны стремиться построить?

562
01:00:58,400 --> 01:01:02,160
Что они должны стремиться построить?
Была одна большая идея, на которой

563
01:01:04,000 --> 01:01:11,120
все зациклились, это
самосовершенствующийся ИИ. Почему это произошло?

564
01:01:11,120 --> 01:01:17,200
Потому что идей меньше, чем компаний.
Но я утверждаю, что есть что-то, что

565
01:01:17,200 --> 01:01:21,360
лучше построить, и я думаю,
что все захотят этого.

566
01:01:21,360 --> 01:01:28,880
Это ИИ, который надёжно выровнен на то, чтобы
заботиться о чувствующей жизни конкретно.

567
01:01:29,680 --> 01:01:35,280
Я думаю, в частности, есть аргумент в пользу того,
что будет легче построить

568
01:01:35,280 --> 01:01:40,640
ИИ, который заботится о чувствующей жизни, чем
ИИ, который заботится только о человеческой жизни,

569
01:01:40,640 --> 01:01:46,080
потому что ИИ сам будет чувствующим.
И если подумать о таких вещах, как зеркальные

570
01:01:46,080 --> 01:01:53,200
нейроны и человеческая эмпатия к животным, о которой ты
можешь сказать, что она недостаточно велика, но она существует.

571
01:01:53,200 --> 01:01:58,240
Я думаю, это эмерджентное свойство от
того факта, что мы моделируем других с помощью той

572
01:01:58,240 --> 01:02:03,680
же схемы, которую мы используем для моделирования себя,
потому что это самая эффективная вещь.

573
01:02:03,680 --> 01:02:08,880
Так что даже если ты заставил ИИ заботиться о
чувствующих существах — и мне на самом деле не

574
01:02:08,880 --> 01:02:10,720
ясно, что это то, что ты
должен пытаться делать, если ты решил

575
01:02:10,720 --> 01:02:16,480
проблему выравнивания — всё равно будет так,
что большинством чувствующих существ будут ИИ.

576
01:02:16,480 --> 01:02:19,360
Будут триллионы,
в конечном итоге квадриллионы ИИ.

577
01:02:19,360 --> 01:02:22,400
Люди будут очень маленькой
долей чувствующих существ.

578
01:02:23,280 --> 01:02:32,480
Так что мне не ясно, если цель — какой-то
человеческий контроль над этой будущей цивилизацией,

579
01:02:32,480 --> 01:02:39,680
что это лучший критерий.
Это правда. Возможно, это не

580
01:02:39,680 --> 01:02:53,120
лучший критерий. Я скажу две вещи. Номер
один, забота о чувствующей жизни, я думаю, в этом есть

581
01:02:53,120 --> 01:03:01,120
заслуга. Это следует рассмотреть. Я думаю,
было бы полезно, если бы был какой-то короткий

582
01:03:01,120 --> 01:03:10,240
список идей, которые компании, когда они в
этой ситуации, могли бы использовать. Это номер два.

583
01:03:10,240 --> 01:03:16,480
Номер три, я думаю, было бы действительно
материально полезно, если бы мощь

584
01:03:16,480 --> 01:03:23,680
самого мощного сверхителлекта была как-то ограничена,
потому что это решило бы многие из этих проблем.

585
01:03:23,680 --> 01:03:29,680
Вопрос о том, как это сделать, я не уверен, но я
думаю, это было бы материально полезно, когда ты

586
01:03:29,680 --> 01:03:35,360
говоришь о действительно, действительно мощных системах.
Прежде чем мы продолжим дискуссию о выравнивании,

587
01:03:35,360 --> 01:03:38,560
я хочу сделать дабл-клик на этом.
Сколько места наверху?

588
01:03:38,560 --> 01:03:44,400
Как ты думаешь о сверхителлекте?
Думаешь ли ты, используя эту идею эффективности обучения,

589
01:03:44,400 --> 01:03:48,880
может быть, он просто чрезвычайно быстр в
изучении новых навыков или новых знаний?

590
01:03:48,880 --> 01:03:54,320
У него просто больший пул стратегий?
Есть ли единое сплочённое "оно" в

591
01:03:54,320 --> 01:04:01,600
центре, которое более мощное или большое?
Если да, представляешь ли ты, что это будет

592
01:04:01,600 --> 01:04:05,120
вроде как богоподобным по сравнению с остальной человеческой
цивилизацией, или это просто ощущается как ещё один

593
01:04:05,120 --> 01:04:10,000
агент, или ещё один кластер агентов?
Это область, где у разных

594
01:04:10,000 --> 01:04:14,800
людей разные интуиции.
Я думаю, он будет очень мощным, безусловно.

595
01:04:16,240 --> 01:04:23,200
Что, я думаю, наиболее вероятно произойдёт,
это то, что будет несколько таких

596
01:04:23,200 --> 01:04:33,280
ИИ, созданных примерно в одно и то же время.
Я думаю, что если кластер достаточно большой — типа

597
01:04:33,280 --> 01:04:39,040
если кластер буквально размером с континент — эта
штука может быть действительно мощной, действительно.

598
01:04:39,040 --> 01:04:44,720
Если у тебя буквально есть кластер размером
с континент, эти ИИ могут быть очень мощными.

599
01:04:46,640 --> 01:04:51,680
Всё, что я могу тебе сказать, это то, что если ты
говоришь о чрезвычайно мощных ИИ,

600
01:04:51,680 --> 01:04:59,760
по-настоящему драматически мощных, было бы неплохо, если бы
они могли быть сдержаны в некоторых отношениях или если бы было

601
01:04:59,760 --> 01:05:11,200
какое-то соглашение или что-то.
В чём беспокойство по поводу сверхителлекта?

602
01:05:11,200 --> 01:05:16,800
Какой один способ объяснить беспокойство?
Если ты представишь систему, которая достаточно

603
01:05:16,800 --> 01:05:23,440
мощная, действительно достаточно мощная — и ты
мог бы сказать, тебе нужно делать что-то разумное, типа

604
01:05:23,440 --> 01:05:29,440
заботиться о чувствующей жизни очень однонаправленным
образом — нам могут не понравиться результаты. Это действительно

605
01:05:29,440 --> 01:05:35,840
то, что это такое. Может быть, кстати, ответ в
том, что ты не строишь RL-агента в обычном

606
01:05:35,840 --> 01:05:42,800
смысле. Я укажу на несколько вещей. Я
думаю, человеческие существа — это полу-RL агенты.

607
01:05:43,600 --> 01:05:48,160
Мы преследуем награду, а затем эмоции
или что-то ещё заставляют нас уставать от

608
01:05:48,160 --> 01:05:55,760
награды, и мы преследуем другую награду.
Рынок — это очень близорукий вид

609
01:05:55,760 --> 01:05:59,600
агента. Эволюция такая же. Эволюция
очень умна в некоторых отношениях,

610
01:05:59,600 --> 01:06:03,040
но очень глупа в других.
Правительство было спроектировано

611
01:06:03,040 --> 01:06:08,320
как бесконечная борьба между
тремя частями, что имеет эффект.

612
01:06:08,320 --> 01:06:13,120
Так что я думаю о вещах вроде этого.
Ещё одна вещь, которая делает эту дискуссию

613
01:06:13,120 --> 01:06:19,600
сложной, это то, что мы говорим о системах,
которые не существуют, которые мы не знаем, как строить.

614
01:06:19,600 --> 01:06:21,760
Это другая вещь, и
это на самом деле моё убеждение.

615
01:06:21,760 --> 01:06:26,560
Я думаю, то, что люди делают прямо сейчас,
пройдёт какое-то расстояние, а затем выдохнется.

616
01:06:26,560 --> 01:06:30,080
Оно продолжит улучшаться,
но это также не будет "оно".

617
01:06:30,080 --> 01:06:38,960
"Оно" мы не знаем, как строить, и
многое зависит от понимания надёжной

618
01:06:38,960 --> 01:06:47,120
генерализации. Я скажу ещё одну вещь.
Одна из вещей, которую ты мог бы сказать о том,

619
01:06:47,120 --> 01:06:55,760
что делает выравнивание трудным, это то, что
твоя способность изучать человеческие ценности хрупка.

620
01:06:55,760 --> 01:07:00,080
Затем твоя способность оптимизировать их хрупка.
Ты на самом деле учишься оптимизировать их.

621
01:07:00,080 --> 01:07:06,640
И разве ты не можешь сказать: "Разве это всё не
примеры ненадёжной генерализации?"

622
01:07:06,640 --> 01:07:10,080
Почему так получается, что человеческие существа, кажется,
генерализуются настолько лучше?

623
01:07:10,080 --> 01:07:13,360
Что если бы генерализация была намного лучше?
Что случилось бы в этом случае? Каков был бы

624
01:07:13,360 --> 01:07:18,560
эффект? Но эти вопросы
сейчас всё ещё без ответа.

625
01:07:19,200 --> 01:07:24,640
Как думать о том, как
выглядит ИИ, когда всё идёт хорошо?

626
01:07:24,640 --> 01:07:28,480
Ты обрисовал, как ИИ может эволюционировать.
У нас будут эти своего рода агенты

627
01:07:28,480 --> 01:07:33,600
непрерывного обучения. ИИ будет очень мощным.
Может быть, будет много разных ИИ.

628
01:07:33,600 --> 01:07:40,800
Как ты думаешь о множестве интеллектов размером с континент,
ходящих вокруг? Насколько это

629
01:07:40,800 --> 01:07:49,840
опасно? Как нам сделать это менее опасным?
И как нам сделать это способом, который защищает

630
01:07:49,840 --> 01:07:56,240
равновесие, где могут быть невыровненные
ИИ там и плохие акторы там?

631
01:07:56,240 --> 01:08:00,960
Вот одна причина, почему мне понравился "ИИ,
который заботится о чувствующей жизни".

632
01:08:00,960 --> 01:08:09,520
Мы можем спорить о том, хорошо это или плохо.
Но если первые N из этих драматических

633
01:08:09,520 --> 01:08:17,520
систем действительно заботятся о, любят человечество
или что-то такое, заботятся о чувствующей жизни,

634
01:08:17,520 --> 01:08:23,840
очевидно, это тоже должно быть достигнуто. Это
должно быть достигнуто. Так что если это достигнуто

635
01:08:23,840 --> 01:08:32,960
первыми N из этих систем, тогда я могу
видеть, что всё идёт хорошо, по крайней мере, какое-то время.

636
01:08:32,960 --> 01:08:36,560
Затем есть вопрос о том,
что происходит в долгосрочной перспективе.

637
01:08:36,560 --> 01:08:44,960
Как достичь долгосрочного равновесия?
Я думаю, что там, там тоже есть ответ.

638
01:08:44,960 --> 01:08:49,200
Мне не нравится этот ответ, но
его нужно рассмотреть.

639
01:08:51,760 --> 01:08:57,120
В долгосрочной перспективе ты мог бы сказать: "Окей, если
у тебя есть мир, где существуют мощные ИИ,

640
01:08:57,120 --> 01:09:01,200
в краткосрочной перспективе ты мог бы сказать,
у тебя есть универсальный высокий доход.

641
01:09:01,200 --> 01:09:04,880
У тебя есть универсальный высокий доход,
и у нас всё хорошо".

642
01:09:04,880 --> 01:09:11,440
Но что говорят буддисты? "Перемены — это
единственная постоянная". Вещи меняются. Есть какое-то

643
01:09:11,440 --> 01:09:18,160
правительство, политическая структура, и
она меняется, потому что у этих вещей есть срок годности.

644
01:09:18,720 --> 01:09:22,560
Появляется какая-то новая правительственная штука, и
она функционирует, а затем через какое-то время

645
01:09:22,560 --> 01:09:25,600
она перестаёт функционировать.
Это то, что

646
01:09:25,600 --> 01:09:32,240
мы видим постоянно.
Так что я думаю, для долгосрочного равновесия,

647
01:09:32,240 --> 01:09:38,800
один подход — ты мог бы сказать, может быть, у каждого
человека будет ИИ, который будет исполнять его волю,

648
01:09:38,800 --> 01:09:41,040
и это хорошо.
Если бы это можно было

649
01:09:41,040 --> 01:09:47,040
поддерживать бесконечно, это правда.
Но недостаток этого в том, что тогда ИИ

650
01:09:47,040 --> 01:09:55,840
идёт и зарабатывает деньги для человека и отстаивает
его потребности в политической сфере, и, может быть,

651
01:09:55,840 --> 01:09:59,520
затем пишет маленький отчёт, говоря: "Окей,
вот что я сделал, вот ситуация",

652
01:09:59,520 --> 01:10:05,760
и человек говорит: "Отлично, продолжай в том же духе".
Но человек больше не участник.

653
01:10:05,760 --> 01:10:08,720
Тогда ты можешь сказать, что это
шаткое положение.

654
01:10:10,480 --> 01:10:16,880
Я начну с того, что скажу, что мне не
нравится это решение, но это решение.

655
01:10:19,040 --> 01:10:23,680
Решение — это если люди станут
частично ИИ с каким-то Neuralink++.

656
01:10:23,680 --> 01:10:27,920
Потому что то, что произойдёт в результате — это
то, что теперь ИИ понимает что-то,

657
01:10:27,920 --> 01:10:34,160
и мы понимаем это тоже, потому что теперь
понимание передаётся целиком.

658
01:10:34,160 --> 01:10:41,920
Так что теперь, если ИИ в какой-то ситуации, ты
вовлечён в эту ситуацию сам полностью.

659
01:10:41,920 --> 01:10:49,840
Я думаю, это ответ на равновесие.
Интересно, является ли тот факт, что эмоции, которые были

660
01:10:49,840 --> 01:10:56,160
развиты миллионы — или во многих случаях, миллиарды —
лет назад в совершенно другой среде,

661
01:10:56,160 --> 01:11:03,520
всё ещё направляют наши действия так сильно,
примером успеха выравнивания.

662
01:11:03,520 --> 01:11:11,520
Чтобы пояснить, что я имею в виду — я не знаю,
точнее ли называть это

663
01:11:11,520 --> 01:11:15,760
функцией ценности или функцией вознаграждения — но у
ствола мозга есть директива, где он говорит:

664
01:11:15,760 --> 01:11:19,600
"Спаривайся с кем-то, кто более успешен".
Кора — это та часть, которая понимает,

665
01:11:19,600 --> 01:11:25,200
что означает успех в современном контексте.
Но ствол мозга способен выровнять кору

666
01:11:25,200 --> 01:11:29,840
и сказать: "Как бы ты ни распознавала успех — и
я недостаточно умён, чтобы понять, что это —

667
01:11:29,840 --> 01:11:36,560
ты всё равно будешь следовать этой директиве".
Я думаю, есть более общий момент.

668
01:11:36,560 --> 01:11:46,960
Я думаю, это на самом деле действительно загадочно,
как эволюция кодирует высокоуровневые желания.

669
01:11:46,960 --> 01:11:51,920
Довольно легко понять, как
эволюция наделила бы нас

670
01:11:51,920 --> 01:11:58,400
желанием еды, которая хорошо пахнет, потому что запах —
это химическое вещество, так что просто преследуй это вещество.

671
01:11:58,400 --> 01:12:02,800
Очень легко представить, как
эволюция делает эту вещь.

672
01:12:02,800 --> 01:12:08,880
Но эволюция также наделила
нас всеми этими социальными желаниями.

673
01:12:08,880 --> 01:12:12,720
Нам действительно важно быть
воспринимаемыми обществом положительно.

674
01:12:12,720 --> 01:12:19,760
Нам важно быть на хорошем счету.
Все эти социальные интуиции, которые у нас есть,

675
01:12:19,760 --> 01:12:26,240
я сильно чувствую, что они вшиты.
Я не знаю, как эволюция сделала это,

676
01:12:26,240 --> 01:12:29,840
потому что это высокоуровневая концепция,
которая представлена в мозге.

677
01:12:31,360 --> 01:12:40,880
Скажем, тебе важна какая-то социальная вещь,
это не низкоуровневый сигнал, как запах.

678
01:12:40,880 --> 01:12:46,320
Это не что-то, для чего есть сенсор.
Мозгу нужно проделать много обработки, чтобы

679
01:12:46,320 --> 01:12:51,440
собрать воедино множество битов информации,
чтобы понять, что происходит в социальном плане.

680
01:12:51,440 --> 01:12:56,640
Как-то эволюция сказала: "Это то, о чём тебе следует
заботиться". Как она это сделала? Она сделала это быстро,

681
01:12:56,640 --> 01:13:04,400
к тому же. Все эти сложные социальные вещи, о которых мы
заботимся, я думаю, они эволюционировали довольно недавно.

682
01:13:04,400 --> 01:13:08,560
Эволюции было легко
жёстко закодировать это высокоуровневое желание.

683
01:13:12,000 --> 01:13:16,000
Я не знаю хорошей
гипотезы о том, как это сделано.

684
01:13:16,000 --> 01:13:23,920
У меня были некоторые идеи, которые я обдумывал,
но ни одна из них не является удовлетворительной.

685
01:13:24,560 --> 01:13:29,680
Что особенно впечатляет, так это то, что это было желание,
которое ты выучил за свою жизнь, это имеет смысл,

686
01:13:29,680 --> 01:13:32,560
потому что твой мозг разумен.
Имеет смысл, почему ты

687
01:13:32,560 --> 01:13:38,880
смог бы выучить разумные желания.
Может быть, это не твой аргумент, но один из способов

688
01:13:38,880 --> 01:13:44,240
понять это — это то, что желание встроено в
геном, а геном не разумен.

689
01:13:44,240 --> 01:13:50,160
Но ты как-то способен описать эту функцию.
Даже не ясно, как ты определяешь эту функцию,

690
01:13:50,160 --> 01:13:55,520
и ты можешь встроить её в гены.
По сути, или, может быть, я скажу по-другому.

691
01:13:55,520 --> 01:14:01,280
Если подумать об инструментах, которые
доступны геному, он говорит:

692
01:14:01,280 --> 01:14:05,760
"Окей, вот рецепт для построения мозга".
Ты мог бы сказать: "Вот рецепт для соединения

693
01:14:05,760 --> 01:14:10,560
дофаминовых нейронов с датчиком запаха".
И если запах определённого

694
01:14:10,560 --> 01:14:15,680
хорошего вида, ты хочешь это съесть.
Я мог бы представить, что геном делает это.

695
01:14:15,680 --> 01:14:21,120
Я утверждаю, что это труднее представить.
Труднее представить, что геном говорит,

696
01:14:21,120 --> 01:14:28,080
что тебе следует заботиться о каком-то сложном вычислении,
которое делает весь твой мозг, большой кусок твоего мозга.

697
01:14:28,080 --> 01:14:33,760
Это всё, что я утверждаю. Я могу рассказать
тебе спекуляцию о том, как это могло быть сделано.

698
01:14:33,760 --> 01:14:37,760
Позволь мне предложить спекуляцию, и я объясню,
почему спекуляция, вероятно, ложна.

699
01:14:37,760 --> 01:14:52,080
Итак, у мозга есть области мозга. У нас есть
наша кора. У неё есть все эти области мозга.

700
01:14:52,080 --> 01:14:57,040
Кора однородна, но области
мозга и нейроны в коре

701
01:14:57,040 --> 01:15:01,120
как бы говорят со своими соседями в основном.
Это объясняет, почему ты получаешь области мозга.

702
01:15:01,120 --> 01:15:04,640
Потому что если ты хочешь делать какую-то
обработку речи, все нейроны, которые

703
01:15:04,640 --> 01:15:08,000
занимаются речью, должны говорить друг с другом.
И поскольку нейроны могут говорить только со

704
01:15:08,000 --> 01:15:11,520
своими ближайшими соседями, по большей
части, это должна быть область.

705
01:15:11,520 --> 01:15:15,280
Все области в основном расположены в
одном и том же месте от человека к человеку.

706
01:15:15,280 --> 01:15:21,360
Так что, может быть, эволюция жёстко закодировала
буквально местоположение в мозге.

707
01:15:21,360 --> 01:15:27,920
Так что она говорит: "О, когда GPS-координаты
мозга такие-то и такие-то, когда это срабатывает,

708
01:15:27,920 --> 01:15:30,720
это то, о чём тебе следует заботиться".
Может быть, это то, что сделала эволюция, потому что

709
01:15:30,720 --> 01:15:36,000
это было бы в рамках инструментария эволюции.
Да, хотя есть примеры, когда,

710
01:15:36,000 --> 01:15:44,160
например, у людей, которые родились слепыми, эта
область их коры захвачена другим чувством.

711
01:15:44,960 --> 01:15:53,200
Я понятия не имею, но я был бы удивлён, если бы
желания или функции вознаграждения, которые требуют

712
01:15:53,200 --> 01:15:58,720
визуального сигнала, больше не работали для людей, у которых
разные области их коры кооптированы.

713
01:15:58,720 --> 01:16:05,680
Например, если у тебя больше нет зрения, можешь
ли ты всё ещё чувствовать, что я хочу, чтобы люди

714
01:16:05,680 --> 01:16:10,000
вокруг меня любили меня и так далее, для чего
обычно также есть визуальные подсказки.

715
01:16:10,000 --> 01:16:14,320
Я полностью согласен с этим. Я думаю, есть
ещё более сильный контраргумент этой теории.

716
01:16:16,880 --> 01:16:22,560
Есть люди, которым удаляют половину
мозга в детстве,

717
01:16:23,360 --> 01:16:27,760
и у них всё ещё есть все их области мозга.
Но они все как-то перемещаются в одно полушарие,

718
01:16:27,760 --> 01:16:32,240
что предполагает, что области мозга,
их местоположение не фиксировано, и поэтому

719
01:16:32,240 --> 01:16:34,240
эта теория не верна.
Было бы круто,

720
01:16:34,240 --> 01:16:37,680
если бы это было правдой, но это не так.
Так что я думаю, это загадка.

721
01:16:37,680 --> 01:16:43,600
Но это интересная загадка. Факт в том,
что как-то эволюция смогла наделить нас

722
01:16:43,600 --> 01:16:49,840
способностью заботиться о социальных вещах очень, очень надёжно.
Даже люди, у которых есть всевозможные странные ментальные

723
01:16:49,840 --> 01:16:54,240
состояния и недостатки и эмоциональные
проблемы, склонны заботиться об этом тоже.

724
01:18:13,360 --> 01:18:18,080
Что SSI планирует делать по-другому?
Предположительно, ваш план — быть одной из

725
01:18:18,080 --> 01:18:27,120
передовых компаний, когда это время придёт.
Предположительно, ты основал SSI, потому что ты типа:

726
01:18:27,120 --> 01:18:30,720
"Я думаю, у меня есть способ подхода к тому,
как сделать это безопасно, способом, которого у других

727
01:18:30,720 --> 01:18:37,760
компаний нет". В чём эта разница?
То, как я бы это описал — это то, что

728
01:18:37,760 --> 01:18:43,040
есть некоторые идеи, которые я считаю многообещающими, и
я хочу исследовать их и посмотреть, действительно ли они

729
01:18:43,040 --> 01:18:48,960
многообещающие или нет. Это действительно так просто.
Это попытка. Если идеи окажутся

730
01:18:48,960 --> 01:19:01,040
верными — эти идеи, которые мы обсуждали
вокруг понимания генерализации — тогда я

731
01:19:01,040 --> 01:19:05,200
думаю, у нас будет что-то достойное.
Окажутся ли они верными? Мы

732
01:19:05,200 --> 01:19:10,720
занимаемся исследованиями. Мы определённо компания "эпохи
исследований". Мы делаем прогресс. Мы

733
01:19:10,720 --> 01:19:14,720
на самом деле сделали довольно хороший прогресс за последний
год, но нам нужно продолжать делать больше прогресса,

734
01:19:14,720 --> 01:19:25,920
больше исследований. Вот как я это вижу. Я вижу это
как попытку быть голосом и участником.

735
01:19:29,840 --> 01:19:37,440
Твой сооснователь и предыдущий CEO ушёл в
Meta недавно, и люди спрашивали: "Ну,

736
01:19:37,440 --> 01:19:40,960
если бы делалось много прорывов, это кажется вещью, которая должна

737
01:19:40,960 --> 01:19:49,200
была быть маловероятной". Интересно, как ты ответишь.
На это я просто напомню несколько фактов, которые,

738
01:19:49,200 --> 01:19:52,640
возможно, были забыты.
Я думаю, эти факты, которые

739
01:19:52,640 --> 01:19:59,200
предоставляют контекст, объясняют ситуацию.
Контекст был таков, что мы привлекали средства при

740
01:19:59,200 --> 01:20:10,720
оценке в 32 миллиарда долларов, и затем Meta пришла
и предложила приобрести нас, и я сказал нет.

741
01:20:10,720 --> 01:20:19,920
Но мой бывший сооснователь в каком-то смысле сказал да.
В результате он также смог насладиться большой

742
01:20:19,920 --> 01:20:24,880
краткосрочной ликвидностью, и он был
единственным человеком из SSI, кто присоединился к Meta.

743
01:20:25,440 --> 01:20:31,200
Звучит так, будто план SSI — быть компанией,
которая находится на передовой, когда вы дойдёте до этого

744
01:20:31,200 --> 01:20:35,600
очень важного периода в человеческой истории,
где у вас есть сверхчеловеческий интеллект.

745
01:20:35,600 --> 01:20:39,360
У вас есть эти идеи о том, как
сделать так, чтобы сверхчеловеческий интеллект прошёл хорошо.

746
01:20:39,360 --> 01:20:42,480
Но другие компании будут
пробовать свои собственные идеи.

747
01:20:42,480 --> 01:20:48,400
Что отличает подход SSI
к тому, чтобы сверхителлект прошёл хорошо?

748
01:20:48,400 --> 01:20:54,080
Главное, что отличает
SSI — это её технический подход.

749
01:20:54,880 --> 01:21:01,520
У нас есть другой технический подход, который
я считаю достойным, и мы его преследуем.

750
01:21:01,520 --> 01:21:06,160
Я утверждаю, что в конце концов будет
конвергенция стратегий.

751
01:21:06,160 --> 01:21:14,960
Я думаю, будет конвергенция стратегий,
где в какой-то момент, по мере того как ИИ становится мощнее,

752
01:21:14,960 --> 01:21:19,520
всем станет более или менее яснее,
какой должна быть стратегия.

753
01:21:19,520 --> 01:21:24,800
Это должно быть что-то вроде: вам нужно найти
какой-то способ говорить друг с другом, и вы хотите,

754
01:21:24,800 --> 01:21:37,920
чтобы ваш первый настоящий реальный сверхителлектуальный ИИ
был выровнен и как-то заботился о чувствующей жизни,

755
01:21:37,920 --> 01:21:42,560
заботился о людях, демократичный, одно
из этого, какая-то комбинация этого.

756
01:21:42,560 --> 01:21:50,400
Я думаю, это условие, к которому
каждый должен стремиться.

757
01:21:50,400 --> 01:21:57,040
Это то, к чему стремится SSI.
Я думаю, что в этот раз, если не уже,

758
01:21:57,040 --> 01:22:00,320
все другие компании поймут, что
они стремятся к тому же самому.

759
01:22:00,320 --> 01:22:03,920
Посмотрим. Я думаю, что мир
действительно изменится, когда ИИ станет мощнее.

760
01:22:07,920 --> 01:22:11,680
Я думаю, вещи будут действительно другими, и
люди будут действовать действительно по-другому.

761
01:22:12,480 --> 01:22:16,560
Говоря о прогнозах, каковы твои
прогнозы до этой системы, которую ты описываешь,

762
01:22:16,560 --> 01:22:23,840
которая может учиться так же хорошо, как человек, и
впоследствии, в результате, стать сверхчеловеческой?

763
01:22:23,840 --> 01:22:27,600
Я думаю, типа от 5 до 20.
От 5 до 20 лет?

764
01:22:27,600 --> 01:22:29,760
Мгм.
Я просто хочу

765
01:22:29,760 --> 01:22:35,680
развернуть то, как ты можешь видеть наступление мира.
Это типа, у нас есть ещё пара лет, когда

766
01:22:35,680 --> 01:22:40,000
эти другие компании продолжают
текущий подход, и он глохнет.

767
01:22:40,000 --> 01:22:44,720
"Глохнет" здесь означает, что они зарабатывают не более
чем низкие сотни миллиардов выручки?

768
01:22:44,720 --> 01:22:57,200
Как ты думаешь о том, что значит "глохнет"?
Я думаю, "глохнет" будет выглядеть как… это

769
01:22:57,200 --> 01:23:00,788
всё будет выглядеть очень похоже у
всех разных компаний.

770
01:23:00,788 --> 01:23:02,640
Это может быть что-то вроде этого.
Я не уверен, потому что я думаю,

771
01:23:05,360 --> 01:23:10,320
даже заглохнув, я думаю, эти
компании могли бы сделать колоссальную выручку.

772
01:23:10,320 --> 01:23:15,280
Может быть, не прибыль, потому что им нужно будет
много работать, чтобы дифференцировать друг друга

773
01:23:15,280 --> 01:23:22,880
от самих себя, но выручку определённо.
Но что-то в твоей модели подразумевает, что

774
01:23:23,760 --> 01:23:27,920
когда правильное решение действительно появится,
будет конвергенция между всеми компаниями.

775
01:23:27,920 --> 01:23:31,440
Мне любопытно, почему ты думаешь, что это так.
Я говорил больше о конвергенции

776
01:23:31,440 --> 01:23:34,720
их стратегий выравнивания.
Я думаю, окончательная конвергенция

777
01:23:34,720 --> 01:23:38,800
технического подхода, вероятно, тоже
произойдёт, но я намекал

778
01:23:38,800 --> 01:23:43,680
на конвергенцию стратегий выравнивания.
Что именно является той вещью, которую нужно сделать?

779
01:23:43,680 --> 01:23:46,880
Я просто хочу лучше понять,
как ты видишь развёртывание будущего.

780
01:23:46,880 --> 01:23:50,320
В настоящее время у нас есть эти разные компании, и
ты ожидаешь, что их подход продолжит генерировать

781
01:23:50,320 --> 01:23:56,560
выручку, но не доберётся до этого человекоподобного ученика.
Так что теперь у нас есть эти разные ветви компаний.

782
01:23:56,560 --> 01:23:59,520
У нас есть вы, у нас есть Thinking Machines,
есть куча других лабораторий.

783
01:24:00,160 --> 01:24:03,120
Может быть, одна из них выяснит
правильный подход.

784
01:24:03,120 --> 01:24:07,600
Но тогда выпуск их продукта делает
ясным для других людей, как делать эту вещь.

785
01:24:07,600 --> 01:24:11,920
Я думаю, не будет ясно, как это делать, но
будет ясно, что что-то другое

786
01:24:11,920 --> 01:24:17,600
возможно, и это информация.
Люди будут затем пытаться

787
01:24:17,600 --> 01:24:26,560
выяснить, как это работает.
Я действительно думаю, однако, что одна из вещей, не

788
01:24:26,560 --> 01:24:34,320
затронутых здесь, не обсуждаемых, это то, что с каждым
увеличением возможностей ИИ, я думаю, будут

789
01:24:34,320 --> 01:24:40,960
какие-то изменения, но я не знаю
точно какие, в том, как делаются дела.

790
01:24:42,960 --> 01:24:47,280
Я думаю, это будет важно, но
я не могу сформулировать, что это именно.

791
01:24:50,320 --> 01:24:55,360
По умолчанию ты бы ожидал, что компания, у которой
есть эта модель, будет получать все эти выгоды,

792
01:24:55,360 --> 01:25:02,480
потому что у них есть модель, у которой есть навыки
и знания, которые она накапливает в мире.

793
01:25:02,480 --> 01:25:05,760
Какова причина думать, что выгоды
от этого будут широко распределены, а не

794
01:25:05,760 --> 01:25:11,200
просто окажутся у любой модельной компании, которая
запустит этот цикл непрерывного обучения первой?

795
01:25:14,880 --> 01:25:25,680
Вот что я думаю, произойдёт.
Номер один, давай посмотрим, как дела

796
01:25:25,680 --> 01:25:32,000
шли до сих пор с ИИ прошлого.
Одна компания произвела продвижение, и

797
01:25:32,000 --> 01:25:40,400
другая компания засуетилась и произвела какие-то похожие
вещи через какое-то время, и они начали

798
01:25:40,400 --> 01:25:48,640
конкурировать на рынке и толкать цены вниз.
Так что я думаю, с рыночной перспективы,

799
01:25:48,640 --> 01:25:54,800
что-то похожее произойдёт и там.
Мы говорим о хорошем мире, кстати.

800
01:25:56,640 --> 01:26:08,000
Что такое хороший мир? Это где у нас есть эти
мощные человекоподобные ученики, которые также… Кстати,

801
802
01:26:12,160 --> 01:26:16,320
я думаю, будет специализация.
Я думаю, будет специализация.

803
01:26:16,320 --> 01:26:22,080
Я думаю, что то, что произойдёт, это то, что
разные компании будут хороши в разных вещах.

804
01:26:22,080 --> 01:26:27,200
Некоторые компании будут хороши в
ИИ-врачах, некоторые компании будут хороши в

805
01:26:27,200 --> 01:26:30,800
ИИ-юристах, некоторые компании будут
хороши в ИИ-инженерах.

806
01:26:30,800 --> 01:26:36,000
Так что будет специализация, и это
будет ещё одна причина, почему будет много игроков.

807
01:26:36,000 --> 01:26:41,200
Я хочу закончить на теме исследовательского вкуса.
Ты упомянул, что у тебя есть этот другой

808
01:26:41,200 --> 01:26:45,440
технический подход, который ты пробуешь в SSI.
Очевидно, ты не можешь поделиться деталями,

809
01:26:45,440 --> 01:26:51,040
но мне любопытно, что определяет твой вкус?
Что ты ищешь в идее?

810
01:26:51,040 --> 01:26:57,200
Что делает идею красивой для тебя?
Для меня красивая идея — это та,

811
01:26:57,200 --> 01:27:04,400
которая объясняет многое очень просто.
Это то, что мне нравится.

812
01:27:04,400 --> 01:27:10,240
Мне нравятся идеи, которые объясняют
почему, которые дают тебе "почему".

813
01:27:10,240 --> 01:27:15,360
Если у тебя есть "почему", тогда ты можешь
понять, что происходит.

814
01:27:15,360 --> 01:27:21,680
Если у тебя нет "почему", ты просто
тычешь в темноте.

815
01:27:21,680 --> 01:27:26,800
Так что мне нравятся идеи, которые дают "почему",
и мне нравятся идеи, которые просты.

816
01:27:26,800 --> 01:27:32,480
Потому что если идея проста, она
вероятно верна.

817
01:27:32,480 --> 01:27:36,480
Если идея сложная, она вероятно неверна.
Это бритва Оккама.

818
01:27:36,480 --> 01:27:42,240
Но это также то, что я видел в своей карьере.
Самые успешные идеи были

819
01:27:42,240 --> 01:27:46,640
самыми простыми.
Глубокое обучение очень простое.

820
01:27:46,640 --> 01:27:51,200
Это просто умножение матриц и
нелинейности. Это всё.

821
01:27:51,200 --> 01:27:56,320
И ты просто складываешь их в стопку.
И ты делаешь это много раз.

822
01:27:56,320 --> 01:28:01,920
И ты используешь много данных.
Это очень просто.

823
01:28:01,920 --> 01:28:06,640
Масштабирование очень простое.
Просто сделай это больше.

824
01:28:06,640 --> 01:28:11,360
Это очень просто.
Так что мне нравятся простые идеи.

825
01:28:11,360 --> 01:28:16,080
Есть ли у тебя пример идеи, которая
была сложной и оказалась неверной?

826
01:28:16,080 --> 01:28:21,600
О, их так много.
Вся область компьютерного зрения

827
01:28:21,600 --> 01:28:27,360
до глубокого обучения была полна сложных идей.
Люди придумывали эти очень сложные

828
01:28:27,360 --> 01:28:32,240
дескрипторы признаков, SIFT и HOG, и
все эти вещи.

829
01:28:32,240 --> 01:28:37,280
Они были очень сложными.
И они работали немного, но они не

830
01:28:37,280 --> 01:28:41,440
работали хорошо.
И затем пришло глубокое обучение и просто

831
01:28:41,440 --> 01:28:46,000
смело их всех.
Потому что оно было простым и оно работало.

832
01:28:46,000 --> 01:28:50,560
Рич Саттон написал знаменитое эссе
"Горький урок".

833
01:28:50,560 --> 01:28:55,280
В нём он утверждает, что единственное, что
имеет значение в долгосрочной перспективе — это

834
01:28:55,280 --> 01:29:00,000
использование вычислений.
И что мы должны прекратить пытаться встроить

835
01:29:00,000 --> 01:29:04,720
наши человеческие знания в ИИ.
Согласен ли ты с этим?

836
01:29:04,720 --> 01:29:09,440
Я думаю, в этом много правды.
Я думаю, что история показала, что

837
01:29:09,440 --> 01:29:14,160
методы, которые масштабируются с вычислениями,
побеждают методы, которые не масштабируются.

838
01:29:14,160 --> 01:29:18,880
И методы, которые полагаются на человеческие знания,
обычно не масштабируются с вычислениями.

839
01:29:18,880 --> 01:29:23,600
Потому что человеческие знания фиксированы.
Ты не можешь просто добавить больше человеческих знаний

840
01:29:23,600 --> 01:29:27,360
так же легко, как ты можешь добавить больше вычислений.
Так что я думаю, Рич Саттон прав.

841
01:29:27,360 --> 01:29:32,080
Но есть нюанс.
Нюанс в том, что тебе всё ещё нужно

842
01:29:32,080 --> 01:29:36,800
иметь правильную архитектуру.
Тебе всё ещё нужно иметь правильный алгоритм.

843
01:29:36,800 --> 01:29:41,520
Ты не можешь просто бросить вычисления на что угодно.
Если ты бросишь вычисления на

844
01:29:41,520 --> 01:29:46,240
логистическую регрессию, ты не получишь AGI.
Так что тебе нужно правильное "что".

845
01:29:46,240 --> 01:29:50,960
И это "что" должно быть простым и масштабируемым.
Это мой вкус.

846
01:29:52,880 --> 01:29:57,600
Когда мы увидим этого человекоподобного ученика?
Каков твой прогноз?

847
01:29:57,600 --> 01:30:02,320
Я не хочу давать конкретную дату.
Но я думаю, что это не так уж далеко.

848
01:30:02,320 --> 01:30:07,040
Я думаю, мы увидим признаки этого
в ближайшие пару лет.

849
01:30:07,040 --> 01:30:11,760
Может быть, 2-3 года?
Что-то вроде того.

850
01:30:11,760 --> 01:30:16,480
И когда это случится, это будет
большим сдвигом парадигмы.

851
01:30:16,480 --> 01:30:21,200
Потому что внезапно у нас будут модели,
которые могут учиться так же, как мы.

852
01:30:21,200 --> 01:30:25,920
И это откроет дверь к
настоящему сверхителлекту.

853
01:30:25,920 --> 01:30:30,640
Потому что тогда мы сможем просто запустить их
и позволить им учиться и становиться умнее.

854
01:30:30,640 --> 01:30:35,360
И они смогут делать исследования.
И они смогут улучшать себя.

855
01:30:35,360 --> 01:30:40,080
И это будет началом
чего-то совершенно нового.

856
01:30:40,080 --> 01:30:44,800
Ты думаешь, текущая парадигма заглохнет
до того, как это случится?

857
01:30:44,800 --> 01:30:49,520
Я думаю, мы уже видим признаки этого.
Я думаю, что выгоды от простого

858
01:30:49,520 --> 01:30:54,240
масштабирования предобучения уменьшаются.
И люди начинают понимать, что

859
01:30:54,240 --> 01:30:58,960
нужно что-то ещё.
И это "что-то ещё" — это то, над чем мы работаем.

860
01:30:58,960 --> 01:31:03,680
И я думаю, другие тоже начнут работать над этим.
Так что да, я думаю, будет переход.

861
01:31:03,680 --> 01:31:08,400
Илья, спасибо большое за разговор.
Это было увлекательно.

862
01:31:08,400 --> 01:31:13,120
Спасибо, что пригласил меня.
Это было весело.

863
01:31:13,120 --> 01:31:17,840
(Конец записи)
