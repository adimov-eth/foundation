Знаешь, что безумно? Что всё это реально.

Что ты имеешь в виду?

Ты так не думаешь? Всё это про ИИ и вся эта Bay Area… что это происходит.

Разве это не прямо из научной фантастики?

Ещё одна безумная вещь — это то, насколько нормальным ощущается медленный взлёт.

Мысль о том, что мы будем инвестировать 1% ВВП в ИИ... мне кажется, это должно было ощущаться как что-то более грандиозное, а сейчас это просто ощущается...

Оказывается, мы довольно быстро ко всему привыкаем.

Но это также довольно абстрактно. Что это значит? Это значит, что ты видишь в новостях, что такая-то компания объявила о такой-то сумме в долларах. Это всё, что ты видишь.

Пока что это не ощущается как-то иначе.

Может, начнём с этого? Я думаю, это интересная дискуссия.

Конечно.

Я думаю, твой аргумент о том, что с точки зрения обычного человека ничего особо не изменилось, останется верным даже в момент сингулярности.

Нет, я так не думаю.

Окей, интересно.

То, что я имел в виду под "не ощущается иначе" — это, окей, такая-то компания объявила о какой-то сложно представимой сумме инвестиций.

Я не думаю, что кто-то знает, что с этим делать.

Но я думаю, что влияние ИИ будет ощутимым.

ИИ будет диффундировать через экономику.

Для этого будут очень сильные экономические силы, и я думаю, что влияние будет ощущаться очень сильно.

Когда ты ожидаешь это влияние?

Мне кажется, модели кажутся умнее, чем подразумевает их экономическое влияние.

Да. Это одна из очень сбивающих с толку вещей в моделях прямо сейчас.

Как примирить тот факт, что они так хорошо справляются с эвалами?

Ты смотришь на эвалы и думаешь: "Это довольно сложные эвалы". Они справляются так хорошо. Но экономическое влияние, кажется, драматически отстаёт.

В этом очень трудно разобраться: как модель может, с одной стороны, делать эти удивительные вещи, а с другой — повторять одно и то же дважды в какой-то ситуации?

Примером может быть, скажем, ты используешь вайб-кодинг, чтобы что-то сделать.

Ты заходишь куда-то и получаешь баг.

Затем ты говоришь модели: "Можешь, пожалуйста, исправить баг?" И модель говорит: "О боже, ты так прав. У меня баг. Дай-ка я исправлю это". И она вносит второй баг.

Затем ты говоришь ей: "У тебя этот новый второй баг", и она говорит тебе: "О боже, как я могла это сделать?

Ты снова так прав", и возвращает первый баг, и ты можешь чередовать их. Как это возможно? Я не уверен, но это действительно предполагает, что происходит что-то странное. У меня есть два возможных объяснения. Более причудливое объяснение заключается в том, что, возможно, RL-тренировка делает модели немного слишком однонаправленными и узко сфокусированными, немного слишком неосведомлёнными, даже если это также делает их осведомлёнными в других отношениях.

Из-за этого они не могут делать базовые вещи.

Но есть и другое объяснение. Когда люди занимались предобучением, вопрос о том, на каких данных тренировать, был решён, потому что ответом было "всё".

Когда ты делаешь предобучение, тебе нужны все данные.

Так что тебе не нужно думать, будут ли это эти данные или те данные.

Но когда люди делают RL-тренировку, им действительно нужно думать.

Они говорят: "Окей, мы хотим иметь этот вид RL-тренировки для этой вещи и тот вид RL-тренировки для той вещи".

Насколько я слышал, во всех компаниях есть команды, которые просто создают новые RL-среды и просто добавляют их в тренировочный микс.

Вопрос в том, что это такое?

Там так много степеней свободы.

Существует такое огромное разнообразие RL-сред, которые можно создать.

Одна вещь, которую можно сделать, и я думаю, что это делается непреднамеренно, это то, что люди черпают вдохновение из эвалов.

Ты говоришь: "Эй, я бы хотел, чтобы наша модель справлялась действительно хорошо, когда мы её выпустим.

Я хочу, чтобы эвалы выглядели отлично.

Какой может быть RL-тренировка, которая помогла бы в этой задаче?" Я думаю, это то, что происходит, и это может объяснить многое из того, что творится.

Если объединить это с тем, что генерализация моделей на самом деле неадекватна, это имеет потенциал объяснить многое из того, что мы видим, этот разрыв между результатами эвалов и реальной производительностью в мире, что является чем-то, что мы сегодня даже не понимаем, что мы под этим подразумеваем.

Мне нравится эта идея, что настоящий хакинг награды — это люди-исследователи, которые слишком сфокусированы на эвалах.

Я думаю, есть два способа понять, или попытаться подумать о том, что ты только что отметил.

Один — это если дело в том, что просто став сверхчеловеком в соревнованиях по кодингу, модель не станет автоматически обладать лучшим вкусом и проявлять лучшее суждение о том, как улучшить твою кодовую базу, ну тогда тебе следует расширить набор сред так, чтобы ты не просто тестировал её на наличие лучших результатов в соревнованиях по кодингу.

Она также должна быть способна создавать лучший вид приложения для вещи X, или вещи Y, или вещи Z.

Другой, может быть, это то, на что ты намекаешь, это сказать: "Почему вообще должно быть так, что становление сверхчеловеком в соревнованиях по кодингу не делает тебя более искусным программистом в целом?" Может быть, нужно не продолжать наращивать количество и разнообразие сред, а придумать подход, который позволит учиться на одной среде и улучшать свои показатели в чём-то другом.

У меня есть человеческая аналогия, которая может быть полезна. Давай возьмём случай спортивного программирования, раз уж ты упомянул это. Допустим, у тебя есть два студента. Один из них решил, что хочет быть лучшим спортивным программистом, поэтому он будет практиковаться 10 000 часов в этой области.

Он решит все задачи, запомнит все техники доказательств и будет очень искусен в быстрой и правильной реализации всех алгоритмов.

Сделав это, он стал одним из лучших.

Студент номер два подумал: "О, спортивное программирование — это круто".

Может быть, он практиковался 100 часов, гораздо меньше, и он тоже справился очень хорошо.

Кто из них, по-твоему, добьётся большего успеха в своей карьере позже?

Второй.

Верно. Я думаю, это в принципе то, что происходит.

Модели гораздо больше похожи на первого студента, но даже ещё больше.

Потому что тогда мы говорим: модель должна быть хороша в спортивном программировании, так что давай возьмём каждую задачу по спортивному программированию, которая когда-либо была. А затем давай сделаем аугментацию данных, чтобы у нас было ещё больше задач по спортивному программированию, и натренируем на этом.

Теперь у тебя есть этот великолепный спортивный программист. С этой аналогией, я думаю, это более интуитивно.

Да, окей, если она так хорошо натренирована, все разные алгоритмы и все разные техники доказательств у неё прямо под рукой.

И более интуитивно понятно, что с таким уровнем подготовки она не обязательно генерализуется на другие вещи.

Но тогда какова аналогия для того, что делает второй студент до того, как потратит 100 часов на файн-тюнинг?

Я думаю, у него есть "это". Фактор "это".

Когда я был студентом, я помню, был такой студент, который учился со мной, так что я знаю, что это существует.

Я думаю, интересно отличить "это" от того, что делает предобучение.

Один из способов понять то, что ты только что сказал о том, что не нужно выбирать данные при предобучении — это сказать, что это на самом деле не отличается от 10 000 часов практики.

Просто ты получаешь эти 10 000 часов практики бесплатно, потому что они уже где-то в распределении предобучения.

Но, может быть, ты предполагаешь, что на самом деле от предобучения не так уж много генерализации.

В предобучении просто так много данных, но оно не обязательно генерализуется лучше, чем RL.

Главная сила предобучения в том, что: А, его так много, и Б, тебе не нужно сильно думать о том, какие данные поместить в предобучение.

Это очень естественные данные, и они действительно включают в себя многое из того, что делают люди: мысли людей и множество особенностей.

Это как целый мир, спроецированный людьми в текст, и предобучение пытается захватить это, используя огромное количество данных.

О предобучении очень трудно рассуждать, потому что так сложно понять, каким образом модель полагается на данные предобучения.

Всякий раз, когда модель совершает ошибку, может ли это быть потому, что что-то случайно не так хорошо подкреплено данными предобучения? "Подкреплено предобучением" — это, возможно, неточный термин.

Я не знаю, могу ли я добавить что-то более полезное по этому поводу.

Я не думаю, что существует человеческий аналог предобучения.

Вот аналогии, которые люди предлагали для того, что является человеческим аналогом предобучения.

Мне любопытно узнать твои мысли о том, почему они потенциально неверны.

Одна — это думать о первых 18, или 15, или 13 годах жизни человека, когда он не обязательно экономически продуктивен, но делает что-то, что заставляет его лучше понимать мир и так далее.

Другая — думать об эволюции как о выполнении некоего поиска в течение 3 миллиардов лет, который затем приводит к экземпляру человеческой жизни.

Мне любопытно, считаешь ли ты какую-либо из них аналогичной предобучению.

Как бы ты думал о том, на что похоже пожизненное человеческое обучение, если не на предобучение?

Я думаю, есть некоторые сходства между обеими этими вещами и предобучением, и предобучение пытается играть роль обеих.

Но я думаю, что есть и некоторые большие различия.

Объём данных предобучения очень, очень ошеломляющий.

Да.

Каким-то образом человек, даже спустя 15 лет с крошечной долей данных предобучения, знает гораздо меньше.

Но то, что он знает, он знает как-то гораздо глубже.

Уже в этом возрасте ты бы не совершал ошибок, которые совершают наши ИИ. Есть ещё одна вещь. Ты можешь сказать, может ли это быть чем-то вроде эволюции? Ответ — возможно. Но в этом случае, я думаю, у эволюции может быть преимущество.

Я помню, читал об этом случае.

Один из способов, которым нейробиологи могут узнать о мозге — это изучение людей с повреждениями различных частей мозга.

У некоторых людей самые странные симптомы, которые только можно представить. Это на самом деле очень, очень интересно. Один случай, который приходит на ум, уместен здесь.

Я читал об этом человеке, у которого было какое-то повреждение мозга, инсульт или несчастный случай, который вывел из строя его эмоциональную обработку.

Так что он перестал чувствовать какие-либо эмоции.

Он всё ещё оставался очень красноречивым и мог решать маленькие головоломки, и на тестах он казался совершенно нормальным.

Но он не чувствовал эмоций. Он не чувствовал грусти, он не чувствовал гнева, он не чувствовал оживления.

Он стал как-то чрезвычайно плох в принятии каких-либо решений вообще.

Ему требовались часы, чтобы решить, какие носки надеть.

Он принимал очень плохие финансовые решения.

Что это говорит о роли наших встроенных эмоций в том, чтобы делать нас жизнеспособным агентом, по сути? Чтобы связать это с твоим вопросом о предобучении, может быть, если ты достаточно хорош в извлечении всего из предобучения, ты мог бы получить и это.

Но это та вещь, которая кажется...

Ну, это может быть возможно, а может и нет, получить это из предобучения.

Что такое "это"? Явно не просто напрямую эмоции. Это кажется чем-то почти похожим на функцию ценности, которая говорит тебе, какой должна быть конечная награда за любое решение.

Ты думаешь, это не приходит как бы неявно из предобучения?

Я думаю, могло бы. Я просто говорю, что это не на 100% очевидно.

Но что это? Как ты думаешь об эмоциях?

Какова ML-аналогия для эмоций?

Это должно быть что-то вроде функции ценности.

Но я не думаю, что есть отличная ML-аналогия, потому что прямо сейчас функции ценности не играют очень заметной роли в том, что делают люди.

Может быть, стоит определить для аудитории, что такое функция ценности, если ты хочешь это сделать.

Конечно, я буду очень рад это сделать.

Когда люди занимаются обучением с подкреплением, то, как обучение с подкреплением делается прямо сейчас, как люди тренируют этих агентов?

У тебя есть нейросеть, и ты даёшь ей задачу, а затем ты говоришь модели: "Иди реши её".

Модель предпринимает, может быть, тысячи, сотни тысяч действий или мыслей или чего-то ещё, а затем она выдаёт решение.

Решение оценивается. И затем оценка используется для предоставления тренировочного сигнала для каждого отдельного действия в твоей траектории.

Это означает, что если ты делаешь что-то, что длится долгое время — если ты тренируешь задачу, решение которой занимает много времени — обучения не будет вообще, пока ты не придёшь к предложенному решению.

Вот как обучение с подкреплением делается наивно.

Вот как o1, R1 якобы делаются.

Функция ценности говорит что-то вроде: "Может быть, я могла бы иногда, не всегда, говорить тебе, хорошо ты справляешься или плохо".

Понятие функции ценности более полезно в одних областях, чем в других.

Например, когда ты играешь в шахматы и ты теряешь фигуру, я облажался.

Тебе не нужно играть всю партию, чтобы знать, что то, что я только что сделал, было плохо, и, следовательно, всё, что этому предшествовало, тоже было плохо. Функция ценности позволяет тебе сократить ожидание до самого конца.

Давай предположим, что ты занимаешься какой-то математической вещью или программированием, и ты пытаешься исследовать конкретное решение или направление.

После, скажем, тысячи шагов размышлений, ты пришёл к выводу, что это направление бесперспективно.

Как только ты приходишь к этому выводу, ты уже мог бы получить сигнал награды тысячу временных шагов назад, когда ты решил пойти по этому пути.

Ты говоришь: "В следующий раз я не должен идти по этому пути в похожей ситуации", задолго до того, как ты на самом деле пришёл к предложенному решению.

Это было в статье DeepSeek R1 — что пространство траекторий настолько широко, что, возможно, трудно выучить отображение из промежуточной траектории и ценности.

И также учитывая, что в кодинге, например, у тебя будет неправильная идея, потом ты вернёшься назад, потом ты что-то изменишь.

Это звучит как такое отсутствие веры в глубокое обучение.

Конечно, это может быть трудно, но нет ничего, чего глубокое обучение не могло бы сделать.

Моё ожидание таково, что функция ценности должна быть полезной, и я полностью ожидаю, что они будут использоваться в будущем, если не уже.

То, на что я намекал с человеком, чей эмоциональный центр был повреждён, это скорее то, что, возможно, это предполагает, что функция ценности людей модулируется эмоциями каким-то важным образом, который жёстко закодирован эволюцией. И, может быть, это важно для того, чтобы люди были эффективны в мире.

Это то, о чём я планировал тебя спросить.

Есть что-то действительно интересное в эмоциях функции ценности, а именно то, что впечатляет, что они имеют такую большую полезность, оставаясь при этом довольно простыми для понимания.

У меня есть два ответа. Я согласен, что по сравнению с теми вещами, которые мы изучаем, и вещами, о которых мы говорим, видом ИИ, о котором мы говорим, эмоции относительно просты.

Они могут быть даже настолько простыми, что, возможно, ты мог бы отобразить их понятным для человека способом.

Я думаю, это было бы круто сделать.

Что касается полезности, однако, я думаю, здесь есть вещь, где существует этот компромисс между сложностью и надежностью, где сложные вещи могут быть очень полезными, но простые вещи очень полезны в очень широком диапазоне ситуаций.

Один из способов интерпретировать то, что мы видим, это то, что у нас есть эти эмоции, которые эволюционировали в основном от наших предков-млекопитающих, а затем были немного подстроены, пока мы были гоминидами, совсем чуть-чуть.

У нас действительно есть приличное количество социальных эмоций, которых, возможно, не хватает млекопитающим. Но они не очень сложные. И поскольку они не сложные, они служат нам так хорошо в этом очень разном мире по сравнению с тем, в котором мы жили. На самом деле, они также совершают ошибки. Например, наши эмоции… Ну на самом деле, я не знаю.

Считается ли голод эмоцией? Это спорно.

Но я думаю, например, наше интуитивное чувство голода не преуспевает в том, чтобы направлять нас правильно в этом мире с изобилием еды.

Люди говорят о масштабировании данных, масштабировании параметров, масштабировании вычислений. Есть ли более общий способ думать о масштабировании?

Каковы другие оси масштабирования?

Вот перспектива, которая, я думаю, может быть верной.

То, как ML работало раньше — это то, что люди просто возились с вещами и пытались получить интересные результаты.

Вот что происходило в прошлом. Затем пришло озарение масштабирования. Законы масштабирования, GPT-3, и внезапно все поняли, что мы должны масштабироваться.

Это пример того, как язык влияет на мышление. "Масштабирование" — это всего лишь одно слово, но это такое мощное слово, потому что оно сообщает людям, что делать.

Они говорят: "Давайте попробуем масштабировать вещи".

И ты говоришь: что мы масштабируем?

Предобучение было тем, что нужно масштабировать.

Это был конкретный рецепт масштабирования.

Большой прорыв предобучения — это осознание того, что этот рецепт хорош.

Ты говоришь: "Эй, если смешать немного вычислений с небольшим количеством данных в нейросети определённого размера, ты получишь результаты.

Ты будешь знать, что станешь лучше, если просто масштабируешь рецепт". Это тоже отлично.

Компании любят это, потому что это даёт очень низкорисковый способ инвестирования ресурсов.

Гораздо сложнее инвестировать свои ресурсы в исследования. Сравни это. Если ты исследуешь, тебе нужно типа: "Идите, исследователи, и исследуйте, и придумайте что-нибудь", против — получи больше данных, получи больше вычислений.

Ты знаешь, что получишь что-то от предобучения.

Действительно, похоже, основываясь на различных вещах, которые некоторые люди говорят в Твиттере, может быть, кажется, что Gemini нашли способ получить больше от предобучения.

В какой-то момент, однако, у предобучения закончатся данные.

Данные совершенно явно конечны. Что ты делаешь дальше? Либо ты делаешь какой-то вид прокачанного предобучения, другой рецепт, отличный от того, что ты делал раньше, либо ты делаешь RL, или, может быть, что-то ещё.

Но теперь, когда вычисления большие, вычисления теперь очень большие, в каком-то смысле мы вернулись в эпоху исследований.

Может быть, вот ещё один способ сказать это.

Вплоть до 2020 года, с 2012 по 2020, это была эпоха исследований.

Теперь, с 2020 по 2025, это была эпоха масштабирования — может быть, плюс-минус, давай добавим погрешности к этим годам — потому что люди говорят: "Это удивительно. Вы должны масштабироваться больше. Продолжайте масштабироваться". Одно слово: масштабирование. Но теперь масштаб такой большой.

Действительно ли вера такова: "О, это так много, но если бы у тебя было в 100 раз больше, всё было бы так по-другому?" Это было бы по-другому, безусловно.

Но такова ли вера, что если ты просто увеличишь масштаб в 100 раз, всё преобразится?

Я не думаю, что это правда. Так что мы снова вернулись в эпоху исследований, просто с большими компьютерами.

Это очень интересный способ сформулировать.

Но позволь мне задать тебе вопрос, который ты только что поставил.

Что мы масштабируем, и что значило бы иметь рецепт?

Я полагаю, я не знаю об очень чистой взаимосвязи, которая почти выглядит как закон физики, который существовал в предобучении.

Был степенной закон между данными или вычислениями или параметрами и лоссом.

Какого рода взаимосвязь мы должны искать, и как нам следует думать о том, как может выглядеть этот новый рецепт?

Мы уже стали свидетелями перехода от одного типа масштабирования к другому типу масштабирования, от предобучения к RL. Теперь люди масштабируют RL. Теперь, основываясь на том, что люди говорят в Твиттере, они тратят больше вычислений на RL, чем на предобучение в данный момент, потому что RL на самом деле может потреблять довольно много вычислений.

Ты делаешь очень длинные траектории, так что требуется много вычислений, чтобы создать эти траектории.

Затем ты получаешь относительно небольшое количество обучения на траекторию, так что ты действительно можешь потратить много вычислений.

Я бы даже не назвал это масштабированием.

Я бы сказал: "Эй, что вы делаете?

Является ли то, что вы делаете, самой продуктивной вещью, которую вы могли бы делать?

Можете ли вы найти более продуктивный способ использования ваших вычислений?" Мы обсуждали дело с функцией ценности ранее.

Может быть, как только люди станут хороши в функциях ценности, они будут использовать свои ресурсы более продуктивно.

Если ты найдёшь совершенно другой способ тренировки моделей, ты мог бы сказать: "Это масштабирование или это просто использование твоих ресурсов?" Я думаю, это становится немного двусмысленным.

В том смысле, что, когда люди были в эпохе исследований тогда, это было: "Давайте попробуем это и это и это.

Давайте попробуем то и то и то.

О, смотрите, происходит что-то интересное".

Я думаю, будет возвращение к этому.

Если мы снова в эре исследований, отступая назад, какая часть рецепта — та, о которой нам нужно думать больше всего?

Когда ты говоришь "функция ценности", люди уже пробуют текущий рецепт, но затем имеют LLM-как-судью и так далее.

Ты мог бы сказать, что это функция ценности, но звучит так, будто у тебя на уме что-то гораздо более фундаментальное.

Должны ли мы вообще переосмыслить предобучение, а не просто добавлять больше шагов в конец этого процесса?

Дискуссия о функции ценности, я думаю, она была интересной.

Я хочу подчеркнуть, что я думаю, что функция ценности — это то, что сделает RL более эффективным, и я думаю, что это имеет значение.

Но я думаю, что всё, что ты можешь сделать с функцией ценности, ты можешь сделать и без неё, просто медленнее.

Вещь, которая, я думаю, является наиболее фундаментальной, это то, что эти модели как-то просто генерализуются драматически хуже, чем люди. Это супер очевидно. Это кажется очень фундаментальной вещью.

Так что это суть: генерализация. Есть два подвопроса. Есть один, который касается эффективности использования данных: почему требуется так много данных для этих моделей, чтобы учиться, по сравнению с людьми? Есть второй вопрос. Даже отдельно от количества данных, которое это занимает, почему так трудно научить модель тому, что мы хотим, по сравнению с человеком?

Для человека нам не обязательно нужна проверяемая награда, чтобы быть в состоянии… Ты, вероятно, менторишь кучу исследователей прямо сейчас, и ты разговариваешь с ними, ты показываешь им свой код, и ты показываешь им, как ты думаешь.

Из этого они перенимают твой образ мышления и то, как им следует проводить исследования.

Тебе не нужно устанавливать для них проверяемую награду, которая типа: "Окей, это следующая часть учебного плана, а теперь это следующая часть твоего учебного плана. О, эта тренировка была нестабильной". Нет этого утомительного, индивидуального процесса.

Возможно, эти две проблемы на самом деле связаны каким-то образом, но мне было бы любопытно исследовать эту вторую вещь, которая больше похожа на непрерывное обучение, и эту первую вещь, которая ощущается просто как эффективность использования данных. Ты мог бы на самом деле задаться вопросом, что одно из возможных объяснений человеческой эффективности использования данных, которое нужно рассмотреть — это эволюция.

Эволюция дала нам небольшое количество наиболее полезной информации.

Для таких вещей, как зрение, слух и локомоция, я думаю, есть довольно сильный аргумент, что эволюция дала нам многое.

Например, человеческая ловкость намного превосходит… Я имею в виду роботы тоже могут стать ловкими, если подвергнуть их огромному количеству тренировок в симуляции.

Но натренировать робота в реальном мире быстро осваивать новый навык, как это делает человек, кажется очень недосягаемым.

Здесь ты мог бы сказать: "О да, локомоция.

Всем нашим предкам нужна была отличная локомоция, белкам.

Так что с локомоцией, может быть, у нас есть какой-то невероятный прайор".

Ты мог бы привести тот же аргумент для зрения.

Я полагаю, Ян Лекун высказал мысль, что дети учатся водить машину после 10 часов практики, что правда.

Но наше зрение такое хорошее.

По крайней мере для меня, я помню себя пятилетним.

Я тогда очень увлекался машинами.

Я почти уверен, что моё распознавание машин было более чем адекватным для вождения уже в пятилетнем возрасте.

Ты не видишь так уж много данных, будучи пятилетним.

Ты проводишь большую часть времени в доме родителей, так что у тебя очень низкое разнообразие данных.

Но ты мог бы сказать, может быть, это тоже эволюция.

Но в языке, математике и кодинге, вероятно, нет.

Это всё ещё кажется лучше, чем модели.

Очевидно, модели лучше, чем средний человек в языке, математике и кодинге.

Но лучше ли они, чем средний человек в обучении?

О да. О да, абсолютно. Что я хотел сказать, это то, что язык, математика и кодинг — и особенно математика и кодинг — предполагают, что что бы это ни было, что делает людей хорошими в обучении, это, вероятно, не столько сложный прайор, сколько что-то большее, какая-то фундаментальная вещь.

Я не уверен, что понял. Почему это должно быть так?

Рассмотрим навык, в котором люди демонстрируют какую-то большую надёжность.

Если навык — это тот, который был очень полезен нашим предкам на протяжении многих миллионов лет, сотен миллионов лет, ты мог бы утверждать, что, может быть, люди хороши в этом из-за эволюции, потому что у нас есть прайор, эволюционный прайор, который закодирован каким-то очень неочевидным образом, что как-то делает нас такими хорошими в этом.

Но если люди демонстрируют большую способность, надёжность, робастность и способность учиться в области, которая действительно не существовала до недавнего времени, то это скорее указывает на то, что у людей может быть просто лучшее машинное обучение, точка.

Как нам следует думать о том, что это такое? Какова ML-аналогия? Есть пара интересных вещей в этом. Это требует меньше сэмплов. Это более неконтролируемо. Ребёнок, учащийся водить машину… Дети не учатся водить машину.

Подросток, учащийся водить машину, не совсем получает какую-то готовую, проверяемую награду.

Это происходит из их взаимодействия с машиной и с окружающей средой.

Это требует гораздо меньше сэмплов. Это кажется более неконтролируемым. Это кажется более робастным?

Гораздо более робастным. Робастность людей действительно ошеломляющая.

Есть ли у тебя единый способ думать о том, почему все эти вещи происходят одновременно?

Какова ML-аналогия, которая могла бы реализовать что-то подобное?

Одна из вещей, о которых ты спрашивал — это как подросток-водитель может самокорректироваться и учиться на своём опыте без внешнего учителя?

Ответ в том, что у них есть их функция ценности.

У них есть общее чувство, которое также, кстати, чрезвычайно робастно у людей.

Какова бы ни была человеческая функция ценности, за некоторыми исключениями, связанными с зависимостью, она на самом деле очень, очень робастна.

Так что для кого-то вроде подростка, который учится водить, они начинают водить, и у них уже есть чувство того, как они водят немедленно, как плохо у них получается, как неуверенно.

И затем они видят: "Окей". И затем, конечно, скорость обучения любого подростка так высока.

Через 10 часов ты готов ехать.

Похоже, у людей есть какое-то решение, но мне любопытно, как они это делают и почему это так сложно?

Как нам нужно переосмыслить то, как мы тренируем модели, чтобы сделать что-то подобное возможным?

Это отличный вопрос, и это вопрос, по которому у меня много мнений.

Но, к сожалению, мы живём в мире, где не все идеи машинного обучения обсуждаются свободно, и это одна из них.

Вероятно, есть способ сделать это.

Я думаю, это можно сделать.

Тот факт, что люди такие, я думаю, это доказательство того, что это можно сделать.

Однако может быть ещё один блокирующий фактор, который заключается в том, что существует вероятность, что человеческие нейроны производят больше вычислений, чем мы думаем. Если это правда, и если это играет важную роль, то всё может быть сложнее.

Но независимо от этого, я действительно думаю, что это указывает на существование некоего принципа машинного обучения, о котором у меня есть мнения.

Но, к сожалению, обстоятельства затрудняют детальное обсуждение.

Никто не слушает этот подкаст, Илья.

Мне любопытно. Если ты говоришь, что мы вернулись в эпоху исследований, ты был там с 2012 по 2020 год.

Каким теперь будет вайб, если мы вернёмся в эпоху исследований?

Например, даже после AlexNet, количество вычислений, которое использовалось для проведения экспериментов, продолжало расти, и размер передовых систем продолжал расти.

Думаешь ли ты теперь, что эта эпоха исследований всё ещё будет требовать огромных объёмов вычислений?

Думаешь ли ты, что это потребует возвращения в архивы и чтения старых статей?

Ты был в Google, OpenAI и Стэнфорде, этих местах, когда там было больше вайба исследований?

Чего нам стоит ожидать в сообществе?

Одно из последствий эпохи масштабирования — это то, что масштабирование высосало весь воздух из комнаты.

Поскольку масштабирование высосало весь воздух из комнаты, все начали делать одно и то же.

Мы дошли до точки, где мы находимся в мире, где компаний больше, чем идей, причём намного.

Кстати об этом, есть такая поговорка в Кремниевой Долине, которая гласит, что идеи дешевы, исполнение — это всё.

Люди часто это говорят, и в этом есть правда.

Но потом я видел, как кто-то сказал в Твиттере что-то вроде: "Если идеи так дешевы, как так вышло, что ни у кого нет идей?" И я думаю, это тоже правда.

Если думать о прогрессе исследований с точки зрения узких мест, есть несколько узких мест.

Одно из них — идеи, и одно из них — твоя способность воплотить их в жизнь, что может быть вычислениями, но также и инженерией.

Если вернуться в 90-е, скажем, были люди, у которых были довольно хорошие идеи, и если бы у них были гораздо большие компьютеры, может быть, они могли бы продемонстрировать, что их идеи жизнеспособны.

Но они не могли, поэтому они могли иметь только очень, очень маленькую демонстрацию, которая никого не убеждала. Так что узким местом были вычисления. Затем в эпоху масштабирования вычисления сильно выросли.

Конечно, есть вопрос о том, сколько вычислений нужно, но вычисления большие.

Вычисления достаточно большие, так что неочевидно, что тебе нужно намного больше вычислений, чтобы доказать какую-то идею. Я дам тебе аналогию. AlexNet был построен на двух GPU.

Это был общий объём вычислений, использованный для него.

Трансформер был построен на от 8 до 64 GPU.

Ни один эксперимент в статье про трансформер не использовал более 64 GPU образца 2017 года, что было бы как, что, два сегодняшних GPU? ResNet, верно? Можно утверждать, что рассуждения o1 были не самой тяжёлой по вычислениям вещью в мире.

Так что для исследований тебе определённо нужно некоторое количество вычислений, но далеко не очевидно, что тебе нужно абсолютно наибольшее количество вычислений для исследований.

Ты мог бы утверждать, и я думаю, это правда, что если ты хочешь построить абсолютно лучшую систему, то тогда помогает иметь гораздо больше вычислений.

Особенно если все находятся в рамках одной парадигмы, тогда вычисления становятся одним из больших дифференциаторов.

Я спрашиваю тебя об истории, потому что ты на самом деле был там.

Я не уверен, что на самом деле произошло.

Звучит так, будто было возможно развить эти идеи, используя минимальные объёмы вычислений.

Но трансформер не сразу стал знаменитым.

Он стал тем, что все начали делать, а затем начали экспериментировать поверх этого и строить поверх этого, потому что он был валидирован на всё более и более высоких уровнях вычислений.

Верно.

И если у вас в SSI есть 50 разных идей, как вы узнаете, какая из них — следующий трансформер, а какая хрупкая, не имея тех видов вычислений, которые есть у других передовых лабораторий?

Я могу прокомментировать это. Короткий комментарий заключается в том, что ты упомянул SSI.

Конкретно для нас, объём вычислений, который есть у SSI для исследований, на самом деле не такой уж маленький. Я хочу объяснить почему. Простая математика может объяснить, почему объём вычислений, который у нас есть, сопоставим для исследований, чем можно подумать. Я объясню. SSI привлекла 3 миллиарда долларов, что много в любом абсолютном смысле.

Но ты мог бы сказать: "Посмотри на другие компании, привлекающие гораздо больше".

Но много их вычислений идёт на инференс.

Эти большие цифры, эти большие займы, это предназначено для инференса. Это номер один.

Номер два, если ты хочешь иметь продукт, на котором ты делаешь инференс, тебе нужно иметь большой штат инженеров, продавцов.

Много исследований должно быть посвящено созданию всевозможных функций, связанных с продуктом.

Так что когда ты смотришь на то, что на самом деле остаётся для исследований, разница становится намного меньше.

Другая вещь — если ты делаешь что-то другое, действительно ли тебе нужен абсолютно максимальный масштаб, чтобы доказать это?

Я вообще не думаю, что это правда.

Я думаю, что в нашем случае у нас достаточно вычислений, чтобы доказать, убедить себя и кого угодно ещё, что то, что мы делаем, правильно.

Были публичные оценки, что компании вроде OpenAI тратят порядка 5-6 миллиардов долларов в год только пока что, на эксперименты.

Это отдельно от количества денег, которые они тратят на инференс и так далее.

Так что похоже, они тратят больше в год на проведение исследовательских экспериментов, чем у вас, ребята, есть общего финансирования.

Я думаю, это вопрос того, что ты с этим делаешь.

Это вопрос того, что ты с этим делаешь.

В их случае, в случае других, существует гораздо больший спрос на тренировочные вычисления.

Там гораздо больше разных рабочих потоков, есть разные модальности, там просто больше всего. Так что это становится фрагментированным.

Как SSI будет зарабатывать деньги?

Мой ответ на этот вопрос примерно такой.

Прямо сейчас мы просто фокусируемся на исследованиях, а затем ответ на этот вопрос откроется сам собой.

Я думаю, будет много возможных ответов.

План SSI всё ещё — прямой путь к сверхителлекту?

Может быть. Я думаю, что в этом есть заслуга.

Я думаю, в этом много заслуги, потому что очень приятно не быть затронутым повседневной рыночной конкуренцией.

Но я думаю, есть две причины, которые могут заставить нас изменить план.

Одна прагматическая, если сроки окажутся длинными, что может быть.

Вторая, я думаю, есть большая ценность в том, чтобы лучший и самый мощный ИИ был там, влияя на мир.

Я думаю, это значимо ценная вещь.

Тогда почему ваш план по умолчанию — прямой путь к сверхителлекту?

Потому что звучит так, будто OpenAI, Anthropic, все эти другие компании, их явное мышление таково: "Смотрите, у нас есть всё более слабые интеллекты, к которым публика может привыкнуть и подготовиться".

Почему потенциально лучше построить сверхителлект напрямую?

Я приведу аргументы за и против.

Аргумент за — это то, что одна из проблем, с которыми сталкиваются люди, когда они на рынке, это то, что они должны участвовать в крысиных бегах.

Крысиные бега довольно сложны в том, что они подвергают тебя сложным компромиссам, которые тебе нужно делать.

Приятно сказать: "Мы изолируем себя от всего этого и просто сфокусируемся на исследованиях и выйдем только когда будем готовы, и не раньше".

Но контраргумент тоже валиден, и это противоборствующие силы.

Контраргумент: "Эй, для мира полезно видеть мощный ИИ.

Для мира полезно видеть мощный ИИ, потому что это единственный способ, которым ты можешь это донести".

Ну, я полагаю, даже не просто то, что ты можешь донести идею— Донести ИИ, не идею. Донести ИИ.

Что ты имеешь в виду, "донести ИИ"?

Давай предположим, ты пишешь эссе об ИИ, и эссе гласит: "ИИ будет таким, и ИИ будет сяким, и он будет этим".

Ты читаешь это и говоришь: "Окей, это интересное эссе".

Теперь предположим, ты видишь ИИ, делающий это, ИИ, делающий то. Это несравнимо. В принципе я думаю, что есть большая польза от того, что ИИ находится в публичном доступе, и это было бы причиной для нас не идти совсем прямым путём.

Я полагаю, дело даже не в этом, но я действительно думаю, что это важная часть этого.

Другая большая вещь — это то, что я не могу придумать другой дисциплины в человеческой инженерии и исследованиях, где конечный артефакт был сделан безопаснее в основном просто через размышления о том, как сделать его безопасным, в отличие от того, почему авиакатастроф на милю так намного меньше сегодня, чем десятилетия назад.

Почему намного сложнее найти баг в Linux, чем это было бы десятилетия назад?

Я думаю, это в основном потому, что эти системы были развёрнуты в мире.

Ты замечал сбои, эти сбои исправлялись, и системы становились более робастными.

Я не уверен, почему AGI и сверхчеловеческий интеллект были бы чем-то иным, особенно учитывая — и я надеюсь, мы доберёмся до этого — кажется, что вред от сверхителлекта не просто в том, чтобы иметь какого-то зловредного скрепочника там.

Но это действительно мощная вещь, и мы даже не знаем, как концептуализировать, как люди взаимодействуют с этим, что люди будут делать с этим.

Иметь постепенный доступ к этому кажется лучшим способом, может быть, распределить влияние этого и помочь людям подготовиться к этому.

Ну, я думаю, по этому пункту, даже в сценарии прямого пути, ты всё равно делал бы постепенный выпуск этого, так я бы это представлял.

Постепенность была бы неотъемлемым компонентом любого плана.

Это просто вопрос того, что является первой вещью, которую ты выпускаешь за дверь. Это номер один. Номер два, я полагаю, ты выступал за непрерывное обучение больше, чем другие люди, и я на самом деле думаю, что это важная и правильная вещь. Вот почему. Я дам тебе ещё один пример того, как язык влияет на мышление.

В данном случае это будут два слова, которые сформировали мышление каждого, я утверждаю.

Первое слово: AGI. Второе слово: предобучение.

Давай я объясню. Термин AGI, почему этот термин существует? Это очень специфический термин. Почему он существует? Есть причина. Причина, по которой термин AGI существует, по моему мнению, не столько в том, что это очень важный, существенный дескриптор какого-то конечного состояния интеллекта, но потому что это реакция на другой термин, который существовал, и этот термин — узкий ИИ.

Если вернуться в древнюю историю геймплея и ИИ, шашечного ИИ, шахматного ИИ, ИИ компьютерных игр, все говорили: посмотрите на этот узкий интеллект.

Конечно, шахматный ИИ может победить Каспарова, но он не может делать ничего другого.

Он такой узкий, искусственный узкий интеллект.

Так что в ответ, как реакция на это, некоторые люди сказали: это не хорошо. Это так узко. Что нам нужно, так это общий ИИ, ИИ, который может просто делать всё.

Этот термин просто получил большую популярность.

Вторая вещь, которая получила большую популярность, это предобучение, конкретно рецепт предобучения.

Я думаю, то, как люди делают RL сейчас, возможно, отменяет концептуальный отпечаток предобучения.

Но у предобучения было это свойство. Ты делаешь больше предобучения, и модель становится лучше во всём, более или менее равномерно.

Общий ИИ. Предобучение даёт AGI. Но вещь, которая случилась с AGI и предобучением, это то, что в каком-то смысле они промахнулись мимо цели.

Если подумать о термине "AGI", особенно в контексте предобучения, ты поймёшь, что человек — это не AGI.

Да, определённо есть фундамент навыков, но человеку не хватает огромного количества знаний.

Вместо этого мы полагаемся на непрерывное обучение.

Так что когда ты думаешь о: "Окей, так давай предположим, что мы достигли успеха и мы произвели какой-то безопасный сверхителлект".

Вопрос в том, как ты его определяешь?

Где на кривой непрерывного обучения он будет находиться?

Я произвожу сверхителлектуального 15-летнего, который очень жаждет действовать.

Он не знает очень многого вообще, отличный студент, очень жаждущий.

Ты иди и будь программистом, ты иди и будь врачом, иди и учись.

Так что ты мог бы представить, что развёртывание само по себе будет включать какой-то период обучения методом проб и ошибок.

Это процесс, в отличие от того, что ты сбрасываешь готовую вещь.

Я понимаю. Ты предполагаешь, что вещь, на которую ты указываешь со сверхителлектом, это не какой-то законченный разум, который знает, как делать каждую отдельную работу в экономике.

Потому что то, как, скажем, оригинальный устав OpenAI или что-то такое определяет AGI — это типа, он может делать каждую отдельную работу, каждую отдельную вещь, которую может делать человек. Ты предлагаешь вместо этого разум, который может научиться делать каждую отдельную работу, и это и есть сверхителлект.

Да.

Но как только у тебя есть алгоритм обучения, он развёртывается в мире так же, как человек-рабочий может присоединиться к организации.

Именно.

Похоже, что одна из этих двух вещей может произойти, может быть, ни одна из них не произойдёт.

Первая: этот супер-эффективный алгоритм обучения становится сверхчеловеческим, становится таким же хорошим, как ты, и потенциально даже лучше, в задаче ML-исследований.

В результате алгоритм сам по себе становится всё более и более сверхчеловеческим.

Другая — даже если этого не произойдёт, если у тебя есть одна модель — это явно твоё видение — где экземпляры модели, которые развёрнуты по всей экономике, выполняя разные работы, учась делать эти работы, непрерывно обучаясь на работе, перенимая все навыки, которые мог бы перенять любой человек, но перенимая их все одновременно, а затем объединяя их знания, у тебя по сути есть модель, которая функционально становится сверхителлектуальной даже без какого-либо рекурсивного самосовершенствования в программном обеспечении. Потому что теперь у тебя есть одна модель, которая может делать каждую отдельную работу в экономике, а люди не могут объединять наши разумы таким же образом.

Так ожидаешь ли ты какого-то взрыва интеллекта от широкого развёртывания?

Я думаю, что вероятно, что у нас будет быстрый экономический рост.

Я думаю, с широким развёртыванием есть два аргумента, которые можно привести, и они противоречат друг другу. Один — это то, что как только ты действительно дойдёшь до точки, где у тебя есть ИИ, который может учиться делать вещи быстро, и у тебя их много, тогда будет сильная сила развёртывать их в экономике, если только не будет какого-то регулирования, которое остановит это, что, кстати, может быть.

Но идея очень быстрого экономического роста в течение некоторого времени, я думаю, очень возможна от широкого развёртывания.

Вопрос в том, насколько быстрым он будет.

Я думаю, это трудно знать, потому что, с одной стороны, у тебя есть этот очень эффективный работник.

С другой стороны, мир просто действительно большой, и там много всего, и это всё движется с разной скоростью.

Но с другой стороны, теперь ИИ мог бы...

Так что я думаю, очень быстрый экономический рост возможен.

Мы увидим всякие вещи, типа разных стран с разными правилами, и те, у которых правила более дружелюбные, экономический рост будет быстрее. Трудно предсказать.

Мне кажется, что это очень шаткая ситуация, в которой можно оказаться.

В пределе, мы знаем, что это должно быть возможно.

Если у тебя есть что-то, что так же хорошо как человек в обучении, но что может объединять свои мозги — объединять разные экземпляры так, как люди не могут объединяться — уже это кажется вещью, которая должна быть физически возможна.

Люди возможны, цифровые компьютеры возможны.

Тебе просто нужно и то и другое объединить, чтобы произвести эту вещь.

Также кажется, что такого рода вещь чрезвычайно мощная.

Экономический рост — это один способ сказать это.

Сфера Дайсона — это много экономического роста.

Но другой способ сказать это — что у тебя будет, потенциально за очень короткий период времени...

Ты нанимаешь людей в SSI, и через шесть месяцев они, вероятно, становятся нетто-продуктивными.

Человек учится очень быстро, и эта штука становится умнее и умнее очень быстро.

Как ты думаешь о том, чтобы сделать так, чтобы это прошло хорошо? Почему SSI позиционирована, чтобы сделать это хорошо?

Каков план SSI там, вот в принципе что я пытаюсь спросить.

Один из способов, которым моё мышление менялось, это то, что я теперь придаю больше значения тому, чтобы ИИ развёртывался постепенно и заранее.

Одна очень сложная вещь с ИИ — это то, что мы говорим о системах, которые ещё не существуют, и их трудно представить.

Я думаю, что одна из вещей, которая происходит, это то, что на практике очень трудно почувствовать AGI.

Очень трудно почувствовать AGI.

Мы можем говорить об этом, но представь, что у тебя разговор о том, каково это — быть старым, когда ты старый и немощный.

Ты можешь поговорить, ты можешь попытаться представить это, но это просто трудно, и ты возвращаешься к реальности, где это не так.

Я думаю, что многие проблемы вокруг AGI и его будущей мощи проистекают из того факта, что его очень трудно представить.

Будущий ИИ будет другим. Он будет мощным. Действительно, вся проблема, в чём проблема ИИ и AGI?

Вся проблема в мощи.

Вся проблема в мощи.

Когда мощь действительно большая, что произойдёт?

Один из способов, которым я изменил своё мнение за последний год — и это изменение мнения, я сделаю небольшую оговорку, может распространиться назад в планы нашей компании — это то, что если это трудно представить, что ты делаешь?

Ты должен показывать эту вещь.

Ты должен показывать эту вещь.

Я утверждаю, что большинство людей, которые работают над ИИ, тоже не могут представить это, потому что это слишком отличается от того, что люди видят на повседневной основе.

Я действительно утверждаю, вот что-то, что я предсказываю, случится. Это предсказание.

Я утверждаю, что по мере того, как ИИ становится более мощным, люди будут менять своё поведение.

Мы увидим всевозможные беспрецедентные вещи, которые не происходят прямо сейчас. Я приведу несколько примеров. Я думаю, к лучшему или худшему, передовые компании будут играть очень важную роль в том, что происходит, как и правительство.

Вид вещей, которые, я думаю, ты увидишь, начало которых ты видишь — это компании, которые являются яростными конкурентами, начинают сотрудничать по безопасности ИИ.

Ты мог видеть OpenAI и Anthropic, делающих первый маленький шаг, но этого не существовало.

Это то, что я предсказывал в одном из моих выступлений года три назад, что такая вещь случится.

Я также утверждаю, что по мере того, как ИИ продолжает становиться более мощным, более видимо мощным, также будет желание со стороны правительств и общественности что-то сделать.

Я думаю, это очень важная сила, показа ИИ. Это номер один.

Номер два, окей, итак, ИИ строится. Что нужно сделать? Одна вещь, которую я утверждаю, случится — это то, что прямо сейчас, люди, которые работают над ИИ, я утверждаю, что ИИ не ощущается мощным из-за его ошибок.

Я действительно думаю, что в какой-то момент ИИ начнёт ощущаться мощным на самом деле.

Я думаю, когда это случится, мы увидим большое изменение в том, как все ИИ-компании подходят к безопасности. Они станут гораздо более параноидальными.

Я говорю это как предсказание, которое мы увидим, сбудется. Посмотрим, прав ли я. Но я думаю, это то, что случится, потому что они увидят, как ИИ становится более мощным.

Всё, что происходит прямо сейчас, я утверждаю, происходит потому, что люди смотрят на сегодняшний ИИ, и трудно представить будущий ИИ.

Есть третья вещь, которая должна произойти.

Я говорю об этом в более широких терминах, не только с точки зрения SSI, потому что ты спросил меня о нашей компании.

Вопрос в том, что компании должны стремиться построить?

Что они должны стремиться построить?

Была одна большая идея, на которой все зациклились, это самосовершенствующийся ИИ. Почему это произошло?

Потому что идей меньше, чем компаний.

Но я утверждаю, что есть что-то, что лучше построить, и я думаю, что все захотят этого.

Это ИИ, который надёжно выровнен на то, чтобы заботиться о чувствующей жизни конкретно.

Я думаю, в частности, есть аргумент в пользу того, что будет легче построить ИИ, который заботится о чувствующей жизни, чем ИИ, который заботится только о человеческой жизни, потому что ИИ сам будет чувствующим.

И если подумать о таких вещах, как зеркальные нейроны и человеческая эмпатия к животным, о которой ты можешь сказать, что она недостаточно велика, но она существует. Я думаю, это эмерджентное свойство от того факта, что мы моделируем других с помощью той же схемы, которую мы используем для моделирования себя, потому что это самая эффективная вещь.

Так что даже если ты заставил ИИ заботиться о чувствующих существах — и мне на самом деле не ясно, что это то, что ты должен пытаться делать, если ты решил проблему выравнивания — всё равно будет так, что большинством чувствующих существ будут ИИ.

Будут триллионы, в конечном итоге квадриллионы ИИ.

Люди будут очень маленькой долей чувствующих существ.

Так что мне не ясно, если цель — какой-то человеческий контроль над этой будущей цивилизацией, что это лучший критерий.

Это правда. Возможно, это не лучший критерий. Я скажу две вещи. Номер один, забота о чувствующей жизни, я думаю, в этом есть заслуга. Это следует рассмотреть. Я думаю, было бы полезно, если бы был какой-то короткий список идей, которые компании, когда они в этой ситуации, могли бы использовать. Это номер два.

Номер три, я думаю, было бы действительно материально полезно, если бы мощь самого мощного сверхителлекта была как-то ограничена, потому что это решило бы многие из этих проблем.

Вопрос о том, как это сделать, я не уверен, но я думаю, это было бы материально полезно, когда ты говоришь о действительно, действительно мощных системах.

Прежде чем мы продолжим дискуссию о выравнивании, я хочу сделать дабл-клик на этом.

Сколько места наверху?

Как ты думаешь о сверхителлекте?

Думаешь ли ты, используя эту идею эффективности обучения, может быть, он просто чрезвычайно быстр в изучении новых навыков или новых знаний?

У него просто больший пул стратегий?

Есть ли единое сплочённое "оно" в центре, которое более мощное или большое?

Если да, представляешь ли ты, что это будет вроде как богоподобным по сравнению с остальной человеческой цивилизацией, или это просто ощущается как ещё один агент, или ещё один кластер агентов?

Это область, где у разных людей разные интуиции.

Я думаю, он будет очень мощным, безусловно.

Что, я думаю, наиболее вероятно произойдёт, это то, что будет несколько таких ИИ, созданных примерно в одно и то же время.

Я думаю, что если кластер достаточно большой — типа если кластер буквально размером с континент — эта штука может быть действительно мощной, действительно.

Если у тебя буквально есть кластер размером с континент, эти ИИ могут быть очень мощными.

Всё, что я могу тебе сказать, это то, что если ты говоришь о чрезвычайно мощных ИИ, по-настоящему драматически мощных, было бы неплохо, если бы они могли быть сдержаны в некоторых отношениях или если бы было какое-то соглашение или что-то.

В чём беспокойство по поводу сверхителлекта?

Какой один способ объяснить беспокойство?

Если ты представишь систему, которая достаточно мощная, действительно достаточно мощная — и ты мог бы сказать, тебе нужно делать что-то разумное, типа заботиться о чувствующей жизни очень однонаправленным образом — нам могут не понравиться результаты. Это действительно то, что это такое. Может быть, кстати, ответ в том, что ты не строишь RL-агента в обычном смысле. Я укажу на несколько вещей. Я думаю, человеческие существа — это полу-RL агенты.

Мы преследуем награду, а затем эмоции или что-то ещё заставляют нас уставать от награды, и мы преследуем другую награду.

Рынок — это очень близорукий вид агента. Эволюция такая же. Эволюция очень умна в некоторых отношениях, но очень глупа в других.

Правительство было спроектировано как бесконечная борьба между тремя частями, что имеет эффект.

Так что я думаю о вещах вроде этого.

Ещё одна вещь, которая делает эту дискуссию сложной, это то, что мы говорим о системах, которые не существуют, которые мы не знаем, как строить.

Это другая вещь, и это на самом деле моё убеждение.

Я думаю, то, что люди делают прямо сейчас, пройдёт какое-то расстояние, а затем выдохнется.

Оно продолжит улучшаться, но это также не будет "оно".

"Оно" мы не знаем, как строить, и многое зависит от понимания надёжной генерализации. Я скажу ещё одну вещь.

Одна из вещей, которую ты мог бы сказать о том, что делает выравнивание трудным, это то, что твоя способность изучать человеческие ценности хрупка.

Затем твоя способность оптимизировать их хрупка.

Ты на самом деле учишься оптимизировать их.

И разве ты не можешь сказать: "Разве это всё не примеры ненадёжной генерализации?" Почему так получается, что человеческие существа, кажется, генерализуются настолько лучше?

Что если бы генерализация была намного лучше?

Что случилось бы в этом случае? Каков был бы эффект? Но эти вопросы сейчас всё ещё без ответа.

Как думать о том, как выглядит ИИ, когда всё идёт хорошо?

Ты обрисовал, как ИИ может эволюционировать.

У нас будут эти своего рода агенты непрерывного обучения. ИИ будет очень мощным.

Может быть, будет много разных ИИ.

Как ты думаешь о множестве интеллектов размером с континент, ходящих вокруг? Насколько это опасно? Как нам сделать это менее опасным?

И как нам сделать это способом, который защищает равновесие, где могут быть невыровненные ИИ там и плохие акторы там?

Вот одна причина, почему мне понравился "ИИ, который заботится о чувствующей жизни".

Мы можем спорить о том, хорошо это или плохо.

Но если первые N из этих драматических систем действительно заботятся о, любят человечество или что-то такое, заботятся о чувствующей жизни, очевидно, это тоже должно быть достигнуто. Это должно быть достигнуто. Так что если это достигнуто первыми N из этих систем, тогда я могу видеть, что всё идёт хорошо, по крайней мере, какое-то время. Затем есть вопрос о том, что происходит в долгосрочной перспективе.

Как достичь долгосрочного равновесия?

Я думаю, что там, там тоже есть ответ.

Мне не нравится этот ответ, но его нужно рассмотреть.

В долгосрочной перспективе ты мог бы сказать: "Окей, если у тебя есть мир, где существуют мощные ИИ, в краткосрочной перспективе ты мог бы сказать, у тебя есть универсальный высокий доход.

У тебя есть универсальный высокий доход, и у нас всё хорошо".

Но что говорят буддисты? "Перемены — это единственная постоянная". Вещи меняются. Есть какое-то правительство, политическая структура, и она меняется, потому что у этих вещей есть срок годности.

Появляется какая-то новая правительственная штука, и она функционирует, а затем через какое-то время она перестаёт функционировать.

Это то, что мы видим постоянно.

Так что я думаю, для долгосрочного равновесия, один подход — ты мог бы сказать, может быть, у каждого человека будет ИИ, который будет исполнять его волю, и это хорошо.

Если бы это можно было поддерживать бесконечно, это правда.

Но недостаток этого в том, что тогда ИИ идёт и зарабатывает деньги для человека и отстаивает его потребности в политической сфере, и, может быть, затем пишет маленький отчёт, говоря: "Окей, вот что я сделал, вот ситуация", и человек говорит: "Отлично, продолжай в том же духе".

Но человек больше не участник.

Тогда ты можешь сказать, что это шаткое положение.

Я начну с того, что скажу, что мне не нравится это решение, но это решение.

Решение — это если люди станут частично ИИ с каким-то Neuralink++.

Потому что то, что произойдёт в результате — это то, что теперь ИИ понимает что-то, и мы понимаем это тоже, потому что теперь понимание передаётся целиком.

Так что теперь, если ИИ в какой-то ситуации, ты вовлечён в эту ситуацию сам полностью.

Я думаю, это ответ на равновесие.

Интересно, является ли тот факт, что эмоции, которые были развиты миллионы — или во многих случаях, миллиарды — лет назад в совершенно другой среде, всё ещё направляют наши действия так сильно, примером успеха выравнивания.

Чтобы пояснить, что я имею в виду — я не знаю, точнее ли называть это функцией ценности или функцией вознаграждения — но у ствола мозга есть директива, где он говорит: "Спаривайся с кем-то, кто более успешен".

Кора — это та часть, которая понимает, что означает успех в современном контексте.

Но ствол мозга способен выровнять кору и сказать: "Как бы ты ни распознавала успех — и я недостаточно умён, чтобы понять, что это — ты всё равно будешь следовать этой директиве".

Я думаю, есть более общий момент.

Я думаю, это на самом деле действительно загадочно, как эволюция кодирует высокоуровневые желания.

Довольно легко понять, как эволюция наделила бы нас желанием еды, которая хорошо пахнет, потому что запах — это химическое вещество, так что просто преследуй это вещество. Очень легко представить, как эволюция делает эту вещь.

Но эволюция также наделила нас всеми этими социальными желаниями.

Нам действительно важно быть воспринимаемыми обществом положительно.

Нам важно быть на хорошем счету.

Все эти социальные интуиции, которые у нас есть, я сильно чувствую, что они вшиты.

Я не знаю, как эволюция сделала это, потому что это высокоуровневая концепция, которая представлена в мозге.

Скажем, тебе важна какая-то социальная вещь, это не низкоуровневый сигнал, как запах.

Это не что-то, для чего есть сенсор.

Мозгу нужно проделать много обработки, чтобы собрать воедино множество битов информации, чтобы понять, что происходит в социальном плане.

Как-то эволюция сказала: "Это то, о чём тебе следует заботиться". Как она это сделала? Она сделала это быстро, к тому же. Все эти сложные социальные вещи, о которых мы заботимся, я думаю, они эволюционировали довольно недавно.

Эволюции было легко жёстко закодировать это высокоуровневое желание.

Я не знаю хорошей гипотезы о том, как это сделано.

У меня были некоторые идеи, которые я обдумывал, но ни одна из них не является удовлетворительной.

Что особенно впечатляет, так это то, что это было желание, которое ты выучил за свою жизнь, это имеет смысл, потому что твой мозг разумен.

Имеет смысл, почему ты смог бы выучить разумные желания.

Может быть, это не твой аргумент, но один из способов понять это — это то, что желание встроено в геном, а геном не разумен.

Но ты как-то способен описать эту функцию.

Даже не ясно, как ты определяешь эту функцию, и ты можешь встроить её в гены.

По сути, или, может быть, я скажу по-другому.

Если подумать об инструментах, которые доступны геному, он говорит: "Окей, вот рецепт для построения мозга".

Ты мог бы сказать: "Вот рецепт для соединения дофаминовых нейронов с датчиком запаха".

И если запах определённого хорошего вида, ты хочешь это съесть.

Я мог бы представить, что геном делает это.

Я утверждаю, что это труднее представить.

Труднее представить, что геном говорит, что тебе следует заботиться о каком-то сложном вычислении, которое делает весь твой мозг, большой кусок твоего мозга.

Это всё, что я утверждаю. Я могу рассказать тебе спекуляцию о том, как это могло быть сделано.

Позволь мне предложить спекуляцию, и я объясню, почему спекуляция, вероятно, ложна.

Итак, у мозга есть области мозга. У нас есть наша кора. У неё есть все эти области мозга.

Кора однородна, но области мозга и нейроны в коре как бы говорят со своими соседями в основном.

Это объясняет, почему ты получаешь области мозга.

Потому что если ты хочешь делать какую-то обработку речи, все нейроны, которые занимаются речью, должны говорить друг с другом.

И поскольку нейроны могут говорить только со своими ближайшими соседями, по большей части, это должна быть область.

Все области в основном расположены в одном и том же месте от человека к человеку.

Так что, может быть, эволюция жёстко закодировала буквально местоположение в мозге.

Так что она говорит: "О, когда GPS-координаты мозга такие-то и такие-то, когда это срабатывает, это то, о чём тебе следует заботиться".

Может быть, это то, что сделала эволюция, потому что это было бы в рамках инструментария эволюции.

Да, хотя есть примеры, когда, например, у людей, которые родились слепыми, эта область их коры захвачена другим чувством.

Я понятия не имею, но я был бы удивлён, если бы желания или функции вознаграждения, которые требуют визуального сигнала, больше не работали для людей, у которых разные области их коры кооптированы.

Например, если у тебя больше нет зрения, можешь ли ты всё ещё чувствовать, что я хочу, чтобы люди вокруг меня любили меня и так далее, для чего обычно также есть визуальные подсказки.

Я полностью согласен с этим. Я думаю, есть ещё более сильный контраргумент этой теории.

Есть люди, которым удаляют половину мозга в детстве, и у них всё ещё есть все их области мозга.

Но они все как-то перемещаются в одно полушарие, что предполагает, что области мозга, их местоположение не фиксировано, и поэтому эта теория не верна.

Было бы круто, если бы это было правдой, но это не так.

Так что я думаю, это загадка.

Но это интересная загадка. Факт в том, что как-то эволюция смогла наделить нас способностью заботиться о социальных вещах очень, очень надёжно. Даже люди, у которых есть всевозможные странные ментальные состояния и недостатки и эмоциональные проблемы, склонны заботиться об этом тоже.

Что SSI планирует делать по-другому?

Предположительно, ваш план — быть одной из передовых компаний, когда это время придёт.

Предположительно, ты основал SSI, потому что ты типа: "Я думаю, у меня есть способ подхода к тому, как сделать это безопасно, способом, которого у других компаний нет". В чём эта разница?

То, как я бы это описал — это то, что есть некоторые идеи, которые я считаю многообещающими, и я хочу исследовать их и посмотреть, действительно ли они многообещающие или нет. Это действительно так просто.

Это попытка. Если идеи окажутся верными — эти идеи, которые мы обсуждали вокруг понимания генерализации — тогда я думаю, у нас будет что-то достойное.

Окажутся ли они верными? Мы занимаемся исследованиями. Мы определённо компания "эпохи исследований". Мы делаем прогресс. Мы на самом деле сделали довольно хороший прогресс за последний год, но нам нужно продолжать делать больше прогресса, больше исследований. Вот как я это вижу. Я вижу это как попытку быть голосом и участником.

Твой сооснователь и предыдущий CEO ушёл в Meta недавно, и люди спрашивали: "Ну, если бы делалось много прорывов, это кажется вещью, которая должна была быть маловероятной". Интересно, как ты ответишь.

На это я просто напомню несколько фактов, которые, возможно, были забыты.

Я думаю, эти факты, которые предоставляют контекст, объясняют ситуацию.

Контекст был таков, что мы привлекали средства при оценке в 32 миллиарда долларов, и затем Meta пришла и предложила приобрести нас, и я сказал нет.

Но мой бывший сооснователь в каком-то смысле сказал да.

В результате он также смог насладиться большой краткосрочной ликвидностью, и он был единственным человеком из SSI, кто присоединился к Meta.

Звучит так, будто план SSI — быть компанией, которая находится на передовой, когда вы дойдёте до этого очень важного периода в человеческой истории, где у вас есть сверхчеловеческий интеллект.

У вас есть эти идеи о том, как сделать так, чтобы сверхчеловеческий интеллект прошёл хорошо. Но другие компании будут пробовать свои собственные идеи.

Что отличает подход SSI к тому, чтобы сверхителлект прошёл хорошо?

Главное, что отличает SSI — это её технический подход.

У нас есть другой технический подход, который я считаю достойным, и мы его преследуем.

Я утверждаю, что в конце концов будет конвергенция стратегий.

Я думаю, будет конвергенция стратегий, где в какой-то момент, по мере того как ИИ становится мощнее, всем станет более или менее яснее, какой должна быть стратегия.

Это должно быть что-то вроде: вам нужно найти какой-то способ говорить друг с другом, и вы хотите, чтобы ваш первый настоящий реальный сверхителлектуальный ИИ был выровнен и как-то заботился о чувствующей жизни, заботился о людях, демократичный, одно из этого, какая-то комбинация этого.

Я думаю, это условие, к которому каждый должен стремиться.

Это то, к чему стремится SSI.

Я думаю, что в этот раз, если не уже, все другие компании поймут, что они стремятся к тому же самому.

Посмотрим. Я думаю, что мир действительно изменится, когда ИИ станет мощнее.

Я думаю, вещи будут действительно другими, и люди будут действовать действительно по-другому.

Говоря о прогнозах, каковы твои прогнозы до этой системы, которую ты описываешь, которая может учиться так же хорошо, как человек, и впоследствии, в результате, стать сверхчеловеческой?

Я думаю, типа от 5 до 20.

От 5 до 20 лет?

Мгм.

Я просто хочу развернуть то, как ты можешь видеть наступление мира.

Это типа, у нас есть ещё пара лет, когда эти другие компании продолжают текущий подход, и он глохнет.

"Глохнет" здесь означает, что они зарабатывают не более чем низкие сотни миллиардов выручки?

Как ты думаешь о том, что значит "глохнет"?

Я думаю, "глохнет" будет выглядеть как… это всё будет выглядеть очень похоже у всех разных компаний.

Это может быть что-то вроде этого.

Я не уверен, потому что я думаю, даже заглохнув, я думаю, эти компании могли бы сделать колоссальную выручку.

Может быть, не прибыль, потому что им нужно будет много работать, чтобы дифференцировать друг друга от самих себя, но выручку определённо.

Но что-то в твоей модели подразумевает, что когда правильное решение действительно появится, будет конвергенция между всеми компаниями.

Мне любопытно, почему ты думаешь, что это так.

Я говорил больше о конвергенции их стратегий выравнивания.

Я думаю, окончательная конвергенция технического подхода, вероятно, тоже произойдёт, но я намекал на конвергенцию стратегий выравнивания.

Что именно является той вещью, которую нужно сделать?

Я просто хочу лучше понять, как ты видишь развёртывание будущего.

В настоящее время у нас есть эти разные компании, и ты ожидаешь, что их подход продолжит генерировать выручку, но не доберётся до этого человекоподобного ученика. Так что теперь у нас есть эти разные ветви компаний.

У нас есть вы, у нас есть Thinking Machines, есть куча других лабораторий.

Может быть, одна из них выяснит правильный подход.

Но тогда выпуск их продукта делает ясным для других людей, как делать эту вещь.

Я думаю, не будет ясно, как это делать, но будет ясно, что что-то другое возможно, и это информация.

Люди будут затем пытаться выяснить, как это работает.

Я действительно думаю, однако, что одна из вещей, не затронутых здесь, не обсуждаемых, это то, что с каждым увеличением возможностей ИИ, я думаю, будут какие-то изменения, но я не знаю точно какие, в том, как делаются дела.

Я думаю, это будет важно, но я не могу сформулировать, что это именно.

По умолчанию ты бы ожидал, что компания, у которой есть эта модель, будет получать все эти выгоды, потому что у них есть модель, у которой есть навыки и знания, которые она накапливает в мире.

Какова причина думать, что выгоды от этого будут широко распределены, а не просто окажутся у любой модельной компании, которая запустит этот цикл непрерывного обучения первой?

Вот что я думаю, произойдёт.

Номер один, давай посмотрим, как дела шли до сих пор с ИИ прошлого.

Одна компания произвела продвижение, и другая компания засуетилась и произвела какие-то похожие вещи через какое-то время, и они начали конкурировать на рынке и толкать цены вниз.

Так что я думаю, с рыночной перспективы, что-то похожее произойдёт и там.

Мы говорим о хорошем мире, кстати.

Что такое хороший мир? Это где у нас есть эти мощные человекоподобные ученики, которые также… Кстати, я думаю, будет специализация.

Я думаю, будет специализация.

Я думаю, что то, что произойдёт, это то, что разные компании будут хороши в разных вещах.

Некоторые компании будут хороши в ИИ-врачах, некоторые компании будут хороши в ИИ-юристах, некоторые компании будут хороши в ИИ-инженерах.

Так что будет специализация, и это будет ещё одна причина, почему будет много игроков.

Я хочу закончить на теме исследовательского вкуса.

Ты упомянул, что у тебя есть этот другой технический подход, который ты пробуешь в SSI.

Очевидно, ты не можешь поделиться деталями, но мне любопытно, что определяет твой вкус?

Что ты ищешь в идее?

Что делает идею красивой для тебя?

Для меня красивая идея — это та, которая объясняет многое очень просто.

Это то, что мне нравится.

Мне нравятся идеи, которые объясняют почему, которые дают тебе "почему".

Если у тебя есть "почему", тогда ты можешь понять, что происходит.

Если у тебя нет "почему", ты просто тычешь в темноте.

Так что мне нравятся идеи, которые дают "почему", и мне нравятся идеи, которые просты.

Потому что если идея проста, она вероятно верна.

Если идея сложная, она вероятно неверна.

Это бритва Оккама.

Но это также то, что я видел в своей карьере.

Самые успешные идеи были самыми простыми.

Глубокое обучение очень простое.

Это просто умножение матриц и нелинейности. Это всё.

И ты просто складываешь их в стопку.

И ты делаешь это много раз.

И ты используешь много данных.

Это очень просто.

Масштабирование очень простое.

Просто сделай это больше.

Это очень просто.

Так что мне нравятся простые идеи.

Есть ли у тебя пример идеи, которая была сложной и оказалась неверной?

О, их так много.

Вся область компьютерного зрения до глубокого обучения была полна сложных идей.

Люди придумывали эти очень сложные дескрипторы признаков, SIFT и HOG, и все эти вещи.

Они были очень сложными.

И они работали немного, но они не работали хорошо.

И затем пришло глубокое обучение и просто смело их всех.

Потому что оно было простым и оно работало.

Рич Саттон написал знаменитое эссе "Горький урок".

В нём он утверждает, что единственное, что имеет значение в долгосрочной перспективе — это использование вычислений.

И что мы должны прекратить пытаться встроить наши человеческие знания в ИИ.

Согласен ли ты с этим?

Я думаю, в этом много правды.

Я думаю, что история показала, что методы, которые масштабируются с вычислениями, побеждают методы, которые не масштабируются.

И методы, которые полагаются на человеческие знания, обычно не масштабируются с вычислениями.

Потому что человеческие знания фиксированы.

Ты не можешь просто добавить больше человеческих знаний так же легко, как ты можешь добавить больше вычислений.

Так что я думаю, Рич Саттон прав.

Но есть нюанс.

Нюанс в том, что тебе всё ещё нужно иметь правильную архитектуру.

Тебе всё ещё нужно иметь правильный алгоритм.

Ты не можешь просто бросить вычисления на что угодно.

Если ты бросишь вычисления на логистическую регрессию, ты не получишь AGI.

Так что тебе нужно правильное "что".

И это "что" должно быть простым и масштабируемым.

Это мой вкус.

Когда мы увидим этого человекоподобного ученика?

Каков твой прогноз?

Я не хочу давать конкретную дату.

Но я думаю, что это не так уж далеко.

Я думаю, мы увидим признаки этого в ближайшие пару лет.

Может быть, 2-3 года?

Что-то вроде того.

И когда это случится, это будет большим сдвигом парадигмы.

Потому что внезапно у нас будут модели, которые могут учиться так же, как мы.

И это откроет дверь к настоящему сверхителлекту.

Потому что тогда мы сможем просто запустить их и позволить им учиться и становиться умнее.

И они смогут делать исследования.

И они смогут улучшать себя.

И это будет началом чего-то совершенно нового.

Ты думаешь, текущая парадигма заглохнет до того, как это случится?

Я думаю, мы уже видим признаки этого.

Я думаю, что выгоды от простого масштабирования предобучения уменьшаются.

И люди начинают понимать, что нужно что-то ещё.

И это "что-то ещё" — это то, над чем мы работаем.

И я думаю, другие тоже начнут работать над этим.

Так что да, я думаю, будет переход.

Илья, спасибо большое за разговор.

Это было увлекательно.

Спасибо, что пригласил меня.

Это было весело.
