Знаешь, что безумно? Что всё это — реальность.

В смысле?

Разве тебе так не кажется? Весь этот AI, вся эта Bay Area... что это происходит прямо сейчас.

Разве это не прямиком из научной фантастики?

Ещё одна безумная штука — насколько нормальным ощущается медленный взлёт.

Казалось бы, мысль о том, что мы будем вкладывать 1% ВВП в AI... это должно было ощущаться грандиознее, а сейчас просто...

Оказывается, мы ко всему привыкаем довольно быстро.

Но это ещё и абстрактно. Что это вообще значит? Ты видишь в новостях, что такая-то компания объявила такую-то сумму. Вот и всё.

Пока это никак иначе не ощущается.

Может, с этого и начнём? По-моему, интересная тема.

Давай.

Мне кажется, твой поинт про то, что для обычного человека ничего особо не изменилось — он останется верным даже в сингулярность.

Нет, я так не думаю.

О, интересно.

Я имел в виду другое под "не ощущается иначе" — ну, такая-то компания объявила какую-то непостижимую сумму инвестиций.

Никто не знает, что с этим делать.

Но влияние AI будет ощутимым.

AI будет проникать в экономику.

Там будут очень сильные экономические стимулы, и влияние будет ощущаться очень сильно.

Когда ты ожидаешь это влияние?

Модели кажутся умнее, чем следует из их экономического импакта.

Да. Это одна из самых сбивающих с толку вещей в моделях сейчас.

Как совместить то, что они так хорошо справляются с эвалами?

Смотришь на эвалы и думаешь: "Это же сложные эвалы". Они справляются отлично. Но экономический импакт драматически отстаёт.

Очень трудно понять: как модель может, с одной стороны, делать удивительные вещи, а с другой — повторяться дважды в какой-то ситуации?

Вот пример — допустим, ты вайб-кодишь.

Делаешь что-то, и вылезает баг.

Говоришь модели: "Исправь баг?" И модель: "Ой, точно, у меня баг. Сейчас исправлю". И вносит второй баг.

Говоришь ей: "Теперь у тебя второй баг", а она: "Ой, как я могла?

Ты снова прав" — и возвращает первый баг. И ты можешь так чередовать бесконечно. Как это возможно? Не знаю, но это явно говорит о чём-то странном. У меня два возможных объяснения. Более экзотическое — что RL-тренировка делает модели слишком однонаправленными и узко сфокусированными, слишком невнимательными в одних аспектах, хотя и более внимательными в других.

Из-за этого они не могут делать базовые вещи.

Но есть и другое объяснение. Когда люди занимались претрейном, вопрос "на каких данных тренировать" был решён — ответ был "на всех".

Для претрейна нужны все данные.

Не надо выбирать между этими данными и теми.

Но когда делают RL — приходится думать.

Говорят: "Окей, нам нужен такой-то RL для этого и такой-то для того".

Насколько я слышал, во всех компаниях есть команды, которые просто генерят новые RL-среды и добавляют их в тренировочный микс.

Вопрос — какие?

Там столько степеней свободы.

Огромное разнообразие RL-сред, которые можно создать.

Одна штука, которую делают — думаю, непреднамеренно — это черпают вдохновение из эвалов.

Типа: "Хочу, чтобы наша модель отлично выступила на релизе.

Хочу, чтобы эвалы выглядели круто.

Какой RL помог бы на этой задаче?" Думаю, именно это и происходит, и это объясняет многое.

Если добавить сюда то, что генерализация моделей на самом деле неадекватна — это объясняет то, что мы видим: разрыв между результатами эвалов и реальной производительностью. Чего мы, кстати, даже толком не понимаем.

Мне нравится идея, что настоящий reward hacking — это люди-исследователи, слишком сфокусированные на эвалах.

Думаю, есть два способа осмыслить то, что ты сказал.

Первый: если просто став сверхчеловеком на соревнованиях по кодингу, модель не обретает автоматически вкус и здравый смысл в улучшении кодовой базы — тогда надо расширить набор сред. Не просто тестировать на результатах соревнований.

Она должна уметь делать лучшие приложения для X, Y, Z.

Второй — может, это то, на что ты намекаешь — сказать: "Почему вообще сверхчеловеческий уровень на соревнованиях не делает тебя лучшим программистом в целом?" Может, дело не в наращивании количества и разнообразия сред, а в подходе, который позволяет учиться на одной среде и улучшаться в другом.

У меня есть человеческая аналогия. Возьмём соревновательное программирование, раз уж ты его упомянул. Допустим, два студента. Один решил стать лучшим соревновательным программистом — и практиковался 10 000 часов.

Решил все задачи, выучил все техники доказательств, натренировался быстро и правильно имплементировать все алгоритмы.

И стал одним из лучших.

Студент номер два подумал: "О, соревновательное программирование — прикольно".

Может, попрактиковался 100 часов, намного меньше, и тоже показал отличные результаты.

Кто из них, по-твоему, добьётся большего в карьере?

Второй.

Верно. Думаю, примерно это и происходит.

Модели гораздо больше похожи на первого студента — но ещё сильнее.

Потому что мы говорим: модель должна быть хороша в соревновательном программировании, так давай возьмём все задачи, которые когда-либо были. А потом аугментируем данные, чтобы было ещё больше задач, и натренируем на этом.

И вот у тебя великолепный соревновательный программист. С этой аналогией интуитивнее.

Да, если она так натренирована — все алгоритмы, все техники доказательств прямо под рукой.

И интуитивнее, что с таким уровнем подготовки она не обязательно генерализуется на другое.

Но тогда какова аналогия для того, что делает второй студент до своих 100 часов файнтюнинга?

Думаю, у него есть "это". It-фактор.

Когда я был студентом, со мной учился такой человек — так что я знаю, что это существует.

Интересно отделить "это" от того, что даёт претрейн.

Один способ понять то, что ты сказал про отсутствие выбора данных в претрейне — сказать, что это не так уж отличается от 10 000 часов практики.

Просто ты получаешь их бесплатно, потому что они уже где-то в претрейн-распределении.

Но может, ты имеешь в виду, что от претрейна на самом деле не так много генерализации.

Просто данных много, но генерализуется он не обязательно лучше RL.

Главная сила претрейна: А — его много, Б — не надо сильно думать, какие данные туда класть.

Это очень естественные данные, и они включают многое из того, что делают люди: мысли, особенности.

Это как целый мир, спроецированный людьми в текст, и претрейн пытается это схватить через огромный объём данных.

О претрейне очень трудно рассуждать — сложно понять, как именно модель опирается на его данные.

Когда модель ошибается — может, что-то случайно плохо поддержано претрейн-данными? "Поддержано претрейном" — может, неточный термин.

Не знаю, могу ли я сказать что-то более полезное.

Не думаю, что есть человеческий аналог претрейна.

Вот аналогии, которые люди предлагали.

Интересно услышать, почему они потенциально неверны.

Одна — думать о первых 18, или 15, или 13 годах жизни человека, когда он не особо экономически продуктивен, но делает что-то, что помогает ему лучше понять мир.

Другая — думать об эволюции как о поиске в течение 3 миллиардов лет, который потом выдаёт экземпляр человеческой жизни.

Как думаешь, что-то из этого аналогично претрейну?

Как бы ты описал человеческое обучение за жизнь, если не как претрейн?

Думаю, есть сходства между обоими и претрейном — и претрейн пытается играть роль обоих.

Но есть и большие различия.

Объём данных претрейна просто ошеломляющий.

Да.

Человек, даже через 15 лет с крошечной долей данных претрейна, знает гораздо меньше.

Но то, что он знает — знает как-то глубже.

Уже в этом возрасте ты не совершал бы ошибок, которые делают наши AI. Ещё можно спросить: может, это что-то вроде эволюции? Возможно. Но тут у эволюции может быть преимущество.

Помню, читал про один случай.

Один из способов, которым нейробиологи изучают мозг — это люди с повреждениями разных его частей.

У некоторых самые странные симптомы. Очень интересно. Один случай к месту.

Читал про человека с каким-то повреждением мозга — инсульт или травма — которое вырубило его эмоциональную обработку.

Он перестал чувствовать какие-либо эмоции.

Остался красноречивым, мог решать головоломки, на тестах выглядел нормально.

Но не чувствовал эмоций. Ни грусти, ни гнева, ни оживления.

Он стал чрезвычайно плох в принятии любых решений.

Часами решал, какие носки надеть.

Принимал очень плохие финансовые решения.

Что это говорит о роли наших встроенных эмоций в том, чтобы делать нас работоспособным агентом? Связывая с твоим вопросом о претрейне — может, если достаточно хорошо извлекать всё из претрейна, можно получить и это.

Но это та штука, которая...

Ну, может, можно получить это из претрейна. А может, и нет.

Что такое "это"? Явно не просто эмоции напрямую. Это похоже на value function — что-то, что говорит тебе, какой должна быть конечная награда за любое решение.

Думаешь, это не приходит неявно из претрейна?

Думаю, могло бы. Просто это не на 100% очевидно.

Но что это такое? Как ты думаешь об эмоциях?

Какова ML-аналогия для эмоций?

Должно быть что-то типа value function.

Но хорошей ML-аналогии нет — сейчас value functions не играют заметной роли в том, что делают люди.

Может, стоит объяснить аудитории, что такое value function?

Конечно, с удовольствием.

Когда люди занимаются RL, как сейчас тренируют этих агентов?

Есть нейросеть, ей дают задачу, и говорят: "Реши её".

Модель делает тысячи, сотни тысяч действий или мыслей, и выдаёт решение.

Решение оценивается. Оценка используется как тренировочный сигнал для каждого действия в траектории.

Это значит: если задача решается долго — обучения не будет вообще, пока не придёшь к предложенному решению.

Вот как RL делается наивно.

Вот как якобы делаются o1, R1.

Value function говорит: "Может, я могла бы иногда говорить тебе, хорошо ты справляешься или плохо".

Понятие value function полезнее в одних областях, чем в других.

Например, в шахматах — теряешь фигуру, облажался.

Не надо играть всю партию, чтобы понять, что я только что сделал плохо, и значит, то, что этому предшествовало — тоже плохо. Value function позволяет не ждать до самого конца.

Допустим, ты делаешь что-то математическое или программируешь, исследуешь какое-то решение или направление.

После тысячи шагов размышлений приходишь к выводу: направление бесперспективно.

Как только приходишь к этому выводу — мог бы уже получить сигнал награды тысячу шагов назад, когда решил пойти этим путём.

Говоришь: "В следующий раз не пойду этим путём в похожей ситуации" — задолго до того, как пришёл к решению.

Это было в статье DeepSeek R1 — что пространство траекторий настолько широко, что, может, трудно выучить маппинг из промежуточной траектории в value.

Плюс в кодинге, например, у тебя будет неправильная идея, потом вернёшься, потом что-то изменишь.

Звучит как недоверие к deep learning.

Да, может быть трудно — но нет ничего, что deep learning не смог бы.

Ожидаю, что value function должна быть полезной, и они будут использоваться в будущем, если не уже.

Что я имел в виду про человека с повреждённым эмоциональным центром — может, это говорит о том, что value function людей модулируется эмоциями важным способом, зашитым эволюцией. И может, это важно для эффективности людей в мире.

Это то, о чём я хотел тебя спросить.

Интересно в эмоциях как value function то, что впечатляет их полезность при том, что они довольно просты для понимания.

Два ответа. Согласен — по сравнению с тем, что мы изучаем, с тем AI, о котором говорим — эмоции относительно просты.

Может, даже настолько, что их можно было бы отобразить понятным человеку способом.

Было бы круто это сделать.

Но насчёт полезности — есть такой трейдофф между сложностью и надёжностью: сложные штуки могут быть очень полезны, но простые полезны в очень широком диапазоне ситуаций.

Один способ интерпретировать: у нас эти эмоции, которые в основном эволюционировали от предков-млекопитающих, потом чуть-чуть подстроились у гоминидов.

У нас есть приличное количество социальных эмоций, которых у млекопитающих может не быть. Но они не очень сложные. И потому что не сложные — служат нам хорошо в этом совсем другом мире. Хотя и ошибаются иногда. Например, наши эмоции... Хотя, не знаю.

Голод — это эмоция? Спорно.

Но думаю, наше интуитивное чувство голода не преуспевает в том, чтобы направлять нас правильно в мире с изобилием еды.

Люди говорят про scaling данных, параметров, compute. Есть ли более общий способ думать о scaling?

Какие ещё оси scaling?

Вот перспектива, которая, думаю, может быть верной.

Раньше ML работало так: люди просто возились и пытались получить интересные результаты.

Так было в прошлом. Потом пришёл scaling insight. Scaling laws, GPT-3, и все внезапно поняли: надо скейлиться.

Это пример того, как язык влияет на мышление. "Scaling" — всего одно слово, но такое мощное, потому что говорит людям, что делать.

Говорят: "Давайте скейлить".

И спрашиваешь: что скейлим?

Претрейн был тем, что надо скейлить.

Конкретный scaling recipe.

Большой прорыв претрейна — осознание, что этот рецепт хорош.

Типа: "Если смешать compute с данными в нейросети определённого размера — получишь результаты.

И будешь знать, что станешь лучше, если просто заскейлишь рецепт". Это тоже отлично.

Компании это любят — низкорисковый способ инвестировать ресурсы.

Гораздо сложнее инвестировать в исследования. Сравни: "Идите, исследователи, и придумайте что-нибудь" — против "возьмите больше данных, больше compute".

Знаешь, что получишь что-то от претрейна.

Судя по тому, что люди пишут в Твиттере — похоже, Gemini нашли способ выжать больше из претрейна.

Но в какой-то момент у претрейна закончатся данные.

Данные явно конечны. Что делать дальше? Либо какой-то прокачанный претрейн — другой рецепт, либо RL, либо что-то ещё.

Но теперь compute большой — очень большой — и в каком-то смысле мы вернулись в эпоху исследований.

Может, вот ещё способ сказать.

С 2012 по 2020 — эпоха исследований.

С 2020 по 2025 — эпоха scaling (плюс-минус погрешность). Все говорили: "Это удивительно. Скейльтесь больше. Продолжайте скейлиться". Одно слово: scaling. Но теперь scale такой большой.

Правда ли вера такова: "О, это много, но если бы было в 100 раз больше — всё было бы по-другому?" Было бы по-другому, конечно.

Но вера ли это — что если просто 100x scale, всё преобразится?

Не думаю. Так что мы снова в эпохе исследований — просто с большими компьютерами.

Интересный способ сформулировать.

Но позволь задать тебе вопрос, который ты сам поставил.

Что мы скейлим, и что значило бы иметь рецепт?

Я не знаю очень чистой зависимости — почти как закон физики — которая была в претрейне.

Был power law между данными или compute или параметрами и loss.

Какую зависимость искать, и как думать о новом рецепте?

Мы уже видели переход от одного типа scaling к другому — от претрейна к RL. Теперь скейлят RL. Судя по Твиттеру, сейчас тратят больше compute на RL, чем на претрейн — потому что RL реально может съесть много compute.

Делаешь очень длинные rollouts — требуется много compute, чтобы их сгенерить.

Получаешь относительно мало обучения на rollout — так что реально можешь потратить много compute.

Я бы даже не назвал это scaling.

Я бы сказал: "Что вы делаете?

То, что делаете — самая продуктивная штука?

Можете найти более продуктивный способ использовать compute?" Мы обсуждали value function раньше.

Может, когда люди станут хороши в value functions — будут использовать ресурсы продуктивнее.

Если найдёшь совсем другой способ тренировки — можно сказать: "Это scaling или просто использование ресурсов?" Становится двусмысленно.

В эпоху исследований было: "Попробуем это и это.

Попробуем то и то.

О, смотри, что-то интересное".

Думаю, будет возврат к этому.

Если мы снова в эре исследований — какая часть рецепта та, о которой надо думать больше всего?

Когда говоришь "value function" — люди уже пробуют текущий рецепт с LLM-as-judge и прочим.

Можно сказать, это value function, но звучит, будто у тебя на уме что-то фундаментальнее.

Надо ли вообще переосмыслить претрейн, а не просто добавлять шаги в конец?

Дискуссия о value function была интересной.

Хочу подчеркнуть: думаю, value function сделает RL эффективнее, и это имеет значение.

Но всё, что можно сделать с value function, можно и без неё — просто медленнее.

Самое фундаментальное — модели просто генерализуются драматически хуже людей. Это суперочевидно. Очень фундаментальная штука.

Так что суть — генерализация. Два подвопроса. Первый про sample efficiency: почему моделям нужно столько данных для обучения по сравнению с людьми? Второй — даже отдельно от количества данных — почему так трудно научить модель тому, что мы хотим, по сравнению с человеком?

Человеку не обязательно нужна verifiable reward... Ты, наверное, менторишь кучу исследователей сейчас — разговариваешь с ними, показываешь свой код, как ты думаешь.

Из этого они перенимают твой образ мышления и как им вести исследования.

Не надо устанавливать verifiable reward типа: "Окей, это следующая часть curriculum, а теперь следующая. О, эта тренировка была нестабильной". Нет этого утомительного bespoke процесса.

Возможно, эти две проблемы связаны, но интересно исследовать вторую — больше похожую на continual learning — и первую, которая ощущается как sample efficiency. Можно задаться вопросом: одно из возможных объяснений человеческой sample efficiency — эволюция.

Эволюция дала нам небольшое количество наиболее полезной информации.

Для зрения, слуха, локомоции — думаю, есть сильный аргумент, что эволюция дала нам много.

Например, человеческая ловкость далеко впереди... Роботы тоже могут стать ловкими, если подвергнуть их огромному количеству тренировки в симуляции.

Но натренировать робота в реальном мире быстро осваивать навык, как человек — кажется очень далёким.

Тут можно сказать: "Да, локомоция.

Всем нашим предкам нужна была отличная локомоция, белкам.

Так что с локомоцией у нас, может, невероятный prior".

То же для зрения.

Ян Лекун высказывал мысль, что дети учатся водить за 10 часов практики — что правда.

Но наше зрение такое хорошее.

По крайней мере я помню себя пятилетним.

Тогда очень увлекался машинами.

Почти уверен, что моё распознавание машин было более чем адекватным для вождения уже в пять лет.

Не так много данных видишь пятилетним.

Большую часть времени в доме родителей — очень низкое разнообразие данных.

Но можно сказать: может, это тоже эволюция.

А вот в языке, математике, кодинге — вероятно, нет.

И всё равно кажется лучше моделей.

Очевидно, модели лучше среднего человека в языке, математике, кодинге.

Но лучше ли среднего человека в обучении?

О да. Абсолютно. Что я хотел сказать: язык, математика, кодинг — особенно математика и кодинг — предполагают, что что бы ни делало людей хорошими в обучении, это, вероятно, не сложный prior, а что-то более фундаментальное.

Не уверен, что понял. Почему так?

Рассмотри навык, в котором люди демонстрируют большую надёжность.

Если навык был очень полезен предкам миллионы лет, сотни миллионов — можно утверждать, что люди хороши в нём из-за эволюции, из-за evolutionary prior, закодированного каким-то неочевидным образом.

Но если люди демонстрируют большую способность, надёжность, robustness и способность учиться в области, которая не существовала до недавнего времени — это скорее указывает, что у людей просто лучший machine learning. Точка.

Как думать о том, что это такое? Какова ML-аналогия? Пара интересных штук. Это требует меньше samples. Более unsupervised. Ребёнок, учащийся водить... Дети не учатся водить.

Подросток, учащийся водить, не получает готовую verifiable reward.

Это из взаимодействия с машиной и средой.

Требует гораздо меньше samples. Кажется более unsupervised. Кажется более robust?

Гораздо более robust. Robustness людей просто ошеломляющая.

Есть ли единый способ думать, почему всё это происходит одновременно?

Какова ML-аналогия, которая могла бы это реализовать?

Ты спрашивал, как подросток-водитель может самокорректироваться и учиться без внешнего учителя?

Ответ — у них есть value function.

Общее чувство, которое, кстати, чрезвычайно robust у людей.

Какой бы ни была человеческая value function — за исключениями вроде зависимости — она очень, очень robust.

Так что подросток начинает водить и сразу имеет чувство того, как водит — как плохо, как неуверенно.

А потом видит: "Окей". И конечно, скорость обучения любого подростка очень высокая.

Через 10 часов готов ехать.

Похоже, у людей есть решение — но интересно, как они это делают и почему так сложно?

Как надо переосмыслить тренировку моделей, чтобы сделать это возможным?

Отличный вопрос, и у меня много мнений по нему.

Но, к сожалению, мы живём в мире, где не все идеи ML обсуждаются свободно — и это одна из них.

Вероятно, есть способ.

Думаю, это можно сделать.

То, что люди такие — доказательство, что это возможно.

Но может быть ещё один blocker: есть вероятность, что человеческие нейроны производят больше compute, чем мы думаем. Если это правда и играет важную роль — вещи могут быть сложнее.

Но независимо от этого — думаю, это указывает на существование какого-то ML-принципа, о котором у меня есть мнения.

Но, к сожалению, обстоятельства затрудняют детальное обсуждение.

Никто не слушает этот подкаст, Илья.

Интересно. Если говоришь, что мы вернулись в эпоху исследований — ты был там с 2012 по 2020.

Каким теперь будет вайб, если вернёмся в эпоху исследований?

Например, даже после AlexNet количество compute на эксперименты продолжало расти, и размер frontier-систем рос.

Думаешь, эта эпоха исследований всё ещё будет требовать огромных объёмов compute?

Думаешь, надо будет лезть в архивы и читать старые статьи?

Ты был в Google, OpenAI, Стэнфорде — в этих местах, когда там было больше research vibe?

Чего ждать от сообщества?

Одно из последствий эпохи scaling — scaling высосал весь воздух из комнаты.

Из-за этого все стали делать одно и то же.

Мы дошли до точки, где компаний больше, чем идей. Причём намного.

Кстати, есть такая поговорка в Кремниевой Долине: идеи дешёвые, исполнение — всё.

Часто это говорят, и в этом есть правда.

Но потом видел, как кто-то написал в Твиттере: "Если идеи так дешёвы — как так вышло, что ни у кого нет идей?" Думаю, это тоже правда.

Если думать о прогрессе исследований через bottlenecks — их несколько.

Один — идеи, другой — способность воплотить их, что может быть compute, но и engineering.

В 90-х были люди с хорошими идеями — если бы у них были большие компьютеры, может, они могли бы показать, что идеи жизнеспособны.

Но не могли — так что могли показать только крошечную демонстрацию, никого не убеждавшую. Bottleneck был в compute. В эпоху scaling compute вырос сильно.

Конечно, вопрос — сколько compute нужно — но compute большой.

Достаточно большой, что неочевидно, нужно ли намного больше, чтобы доказать какую-то идею. Дам аналогию. AlexNet построен на двух GPU.

Весь compute, который на него ушёл.

Transformer построен на 8-64 GPU.

Ни один эксперимент в статье про Transformer не использовал больше 64 GPU 2017 года — что было бы как два сегодняшних GPU? ResNet, верно? Можно утверждать, что o1 reasoning не был самой тяжёлой по compute штукой.

Так что для исследований нужен какой-то compute, но далеко не очевидно, что нужен абсолютный максимум.

Можно утверждать — и думаю, это правда — что если хочешь построить абсолютно лучшую систему, помогает иметь намного больше compute.

Особенно если все в одной парадигме — compute становится одним из главных дифференциаторов.

Спрашиваю про историю, потому что ты реально там был.

Не уверен, что на самом деле произошло.

Звучит, будто было возможно развить эти идеи с минимальным compute.

Но Transformer не сразу стал знаменитым.

Он стал тем, что все начали делать и строить поверх — потому что был валидирован на всё более высоких уровнях compute.

Верно.

И если у вас в SSI 50 разных идей — как узнать, какая следующий Transformer, а какая хрупкая, без того compute, что есть у других frontier labs?

Могу прокомментировать. Коротко: ты упомянул SSI.

Конкретно для нас — compute, который есть у SSI на исследования, не такой уж маленький. Объясню почему. Простая математика объясняет, почему наш compute сопоставим для исследований больше, чем кажется. SSI привлекла 3 миллиарда долларов — много в абсолютном смысле.

Можно сказать: "Посмотри на другие компании, привлекающие больше".

Но много их compute идёт на inference.

Эти большие цифры, большие займы — предназначены для inference. Это раз.

Два — если хочешь иметь продукт для inference, нужен большой штат инженеров, продавцов.

Много исследований должно идти на продуктовые фичи.

Так что когда смотришь, что реально остаётся на исследования — разница становится намного меньше.

Другое — если делаешь что-то другое, реально ли нужен абсолютно максимальный scale, чтобы доказать?

Вообще не думаю, что это правда.

Думаю, у нас достаточно compute, чтобы доказать — убедить себя и кого угодно — что то, что мы делаем, правильно.

Были публичные оценки, что компании вроде OpenAI тратят порядка 5-6 миллиардов в год — только пока — на эксперименты.

Отдельно от денег на inference и прочее.

Так что похоже, они тратят в год на research-эксперименты больше, чем у вас всего финансирования.

Думаю, вопрос в том, что ты с этим делаешь.

Вопрос в том, что ты с этим делаешь.

В их случае — гораздо больше спрос на training compute.

Больше разных workstreams, разные modalities, просто больше всего. Фрагментируется.

Как SSI будет зарабатывать?

Мой ответ примерно такой.

Сейчас просто фокусируемся на исследованиях, а ответ откроется сам.

Думаю, будет много возможных ответов.

План SSI всё ещё — straight shot к superintelligence?

Может быть. Думаю, в этом есть merit.

Много merit — потому что очень приятно не быть затронутым повседневной рыночной конкуренцией.

Но есть две причины, которые могут заставить изменить план.

Одна прагматическая — если timelines окажутся длинными, что может быть.

Вторая — думаю, есть большая ценность в том, чтобы лучший, самый мощный AI был там, влияя на мир.

Это значимо ценно.

Тогда почему дефолтный план — straight shot к superintelligence?

Звучит, будто OpenAI, Anthropic, все эти компании — их explicit thinking: "У нас всё более слабые интеллекты, к которым публика может привыкнуть и подготовиться".

Почему потенциально лучше строить superintelligence напрямую?

Приведу аргументы за и против.

Аргумент за: одна из проблем на рынке — приходится участвовать в rat race.

Rat race сложен тем, что подвергает сложным trade-offs.

Приятно сказать: "Изолируем себя от всего этого, сфокусируемся на исследованиях и выйдем, только когда готовы".

Но контраргумент тоже валиден — противоборствующие силы.

Контраргумент: "Для мира полезно видеть мощный AI.

Полезно, потому что это единственный способ донести..."

Ну, даже не просто донести идею... Донести AI, не идею. Донести AI.

В смысле "донести AI"?

Допустим, ты пишешь эссе про AI: "AI будет таким, AI будет сяким".

Читаешь и думаешь: "Окей, интересное эссе".

А теперь представь, что видишь AI, делающий это и то. Несравнимо. В принципе думаю, есть большая польза от AI в публичном доступе — и это было бы причиной не идти совсем прямым путём.

Полагаю, дело даже не только в этом — но это важная часть.

Другая большая штука — не могу придумать другой дисциплины в человеческой инженерии, где конечный артефакт стал безопаснее в основном через размышления о безопасности — а не как с авиакатастрофами: их на милю намного меньше сегодня, чем десятилетия назад.

Почему намного сложнее найти баг в Linux, чем десятилетия назад?

Думаю, в основном потому, что эти системы были deployed в мир.

Замечали сбои, исправляли, системы становились robust'нее.

Не уверен, почему AGI и superhuman intelligence были бы иначе — особенно учитывая (надеюсь, дойдём до этого) — что вред от superintelligence не просто в злонамеренном paperclip maximizer.

Это реально мощная штука, и мы даже не знаем, как концептуализировать взаимодействие людей с ней, что люди будут с ней делать.

Постепенный доступ кажется лучшим способом распределить impact и помочь людям подготовиться.

Думаю, по этому пункту — даже в straight shot сценарии всё равно был бы gradual release, так я это представляю.

Gradual'ность была бы неотъемлемым компонентом любого плана.

Просто вопрос, что является первой штукой, которую выпускаешь. Это раз. Два — ты, кажется, выступал за continual learning больше других, и думаю, это важно и правильно. Вот почему. Дам ещё пример, как язык влияет на мышление.

В данном случае — два слова, которые сформировали мышление каждого, утверждаю.

Первое: AGI. Второе: pretraining.

Объясню. Термин AGI — почему он существует? Очень специфический термин. Почему? Есть причина. По-моему, причина не столько в том, что это важный дескриптор какого-то конечного состояния интеллекта — а потому что это реакция на другой термин: narrow AI.

В древней истории game AI, шашечного AI, шахматного AI, компьютерных игр — все говорили: посмотрите на этот narrow intelligence.

Конечно, шахматный AI может победить Каспарова — но ничего другого не может.

Такой narrow — artificial narrow intelligence.

В ответ некоторые сказали: это не хорошо. Так narrow. Нам нужен general AI — AI, который может всё.

Термин стал популярным.

Вторая штука, ставшая популярной — pretraining, конкретно pretraining recipe.

Думаю, как люди делают RL сейчас — возможно, отменяет концептуальный отпечаток pretraining.

Но у pretraining было свойство: больше pretraining — модель лучше во всём, более-менее равномерно.

General AI. Pretraining даёт AGI. Но штука, которая случилась с AGI и pretraining — в каком-то смысле они промахнулись мимо цели.

Если подумать о термине "AGI", особенно в контексте pretraining — поймёшь, что человек не AGI.

Да, есть фундамент навыков, но человеку не хватает огромного количества знаний.

Вместо этого мы полагаемся на continual learning.

Так что когда думаешь: "Окей, допустим, мы достигли успеха и произвели безопасный superintelligence".

Вопрос — как его определять?

Где на кривой continual learning он будет?

Я произвожу superintelligent'ного 15-летнего, который очень жаждет действовать.

Он не знает очень многого, отличный студент, очень жаждущий.

Ты иди и будь программистом, ты иди и будь врачом, иди и учись.

Так что можно представить, что deployment сам по себе будет включать период обучения методом проб и ошибок.

Это процесс, а не сброс готовой штуки.

Понятно. Ты предполагаешь, что superintelligence — это не какой-то законченный разум, который знает, как делать каждую работу в экономике.

Потому что оригинальный устав OpenAI определяет AGI как — она может делать каждую работу, каждую вещь, которую может человек. Ты предлагаешь разум, который может научиться делать каждую работу — и это superintelligence.

Да.

Но как только у тебя есть learning algorithm — он deployится в мир так же, как человек может присоединиться к организации.

Именно.

Похоже, одна из двух вещей может произойти — а может, ни одна.

Первая: этот супер-эффективный learning algorithm становится superhuman, становится таким же хорошим, как ты, и потенциально лучше в ML research.

В результате алгоритм сам становится всё более superhuman.

Другая — даже если этого не произойдёт — если у тебя одна модель (явно твоё видение), где instances модели deployed по всей экономике, выполняют разные работы, учатся на работе, перенимают все навыки, которые мог бы перенять человек, но все одновременно, а потом объединяют свои знания — у тебя по сути модель, которая функционально становится superintelligent'ной даже без recursive self-improvement в software. Потому что теперь одна модель может делать каждую работу, а люди не могут объединять разумы таким образом.

Ожидаешь intelligence explosion от широкого deployment?

Думаю, вероятен быстрый экономический рост.

С широким deployment два аргумента, противоречащих друг другу. Один — как только дойдёшь до точки, где AI может учиться быстро и их много, будет сильная сила deployить их в экономику, если только не будет regulation, которое остановит — что, кстати, может быть.

Но идея очень быстрого экономического роста на какое-то время — думаю, очень возможна от широкого deployment.

Вопрос — насколько быстрым.

Трудно знать — с одной стороны, очень эффективный работник.

С другой — мир просто большой, много всего, движется с разной скоростью.

Но с другой стороны, AI мог бы...

Так что очень быстрый экономический рост возможен.

Увидим разные страны с разными правилами — те, у кого правила дружелюбнее, экономический рост быстрее. Трудно предсказать.

Кажется, это очень шаткая ситуация.

В пределе знаем — это должно быть возможно.

Если есть что-то, так же хорошее в learning как человек, но способное объединять свои мозги — объединять instances так, как люди не могут — уже это кажется физически возможным.

Люди возможны, digital computers возможны.

Надо просто объединить и то, и другое, чтобы произвести эту штуку.

Также кажется, что такая штука чрезвычайно мощная.

Экономический рост — один способ сказать.

Dyson sphere — это много экономического роста.

Но другой способ — у тебя будет, потенциально за очень короткое время...

Ты нанимаешь людей в SSI, и через шесть месяцев они, вероятно, становятся net-positive.

Человек учится очень быстро, и эта штука становится умнее очень быстро.

Как ты думаешь о том, чтобы это прошло хорошо? Почему SSI positioned, чтобы сделать это хорошо?

Каков план SSI — вот что я пытаюсь спросить.

Одно из изменений в моём thinking — теперь придаю больше значения тому, чтобы AI deployился постепенно и заранее.

Очень сложная штука с AI — мы говорим о системах, которые ещё не существуют, и их трудно представить.

Одна из вещей, которая происходит — на практике очень трудно почувствовать AGI.

Очень трудно почувствовать AGI.

Можем говорить об этом, но представь разговор о том, каково быть старым и немощным.

Можно поговорить, попытаться представить, но трудно — и возвращаешься к реальности, где это не так.

Многие проблемы вокруг AGI и его будущей мощи — из того факта, что его очень трудно представить.

Будущий AI будет другим. Мощным. В чём вообще проблема AI и AGI?

Вся проблема в мощи.

Вся проблема в мощи.

Когда мощь реально большая — что произойдёт?

Одно из изменений моего мнения за последний год — и это может распространиться назад в планы компании — если это трудно представить, что ты делаешь?

Надо показывать эту штуку.

Надо показывать эту штуку.

Утверждаю, что большинство людей, работающих над AI, тоже не могут представить это — потому что слишком отличается от того, что видят повседневно.

Вот предсказание.

Утверждаю, что по мере того как AI становится мощнее — люди будут менять поведение.

Увидим беспрецедентные вещи, которые не происходят сейчас. Несколько примеров. Думаю, к лучшему или худшему, frontier companies будут играть важную роль, как и правительство.

То, что увидишь, начало чего уже видишь — компании, яростные конкуренты, начинают сотрудничать по AI safety.

OpenAI и Anthropic делают первый маленький шаг — но этого не существовало.

Это я предсказывал в одном из выступлений года три назад.

Также утверждаю — по мере того как AI становится мощнее, видимо мощнее — будет желание со стороны правительств и публики что-то сделать.

Это очень важная сила — показ AI. Это раз.

Два — AI строится. Что надо сделать? Утверждаю — сейчас люди, работающие над AI... AI не ощущается мощным из-за его ошибок.

Реально думаю — в какой-то момент AI начнёт ощущаться мощным по-настоящему.

Когда это случится — увидим большое изменение в том, как все AI-компании подходят к safety. Станут намного параноидальнее.

Это предсказание. Посмотрим, прав ли я. Но думаю, это случится — потому что увидят, как AI становится мощнее.

Всё, что происходит сейчас — происходит потому, что люди смотрят на сегодняшний AI, и трудно представить будущий.

Третья вещь, которая должна произойти.

Говорю в широких терминах, не только с точки зрения SSI — ты спросил о нашей компании.

Вопрос — что компании должны стремиться построить?

Что они должны стремиться построить?

Была одна большая идея, на которой все зациклились — self-improving AI. Почему?

Потому что идей меньше, чем компаний.

Но утверждаю — есть что-то лучше для построения, и все захотят этого.

Это AI, который надёжно aligned на заботу о sentient life конкретно.

В частности — есть аргумент, что будет легче построить AI, который заботится о sentient life, чем AI, который заботится только о human life — потому что AI сам будет sentient.

Если подумать о mirror neurons и человеческой эмпатии к животным — которой, можно сказать, недостаточно, но она есть — думаю, это emergent property от того, что мы моделируем других той же схемой, что себя, потому что это самое эффективное.

Даже если заставишь AI заботиться о sentient beings — и мне не ясно, что это то, что надо делать, если решил alignment problem — всё равно будет так, что большинством sentient beings будут AI.

Триллионы, в конечном итоге квадриллионы AI.

Люди будут очень маленькой долей sentient beings.

Так что не ясно, если цель — human control над будущей цивилизацией — что это лучший критерий.

Правда. Возможно, не лучший. Скажу две вещи. Раз — забота о sentient life, думаю, в этом merit. Стоит рассмотреть. Было бы полезно, если бы был короткий список идей, которые компании в этой ситуации могли бы использовать. Это два.

Три — было бы реально materially полезно, если бы мощь самого мощного superintelligence была как-то ограничена — потому что это решило бы многие проблемы.

Как это сделать — не уверен, но было бы materially полезно, когда говоришь о реально мощных системах.

Прежде чем продолжим про alignment — хочу double-click на этом.

Сколько места наверху?

Как ты думаешь о superintelligence?

Думаешь ли — используя идею learning efficiency — может, он просто очень быстр в изучении новых навыков или знаний?

У него просто больший пул стратегий?

Есть ли единое сплочённое "оно" в центре, которое более мощное или большое?

Если да — представляешь, что это будет вроде богоподобным по сравнению с остальной цивилизацией, или это просто ещё один агент или кластер агентов?

Это область, где у разных людей разные интуиции.

Думаю, он будет очень мощным.

Что наиболее вероятно — будет несколько таких AI, созданных примерно в одно время.

Думаю, если кластер достаточно большой — буквально размером с континент — эта штука может быть реально мощной.

Если буквально кластер размером с континент — эти AI могут быть очень мощными.

Всё, что могу сказать — если говоришь о чрезвычайно мощных AI, драматически мощных — было бы неплохо, если бы они были сдержаны в чём-то, или было соглашение.

В чём беспокойство по поводу superintelligence?

Один способ объяснить?

Если представишь систему достаточно мощную — реально достаточно мощную — и ты мог бы сказать, тебе нужно делать что-то разумное, типа заботиться о sentient life очень однонаправленно — нам могут не понравиться результаты. Это то, что это такое. Может, кстати, ответ — не строить RL-агента в обычном смысле. Укажу на несколько вещей. Думаю, люди — полу-RL агенты.

Мы преследуем reward, потом эмоции заставляют уставать от reward, и преследуем другой.

Рынок — очень близорукий агент. Эволюция такая же. Очень умна в одних аспектах, очень глупа в других.

Правительство было designed как бесконечная борьба между тремя частями — что имеет эффект.

Думаю о вещах вроде этого.

Ещё штука, делающая дискуссию сложной — мы говорим о системах, которые не существуют, которые не знаем, как строить.

Это моё убеждение.

Думаю, то, что люди делают сейчас, пройдёт какое-то расстояние и выдохнется.

Будет продолжать улучшаться, но это не будет "оно".

"Оно" мы не знаем, как строить — и многое зависит от понимания reliable generalization. Ещё одна вещь.

Можно сказать, что делает alignment трудным — способность изучать human values хрупка.

Способность оптимизировать их хрупка.

На самом деле учишься оптимизировать их.

Разве нельзя сказать: "Разве это всё не примеры unreliable generalization?" Почему люди, кажется, генерализуются настолько лучше?

Что если бы generalization была намного лучше?

Что бы случилось тогда? Каков был бы эффект? Но эти вопросы пока без ответа.

Как думать о том, как выглядит AI, когда всё идёт хорошо?

Ты обрисовал, как AI может evolve.

У нас эти continual learning агенты. AI очень мощный.

Может быть много разных AI.

Как думаешь о множестве continent-size интеллектов? Насколько опасно? Как сделать менее опасным?

Как сделать способом, который защищает equilibrium, где могут быть unaligned AI и bad actors?

Вот почему мне понравился "AI, который заботится о sentient life".

Можем спорить, хорошо это или плохо.

Но если первые N таких драматических систем реально заботятся о, любят человечество или что-то такое — заботятся о sentient life — очевидно, это должно быть достигнуто. Если достигнуто первыми N систем — могу видеть, что всё идёт хорошо, по крайней мере какое-то время. Потом вопрос — что в долгосрочной перспективе.

Как достичь долгосрочного equilibrium?

Думаю, там тоже есть ответ.

Мне не нравится этот ответ, но его надо рассмотреть.

В долгосрочной перспективе можно сказать: "Если есть мир с мощными AI — в краткосрочной можно сказать, universal high income.

Universal high income, и всё хорошо".

Но что говорят буддисты? "Перемены — единственная постоянная". Вещи меняются. Есть правительство, политическая структура — она меняется, у неё срок годности.

Появляется новая government штука, функционирует, потом перестаёт.

Это видим постоянно.

Для долгосрочного equilibrium — один подход: у каждого человека будет AI, исполняющий его волю — и это хорошо.

Если поддерживать бесконечно — правда.

Но недостаток — AI идёт и зарабатывает для человека, отстаивает его нужды в политике, пишет отчёт: "Вот что сделал, вот ситуация", человек: "Отлично, продолжай".

Но человек больше не участник.

Тогда это шаткое положение.

Начну с того, что мне не нравится это решение, но это решение.

Решение — если люди станут частично AI с каким-то Neuralink++.

Потому что тогда — AI понимает что-то, и мы понимаем тоже, потому что понимание передаётся целиком.

Если AI в какой-то ситуации — ты вовлечён полностью.

Думаю, это ответ на equilibrium.

Интересно, является ли то, что эмоции, evolved миллионы — во многих случаях миллиарды — лет назад в совершенно другой среде, всё ещё направляют наши действия так сильно — примером alignment success.

Поясню — не знаю, точнее ли называть это value function или reward function — но brainstem имеет директиву: "Спаривайся с кем-то более успешным".

Cortex понимает, что означает успех в современном контексте.

Но brainstem способен aligned'ить cortex: "Как бы ты ни распознавала успех — и я недостаточно умён, чтобы понять — ты всё равно будешь следовать этой директиве".

Думаю, есть более общий момент.

Это реально загадочно, как эволюция кодирует high-level desires.

Довольно легко понять, как эволюция наделила бы нас желанием еды, которая хорошо пахнет — запах это chemical, просто преследуй вещество. Легко представить.

Но эволюция также наделила нас социальными desires.

Нам реально важно быть воспринимаемыми обществом положительно.

Важно быть на хорошем счету.

Все эти социальные intuitions — сильно чувствую, что они hardwired.

Не знаю, как эволюция сделала это — потому что это high-level concept, represented в мозге.

Тебе важна какая-то социальная вещь — это не low-level signal как запах.

Это не что-то, для чего есть sensor.

Мозгу надо много processing, чтобы собрать воедино биты информации, чтобы понять, что происходит социально.

Как-то эволюция сказала: "Это то, о чём тебе следует заботиться". Как? И быстро, к тому же. Все эти сложные социальные вещи — думаю, evolved довольно недавно.

Эволюции было легко hardcode'ить это high-level желание.

Не знаю хорошей гипотезы, как это сделано.

Были идеи, которые обдумывал — но ни одна не satisfactory.

Что особенно впечатляет — если бы это было желание, которое ты выучил за жизнь — имеет смысл, твой мозг разумен.

Имеет смысл, почему ты смог бы выучить разумные желания.

Может, это не твой аргумент, но один способ понять — желание embedded в геном, а геном не разумен.

Но как-то способен описать эту функцию.

Даже не ясно, как определить эту функцию — и можно embed'ить её в гены.

По сути — или скажу по-другому.

Если подумать об инструментах, доступных геному — он говорит: "Вот recipe для построения мозга".

Можно сказать: "Вот recipe для соединения dopamine neurons с датчиком запаха".

Если запах определённого хорошего вида — хочешь съесть.

Могу представить, что геном делает это.

Утверждаю — это труднее представить.

Труднее представить, что геном говорит: тебе следует заботиться о каком-то сложном computation, которое делает весь твой мозг, большой кусок мозга.

Это всё, что утверждаю. Могу рассказать speculation о том, как это могло быть сделано.

Позволь предложить speculation — и объясню, почему она, вероятно, false.

У мозга есть brain regions. У нас cortex. У неё все эти regions.

Cortex однородна, но brain regions и neurons в cortex как бы говорят с соседями в основном.

Это объясняет, почему есть regions.

Если хочешь делать speech processing — все neurons, занимающиеся speech, должны говорить друг с другом.

Поскольку neurons могут говорить только с ближайшими соседями — это должна быть region.

Все regions в основном в одном месте от человека к человеку.

Может, эволюция hardcoded'ила буквально location в мозге.

Типа: "Когда GPS-координаты мозга такие-то — когда это fires, это то, о чём следует заботиться".

Может, это то, что сделала эволюция — было бы в toolkit'е эволюции.

Хотя есть примеры — у людей, родившихся слепыми, эта region их cortex захвачена другим sense.

Не знаю, но был бы surprised, если бы desires или reward functions, требующие visual signal, больше не работали для людей, у которых разные regions кооптированы.

Например, без зрения — можешь ли всё ещё чувствовать "хочу, чтобы люди любили меня" и т.д., для чего обычно есть visual cues.

Полностью согласен. Есть ещё более сильный контраргумент этой теории.

Есть люди, которым удаляют половину мозга в детстве — и у них всё ещё все brain regions.

Но они все перемещаются в одно hemisphere — что предполагает, что locations regions не fixed, и теория неверна.

Было бы круто, если бы было правдой — но не так.

Думаю, это загадка.

Интересная загадка. Факт — как-то эволюция смогла наделить нас способностью заботиться о социальных вещах очень надёжно. Даже люди со странными mental states, дефицитами, эмоциональными проблемами — склонны заботиться об этом тоже.

Что SSI планирует делать по-другому?

Предположительно, план — быть одной из frontier companies, когда время придёт.

Предположительно, ты основал SSI, потому что: "Думаю, у меня есть способ подхода к тому, как сделать это safe'но, которого у других нет". В чём разница?

Как бы описал — есть идеи, которые считаю promising, хочу исследовать и посмотреть, реально ли они promising. Вот так просто.

Это попытка. Если идеи окажутся верными — вокруг understanding generalization — думаю, будет что-то достойное.

Окажутся ли верными? Занимаемся research. Мы определённо "эпоха research" компания. Делаем прогресс. Сделали довольно хороший прогресс за год, но надо продолжать. Вот как вижу. Вижу как попытку быть голосом и участником.

Твой сооснователь и предыдущий CEO ушёл в Meta недавно — люди спрашивали: "Если бы делались прорывы, это кажется маловероятным". Как ответишь?

Напомню несколько фактов, которые, возможно, забыты.

Эти факты, предоставляющие context, объясняют ситуацию.

Context был — мы привлекали при valuation 32 миллиарда, и Meta пришла и предложила acquire нас — я сказал нет.

Но бывший сооснователь в каком-то смысле сказал да.

В результате он также смог насладиться большой краткосрочной liquidity — и он был единственным из SSI, кто присоединился к Meta.

Звучит, будто план SSI — быть frontier company, когда дойдём до этого важного периода с superhuman intelligence.

У вас идеи о том, как сделать так, чтобы superhuman intelligence прошёл хорошо. Но другие компании будут пробовать свои идеи.

Что отличает подход SSI?

Главное — технический подход.

У нас другой технический подход, который считаю достойным, и мы его преследуем.

Утверждаю — в конце будет convergence стратегий.

Будет convergence, где в какой-то момент, по мере того как AI становится мощнее, всем станет яснее, какой должна быть стратегия.

Что-то вроде: надо найти способ говорить друг с другом, и хочется, чтобы первый реальный superintelligent AI был aligned и как-то заботился о sentient life, о людях, демократичный — какая-то комбинация.

Думаю, это условие, к которому каждый должен стремиться.

Это то, к чему стремится SSI.

Думаю, в этот раз, если не уже, все другие компании поймут, что стремятся к тому же.

Посмотрим. Думаю, мир реально изменится, когда AI станет мощнее.

Вещи будут реально другими, люди будут действовать реально по-другому.

Говоря о прогнозах — каковы твои прогнозы до системы, которую описываешь — человекоподобный learner, и в результате superhuman?

Думаю, от 5 до 20.

5 до 20 лет?

Мгм.

Хочу развернуть, как ты видишь наступление мира.

Типа — ещё пара лет, другие компании продолжают текущий подход, и он stalls.

"Stalls" значит — зарабатывают не более чем низкие сотни миллиардов revenue?

Как думаешь, что значит "stalls"?

Думаю, "stalls" будет выглядеть... всё будет выглядеть очень похоже у всех компаний.

Что-то вроде этого.

Не уверен — думаю, даже stalling, компании могут сделать колоссальный revenue.

Может, не profit — потому что надо много работать, чтобы differentiate друг от друга — но revenue определённо.

Но что-то в твоей модели подразумевает — когда правильное решение появится, будет convergence между компаниями.

Почему так?

Говорил больше о convergence alignment стратегий.

Думаю, окончательная convergence технического подхода тоже будет — но намекал на convergence alignment стратегий.

Что именно надо сделать?

Хочу лучше понять, как видишь развёртывание будущего.

Сейчас разные компании — ожидаешь, что их подход продолжит генерировать revenue, но не доберётся до человекоподобного learner. Теперь разные ветви.

Вы, Thinking Machines, куча других лабораторий.

Может, одна выяснит правильный подход.

Но выпуск их продукта делает ясным для других, как делать?

Думаю, не будет ясно, как делать — но будет ясно, что что-то другое возможно — и это информация.

Люди будут пытаться выяснить, как это работает.

Реально думаю — одна из вещей, не затронутых — с каждым увеличением capabilities AI будут какие-то изменения, но не знаю какие, в том, как делаются дела.

Думаю, будет важно — но не могу сформулировать.

По умолчанию ожидал бы — компания с этой моделью будет получать все выгоды, потому что у них модель с навыками и знаниями, которые она накапливает.

Почему думать, что выгоды будут широко распределены, а не окажутся у той model company, которая запустит continual learning cycle первой?

Вот что думаю произойдёт.

Раз — как дела шли с AI прошлого.

Одна компания произвела advancement — другая засуетилась и произвела похожее — начали конкурировать и толкать цены вниз.

С рыночной перспективы — похожее будет и там.

Говорим о хорошем мире, кстати.

Хороший мир — мощные human-like learners, которые также... Кстати, будет специализация.

Будет специализация.

Думаю — разные компании будут хороши в разном.

Некоторые в AI-врачах, некоторые в AI-юристах, некоторые в AI-инженерах.

Специализация — ещё причина, почему будет много игроков.

Хочу закончить на research taste.

Ты упомянул другой технический подход в SSI.

Очевидно, не можешь поделиться деталями — но что определяет твой вкус?

Что ищешь в идее?

Что делает идею красивой?

Для меня красивая идея — та, которая объясняет многое очень просто.

Это то, что нравится.

Нравятся идеи, которые объясняют "почему", дают "почему".

Если есть "почему" — можешь понять, что происходит.

Если нет "почему" — тычешь в темноте.

Нравятся идеи, которые дают "почему", и нравятся простые идеи.

Если идея проста — вероятно, верна.

Если сложная — вероятно, неверна.

Это бритва Оккама.

Но это также то, что видел в карьере.

Самые успешные идеи были самыми простыми.

Deep learning очень простое.

Просто matrix multiplication и nonlinearities. Это всё.

Складываешь в стопку.

Делаешь много раз.

Используешь много данных.

Очень просто.

Scaling очень простое.

Просто сделай больше.

Очень просто.

Нравятся простые идеи.

Есть пример идеи, которая была сложной и оказалась неверной?

О, их так много.

Вся область computer vision до deep learning была полна сложных идей.

Люди придумывали сложные feature descriptors — SIFT, HOG и всё это.

Очень сложные.

Работали немного, но не хорошо.

Потом пришло deep learning и смело их.

Потому что было простым и работало.

Rich Sutton написал знаменитое эссе "The Bitter Lesson".

В нём утверждает — единственное, что важно в долгосрочной перспективе, это использование compute.

И что надо прекратить пытаться встраивать human knowledge в AI.

Согласен?

Думаю, много правды.

История показала — методы, которые scale'ятся с compute, побеждают методы, которые не scale'ятся.

Методы, полагающиеся на human knowledge, обычно не scale'ятся с compute.

Потому что human knowledge fixed.

Не можешь просто добавить больше human knowledge так легко, как добавить больше compute.

Думаю, Rich Sutton прав.

Но есть нюанс.

Нюанс — всё ещё нужна правильная архитектура.

Нужен правильный алгоритм.

Не можешь просто бросить compute на что угодно.

Если бросишь compute на logistic regression — не получишь AGI.

Нужно правильное "что".

И это "что" должно быть простым и scalable.

Это мой вкус.

Когда увидим человекоподобного learner?

Каков прогноз?

Не хочу давать конкретную дату.

Но думаю, не так далеко.

Увидим признаки в ближайшие пару лет.

Может, 2-3 года?

Что-то вроде того.

Когда это случится — большой paradigm shift.

Внезапно модели, которые могут учиться как мы.

Откроет дверь к настоящему superintelligence.

Можно просто запустить их и позволить учиться и становиться умнее.

Они смогут делать research.

Смогут улучшать себя.

Начало чего-то совершенно нового.

Думаешь, текущая парадигма stalls до того, как это случится?

Думаю, уже видим признаки.

Выгоды от простого scaling pretraining уменьшаются.

Люди начинают понимать, что нужно что-то ещё.

И это "что-то ещё" — то, над чем мы работаем.

И другие тоже начнут.

Так что да, будет переход.

Илья, спасибо большое за разговор.

Было увлекательно.

Спасибо, что пригласил.

Было весело.
